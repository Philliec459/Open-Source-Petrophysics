{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "011543e2-20ff-42b3-81d8-c77ad51a49c1",
   "metadata": {},
   "source": [
    "# Qt Petrophysical Workflow – Executive Summary & Technical Overview\n",
    "\n",
    "![Qt](Qt_workflow.png)\n",
    "\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This application implements a **Qt (PySide6)–based petrophysical workflow** that integrates multi‑run LAS data, parameterized saturation models, and interactive visualization into a single, reproducible system. The core design principle is simple but powerful:\n",
    "\n",
    "> **All computations write into a single authoritative table (`state.analysis_df`), and all visualizations read from it.**\n",
    "\n",
    "This guarantees consistency between calculations, plots, and exported parameters, while allowing iterative re‑computation as the user adjusts assumptions (Rw, m, n, Vsh model, etc.).\n",
    "\n",
    "The workflow currently supports:\n",
    "\n",
    "- Computation of a Neutron-Density Chartbook Porosity\n",
    "- Compute Shale Volume from multiple shale indicators using Hodges-Lehman for the final Vsh\n",
    "- Standard (Pickett/Archie‑style) saturation products\n",
    "- Waxman–Smits saturation with physically consistent porosity partitioning\n",
    "- Composite porosity/water visualization using PyQtGraph fill tracks\n",
    "- Persistent parameter export (`Pickett.txt`) for reproducibility and tracking of parameters\n",
    "\n",
    "---\n",
    "\n",
    "## High‑Level Architecture\n",
    "\n",
    "### Core Objects\n",
    "\n",
    "- **`WorkflowController`**  \n",
    "  Orchestrates computation and visualization refresh. Owns the active `state`.\n",
    "\n",
    "- **`state`**  \n",
    "  A lightweight container holding:\n",
    "  - `analysis_df` – the master DataFrame of all curves (measured + derived)\n",
    "  - `params` – the current parameter dictionary driven by the UI\n",
    "\n",
    "- **UI Panels**\n",
    "  - Compute panels (SW / WS tabs)\n",
    "  - `PlotsPanel` (depth tracks and crossplots)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Flow (Authoritative Pattern)\n",
    "\n",
    "1. **Load / Merge LAS runs**  \n",
    "   → populate `state.analysis_df`\n",
    "\n",
    "2. **Resolve curve families**  \n",
    "   (RT, PHIT, CBW, VSH, etc.) to actual mnemonics\n",
    "\n",
    "3. **Compute products**  \n",
    "   - Write *only* into `analysis_df`\n",
    "   - Never compute directly inside plotting code\n",
    "\n",
    "4. **Sync UI**  \n",
    "   - `rebuild_view()` ensures widgets reference the latest DataFrame\n",
    "   - `update_plots()` / `refresh_plots()` redraw tracks\n",
    "\n",
    "This separation is what makes the system stable and extensible.\n",
    "\n",
    "---\n",
    "\n",
    "## Saturation Workflows\n",
    "\n",
    "### 1) Standard SW (Pickett‑Style)\n",
    "\n",
    "Triggered by:\n",
    "```python\n",
    "_on_compute_sw_clicked()\n",
    "```\n",
    "\n",
    "**Purpose**\n",
    "- Compute Archie/Pickett‑style saturation and bulk volumes\n",
    "\n",
    "**Typical outputs written to `analysis_df`:**\n",
    "- `SWT`, `BVW`, `BVO`\n",
    "- `MSTAR_FIT`, `MSTAR_APP`\n",
    "\n",
    "These products are typically used for quick QC and chartbook‑style analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Waxman–Smits (WS) Workflow\n",
    "\n",
    "Triggered by:\n",
    "```python\n",
    "_on_compute_ws_clicked()\n",
    "```\n",
    "\n",
    "**Purpose**\n",
    "- Compute Waxman–Smits saturation\n",
    "- Partition porosity into physically meaningful components\n",
    "\n",
    "**Key derived curves:**\n",
    "- `SW_CP` – Waxman–Smits water saturation\n",
    "- `BVWT_CP = PHIT * SW_CP`\n",
    "- `BVWe_CP = max(BVWT_CP − CBW, 0)`\n",
    "- `PHIE = max(PHIT − CBW, 0)`\n",
    "\n",
    "**Important constraints**\n",
    "- `PHIT` is clamped ≥ 0 before use\n",
    "- `PHIE ≤ PHIT` enforced only after PHIT is non‑negative\n",
    "\n",
    "---\n",
    "\n",
    "## Qv and B (Physical Implementation)\n",
    "\n",
    "### Qv (Hill–Shirley–Klein)\n",
    "\n",
    "When `Swb` is available, Qv is computed as:\n",
    "\n",
    "\\[\n",
    "Q_v = \\frac{S_{wb}}{0.6425 / \\sqrt{\\rho_f \\cdot SAL} + 0.22}\n",
    "\\]\n",
    "\n",
    "- Requires `Swb`, `SAL`, and fluid density\n",
    "- If `Swb` is missing, Qv falls back to zero (Archie‑like behavior)\n",
    "\n",
    "### B (Bdacy)\n",
    "\n",
    "- Computed from temperature and Rw using the **Bdacy correlation**\n",
    "- Reduced to a **single scalar** (median) for Waxman–Smits\n",
    "\n",
    "This ensures B is consistent and stable across the interval.\n",
    "\n",
    "---\n",
    "\n",
    "## Track‑Based Visualization\n",
    "\n",
    "### Track Keys vs Curve Names (Critical Concept)\n",
    "\n",
    "- **Track keys** (e.g. `\"bvw\"`) identify a plot area\n",
    "- **Curve names** (e.g. `\"PHIE\"`) are data columns\n",
    "\n",
    "Confusing the two leads to `KeyError` and empty tracks.\n",
    "\n",
    "---\n",
    "\n",
    "### Track 5 – Porosity & Water Partition (Composite Fill Track)\n",
    "\n",
    "This track intentionally **does not use NMR**. It visualizes the Waxman–Smits porosity system:\n",
    "\n",
    "| Region | Meaning |\n",
    "|------|--------|\n",
    "| PHIT → PHIE | Bound water porosity (CBW) |\n",
    "| PHIE → BVWe | Effective bulk water |\n",
    "| BVWe → 0 | Remaining effective pore volume |\n",
    "\n",
    "Implemented using `FillBetweenItem` in PyQtGraph.\n",
    "\n",
    "**Key rule:** composite tracks must *not* be indiscriminately cleared with `PlotItem.clear()` after plotting.\n",
    "\n",
    "---\n",
    "\n",
    "## UI Parameter Handling (Why Defaults Were Written)\n",
    "\n",
    "Qt widgets do **not commit typed values** until focus changes.\n",
    "\n",
    "### Fixes implemented\n",
    "\n",
    "1. **Live synchronization**\n",
    "   ```python\n",
    "   spin.valueChanged.connect(lambda v: params[key] = v)\n",
    "   ```\n",
    "\n",
    "2. **Forced commit before compute/export**\n",
    "   ```python\n",
    "   spin.interpretText()\n",
    "   ```\n",
    "\n",
    "This guarantees `Pickett.txt` always reflects what the user sees.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameter Export (Reproducibility)\n",
    "\n",
    "At WS compute time, constants are written to:\n",
    "\n",
    "```\n",
    "./data/parameters/Pickett.txt\n",
    "```\n",
    "\n",
    "Contents include:\n",
    "- `m_cem`\n",
    "- `n_sat`\n",
    "- `Rw`\n",
    "- `mslope`\n",
    "- `Bdacy`\n",
    "\n",
    "This creates a durable record of assumptions used to generate results.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls and Lessons Learned\n",
    "\n",
    "- **Empty tracks** → caused by clearing after plotting\n",
    "- **Missing legends** → `addLegend()` must be called explicitly\n",
    "- **Dark fills** → use RGBA brushes with low alpha\n",
    "- **Dashed Rt** → set pen style (`Qt.DashLine`)\n",
    "- **Qv = 0 everywhere** → `Swb` missing\n",
    "- **Negative PHIE** → PHIT must be clamped before enforcing PHIE ≤ PHIT\n",
    "\n",
    "---\n",
    "\n",
    "## Design Philosophy (Why This Works)\n",
    "\n",
    "- Single source of truth (`analysis_df`)\n",
    "- Strict separation of compute vs display\n",
    "- Physically interpretable saturation partitioning\n",
    "- Parameter persistence for auditability\n",
    "\n",
    "This architecture mirrors professional petrophysical platforms while remaining transparent, extensible, and reproducible.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Component | File / Module | Responsibility |\n",
    "|---------|---------------|----------------|\n",
    "| App entry | apps/merge_gui/main.py | Launch application |\n",
    "| Main window | ui_main_window.py | Build UI, docks, tabs |\n",
    "| Controller | workflow_controller.py | Own state, trigger refresh |\n",
    "| State | state object | analysis_df + params |\n",
    "| Plotting | plots_panel.py | Depth tracks, fills, legends |\n",
    "| SW compute | *_panel.py | Pickett-style saturation |\n",
    "| WS compute | *_panel.py | Waxman–Smits saturation |\n",
    "| Math utils | utils / models | compute_sw_products, waxsmits_it |\n",
    "| Export | utils | write Pickett.txt |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **run the process:**\n",
    "\n",
    "## *python -m apps.merge_gui.main*\n",
    "\n",
    "---\n",
    "## **QC Merged data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba033f-63db-4b4b-a583-4a28ff23effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lasio\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def robust_z(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Robust z-score using median/MAD. Keeps NaNs.\"\"\"\n",
    "    x = x.astype(float)\n",
    "    m = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - m))\n",
    "    if not np.isfinite(mad) or mad < 1e-12:\n",
    "        # fallback to standard deviation\n",
    "        s = np.nanstd(x)\n",
    "        if not np.isfinite(s) or s < 1e-12:\n",
    "            return x * np.nan\n",
    "        return (x - m) / s\n",
    "    return (x - m) / (1.4826 * mad)\n",
    "\n",
    "\n",
    "def interpolate_to_grid(depth: np.ndarray, x: np.ndarray, grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Linear interpolation to grid, keeping NaNs where data absent.\"\"\"\n",
    "    m = np.isfinite(depth) & np.isfinite(x)\n",
    "    if m.sum() < 5:\n",
    "        return np.full_like(grid, np.nan, dtype=float)\n",
    "\n",
    "    d = depth[m]\n",
    "    v = x[m]\n",
    "    # ensure increasing for interp\n",
    "    o = np.argsort(d)\n",
    "    d = d[o]\n",
    "    v = v[o]\n",
    "\n",
    "    y = np.interp(grid, d, v, left=np.nan, right=np.nan)\n",
    "    # np.interp can't produce NaNs in-between gaps; mask large gaps:\n",
    "    # mark grid points farther than 2*step from nearest original sample as NaN\n",
    "    # (simple gap handling)\n",
    "    step = np.nanmedian(np.diff(d))\n",
    "    if not np.isfinite(step) or step <= 0:\n",
    "        return y\n",
    "    # nearest distance via searchsorted\n",
    "    idx = np.searchsorted(d, grid)\n",
    "    idx0 = np.clip(idx - 1, 0, len(d) - 1)\n",
    "    idx1 = np.clip(idx, 0, len(d) - 1)\n",
    "    dist = np.minimum(np.abs(grid - d[idx0]), np.abs(grid - d[idx1]))\n",
    "    y[dist > 2.5 * step] = np.nan\n",
    "    return y\n",
    "\n",
    "\n",
    "def best_lag_corr(a: np.ndarray, b: np.ndarray, max_lag_samples: int) -> tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Find lag (in samples) that maximizes correlation between a and b.\n",
    "    Positive lag means b is shifted DOWN (to deeper) relative to a (b occurs later).\n",
    "    \"\"\"\n",
    "    best_lag = 0\n",
    "    best_r = -np.inf\n",
    "\n",
    "    for lag in range(-max_lag_samples, max_lag_samples + 1):\n",
    "        if lag < 0:\n",
    "            aa = a[-lag:]\n",
    "            bb = b[: len(aa)]\n",
    "        elif lag > 0:\n",
    "            bb = b[lag:]\n",
    "            aa = a[: len(bb)]\n",
    "        else:\n",
    "            aa = a\n",
    "            bb = b\n",
    "\n",
    "        m = np.isfinite(aa) & np.isfinite(bb)\n",
    "        if m.sum() < 50:\n",
    "            continue\n",
    "\n",
    "        x = aa[m]\n",
    "        y = bb[m]\n",
    "        # correlation\n",
    "        r = np.corrcoef(x, y)[0, 1]\n",
    "        if np.isfinite(r) and r > best_r:\n",
    "            best_r = r\n",
    "            best_lag = lag\n",
    "\n",
    "    return best_lag, float(best_r if np.isfinite(best_r) else np.nan)\n",
    "\n",
    "\n",
    "def qc_alignment(\n",
    "    las_path: Path,\n",
    "    out_dir: Path,\n",
    "    families: dict[str, list[str]],\n",
    "    grid_step: float | None,\n",
    "    max_lag_ft: float,\n",
    "    make_plots: bool,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    las = lasio.read(str(las_path))\n",
    "    df = las.df().copy()\n",
    "    df.index.name = \"DEPT\"\n",
    "\n",
    "    depth = df.index.to_numpy(dtype=float)\n",
    "    if depth.size < 10:\n",
    "        raise RuntimeError(\"LAS has too few depth samples.\")\n",
    "\n",
    "    # Depth sanity\n",
    "    diffs = np.diff(depth)\n",
    "    step_med = float(np.nanmedian(diffs)) if diffs.size else np.nan\n",
    "    monotonic = bool(np.all(diffs > 0))\n",
    "    has_dupes = bool(np.any(diffs == 0))\n",
    "\n",
    "    # Define grid\n",
    "    if grid_step is None:\n",
    "        grid_step = step_med if np.isfinite(step_med) and step_med > 0 else 0.5\n",
    "\n",
    "    d0 = float(np.nanmin(depth))\n",
    "    d1 = float(np.nanmax(depth))\n",
    "    grid = np.arange(d0, d1 + grid_step * 0.5, grid_step)\n",
    "\n",
    "    report_rows = []\n",
    "    worst_examples = []\n",
    "\n",
    "    for fam, cols in families.items():\n",
    "        present = [c for c in cols if c in df.columns]\n",
    "        if len(present) < 2:\n",
    "            continue\n",
    "\n",
    "        # interpolate + robust normalize each curve\n",
    "        series = {}\n",
    "        for c in present:\n",
    "            y = interpolate_to_grid(depth, pd.to_numeric(df[c], errors=\"coerce\").to_numpy(), grid)\n",
    "            y = robust_z(y)\n",
    "            series[c] = y\n",
    "\n",
    "        max_lag_samples = int(round(max_lag_ft / grid_step))\n",
    "\n",
    "        # pairwise lags\n",
    "        for i in range(len(present)):\n",
    "            for j in range(i + 1, len(present)):\n",
    "                c1, c2 = present[i], present[j]\n",
    "                lag_s, r = best_lag_corr(series[c1], series[c2], max_lag_samples)\n",
    "                lag_ft = lag_s * grid_step\n",
    "\n",
    "                report_rows.append(\n",
    "                    {\n",
    "                        \"family\": fam,\n",
    "                        \"curve_a\": c1,\n",
    "                        \"curve_b\": c2,\n",
    "                        \"grid_step_ft\": grid_step,\n",
    "                        \"best_lag_samples\": lag_s,\n",
    "                        \"best_lag_ft\": lag_ft,\n",
    "                        \"corr_at_best_lag\": r,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                worst_examples.append((abs(lag_ft), fam, c1, c2, lag_ft, r))\n",
    "\n",
    "    #report = pd.DataFrame(report_rows).sort_values([\"family\", \"best_lag_ft\"], key=lambda s: np.abs(s))\n",
    "\n",
    "    report = pd.DataFrame(report_rows)\n",
    "    if not report.empty:\n",
    "        report[\"abs_lag_ft\"] = report[\"best_lag_ft\"].abs()\n",
    "        report = report.sort_values([\"family\", \"abs_lag_ft\", \"corr_at_best_lag\"], ascending=[True, True, False])\n",
    "\n",
    "    \n",
    "    report_path = out_dir / \"qc_alignment_report.csv\"\n",
    "    report.to_csv(report_path, index=False)\n",
    "\n",
    "    # Save depth sanity\n",
    "    sanity = {\n",
    "        \"monotonic_increasing_depth\": monotonic,\n",
    "        \"has_duplicate_depths\": has_dupes,\n",
    "        \"median_step_ft\": step_med,\n",
    "        \"grid_step_ft_used\": grid_step,\n",
    "        \"depth_min\": float(np.nanmin(depth)),\n",
    "        \"depth_max\": float(np.nanmax(depth)),\n",
    "    }\n",
    "    (out_dir / \"qc_depth_sanity.txt\").write_text(\"\\n\".join(f\"{k}: {v}\" for k, v in sanity.items()) + \"\\n\")\n",
    "\n",
    "    # Optional plots of the worst offenders\n",
    "    if make_plots and worst_examples:\n",
    "        worst_examples.sort(reverse=True)\n",
    "        for k, (abs_lag, fam, c1, c2, lag_ft, r) in enumerate(worst_examples[:6], start=1):\n",
    "            y1 = pd.to_numeric(df[c1], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            y2 = pd.to_numeric(df[c2], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "            # choose a representative window with lots of valid data\n",
    "            m = np.isfinite(y1) & np.isfinite(y2)\n",
    "            if m.sum() < 200:\n",
    "                continue\n",
    "            # window centered on median valid depth\n",
    "            d_valid = depth[m]\n",
    "            center = float(np.nanmedian(d_valid))\n",
    "            w = 50.0  # ft\n",
    "            sel = (depth >= center - w) & (depth <= center + w)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 10))\n",
    "            ax.plot(y1[sel], depth[sel], label=c1)\n",
    "            ax.plot(y2[sel], depth[sel], label=c2)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_title(f\"{fam}: {c1} vs {c2} | best lag ~ {lag_ft:+.2f} ft | r={r:.3f}\")\n",
    "            ax.set_xlabel(\"Value\")\n",
    "            ax.set_ylabel(\"Depth (ft)\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(out_dir / f\"qc_worst_{k}_{fam}_{c1}_vs_{c2}.png\", dpi=160)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"Wrote: {report_path}\")\n",
    "    print(f\"Wrote: {out_dir / 'qc_depth_sanity.txt'}\")\n",
    "    if make_plots:\n",
    "        print(f\"Wrote plots into: {out_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"las\", type=str, help=\"Path to merged LAS file\")\n",
    "    p.add_argument(\"--out\", type=str, default=\"qc_out\", help=\"Output directory\")\n",
    "    p.add_argument(\"--step\", type=float, default=None, help=\"Resample grid step (ft). Default uses median LAS step.\")\n",
    "    p.add_argument(\"--max_lag_ft\", type=float, default=10.0, help=\"Max lag search (ft)\")\n",
    "    p.add_argument(\"--plots\", action=\"store_true\", help=\"Write a few plots for worst offenders\")\n",
    "    args = p.parse_args()\n",
    "\n",
    "    families = {\n",
    "        \"gamma\": [\"GR_EDTC \",\"GR\", \"CGR\",\"HCGR\", \"SGR\",\"HSGR\"],\n",
    "        \"porosity_sonic\": [\"NPOR\",\"NPHI\", \"TNPH\", \"RHOB\", \"RHOZ\",\"PHIT_NMR\", \"DT\", \"DTC\", \"DTCO\"],\n",
    "        #\"resistivity\": [\"RT\", \"ILD\", \"LLD\", \"ILM\", \"LLS\", \"AT90\", \"AF90\", \"AT60\", \"AF60\"],\n",
    "    }\n",
    "\n",
    "    qc_alignment(\n",
    "        las_path=Path(args.las),\n",
    "        out_dir=Path(args.out),\n",
    "        families=families,\n",
    "        grid_step=args.step,\n",
    "        max_lag_ft=args.max_lag_ft,\n",
    "        make_plots=args.plots,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "#python qc_merged_las_alignment.py path/to/Merged_....las --out qc_merge --max_lag_ft 10 --plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b478ab0-faff-4490-85f5-79ff713a5326",
   "metadata": {},
   "source": [
    "# QC plots too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76a7798-7fb1-4baa-9d4b-8417f2ab82c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lf/9hqr0q153ls52s9btzjc56080000gp/T/ipykernel_57373/685447758.py:116: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax.legend(loc=\"best\", fontsize=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: qc_merge_tracks.png\n",
      "Wrote: qc_merge_alignment_vs_GR_EDTC.csv\n",
      "series  best_lag_ft_vs_GR_EDTC  corr_at_best  abs_corr_score\n",
      "  HSGR                     3.0      0.670822        0.670822\n",
      "  HCGR                     3.0      0.444946        0.444946\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lasio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def to_num(df, c):\n",
    "    if c not in df.columns:\n",
    "        return None\n",
    "    return pd.to_numeric(df[c], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "def interp_to_grid(depth, x, grid):\n",
    "    m = np.isfinite(depth) & np.isfinite(x)\n",
    "    if m.sum() < 50:\n",
    "        return np.full_like(grid, np.nan, float)\n",
    "    d = depth[m]; v = x[m]\n",
    "    o = np.argsort(d)\n",
    "    d = d[o]; v = v[o]\n",
    "    return np.interp(grid, d, v, left=np.nan, right=np.nan)\n",
    "\n",
    "def deriv(x):\n",
    "    dx = np.full_like(x, np.nan, float)\n",
    "    m = np.isfinite(x)\n",
    "    idx = np.where(m)[0]\n",
    "    if len(idx) < 5:\n",
    "        return dx\n",
    "    dx[idx[1:]] = x[idx[1:]] - x[idx[:-1]]\n",
    "    return dx\n",
    "\n",
    "def best_lag_abs_corr(a, b, max_lag):\n",
    "    best_lag = 0\n",
    "    best_score = -np.inf\n",
    "    best_r = np.nan\n",
    "    for lag in range(-max_lag, max_lag+1):\n",
    "        if lag < 0:\n",
    "            aa = a[-lag:]; bb = b[:len(aa)]\n",
    "        elif lag > 0:\n",
    "            bb = b[lag:]; aa = a[:len(bb)]\n",
    "        else:\n",
    "            aa = a; bb = b\n",
    "        m = np.isfinite(aa) & np.isfinite(bb)\n",
    "        if m.sum() < 200:\n",
    "            continue\n",
    "        r = np.corrcoef(aa[m], bb[m])[0, 1]\n",
    "        if not np.isfinite(r):\n",
    "            continue\n",
    "        score = abs(r)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_lag = lag\n",
    "            best_r = r\n",
    "    return best_lag, best_score, best_r\n",
    "\n",
    "def density_porosity(rhob, rho_ma=2.71, rho_f=1.10):\n",
    "    rhob = np.asarray(rhob, float)\n",
    "    denom = (rho_ma - rho_f)\n",
    "    phi = (rho_ma - rhob) / denom\n",
    "    return np.clip(phi, -0.15, 0.60)\n",
    "\n",
    "def sonic_porosity(dt, dt_ma=47.6, dt_f=189.0):\n",
    "    dt = np.asarray(dt, float)\n",
    "    denom = (dt_f - dt_ma)\n",
    "    phi = (dt - dt_ma) / denom\n",
    "    return np.clip(phi, -0.15, 0.60)\n",
    "\n",
    "def qc_run_alignment(df, ref_curve=\"GR_EDTC\", other_series=None, step=0.5, max_lag_ft=10):\n",
    "    \"\"\"\n",
    "    other_series: dict[label -> ndarray on original depth index]\n",
    "    \"\"\"\n",
    "    depth = df.index.to_numpy(float)\n",
    "    grid = np.arange(np.nanmin(depth), np.nanmax(depth)+step*0.5, step)\n",
    "\n",
    "    ref = to_num(df, ref_curve)\n",
    "    if ref is None:\n",
    "        raise ValueError(f\"Missing reference curve {ref_curve}\")\n",
    "\n",
    "    ref_g = interp_to_grid(depth, ref, grid)\n",
    "    ref_d = deriv(ref_g)\n",
    "\n",
    "    out = []\n",
    "    max_lag = int(round(max_lag_ft/step))\n",
    "\n",
    "    for label, x in other_series.items():\n",
    "        if x is None:\n",
    "            continue\n",
    "        xg = interp_to_grid(depth, x, grid)\n",
    "        xd = deriv(xg)\n",
    "        lag_s, score, r = best_lag_abs_corr(ref_d, xd, max_lag)\n",
    "        out.append((label, lag_s*step, r, score))\n",
    "    return pd.DataFrame(out, columns=[\"series\", \"best_lag_ft_vs_GR_EDTC\", \"corr_at_best\", \"abs_corr_score\"])\n",
    "\n",
    "def plot_tracks(df, out_png=\"qc_tracks.png\", dmin=None, dmax=None):\n",
    "    depth = df.index.to_numpy(float)\n",
    "    if dmin is None: dmin = float(np.nanmin(depth))\n",
    "    if dmax is None: dmax = float(np.nanmax(depth))\n",
    "\n",
    "    # Build porosity equivalents\n",
    "    npor = to_num(df, \"NPOR\")\n",
    "    rhoz = to_num(df, \"RHOZ\")\n",
    "    dtco = to_num(df, \"DTCO\")\n",
    "\n",
    "    phi_d = density_porosity(rhoz) if rhoz is not None else None\n",
    "    phi_s = sonic_porosity(dtco) if dtco is not None else None\n",
    "\n",
    "    def plot_track(ax, series_dict, title, xlab=None, invert_x=False, xscale=None):\n",
    "        for name, x in series_dict.items():\n",
    "            if x is None:\n",
    "                continue\n",
    "            ax.plot(x, depth, label=name)\n",
    "        ax.set_ylim(dmax, 10000)  # inverted depth\n",
    "        if invert_x:\n",
    "            ax.invert_xaxis()\n",
    "        if xscale:\n",
    "            ax.set_xscale(xscale)\n",
    "        ax.set_title(title)\n",
    "        ax.grid(True, alpha=0.25)\n",
    "        ax.legend(loc=\"best\", fontsize=8)\n",
    "        if xlab:\n",
    "            ax.set_xlabel(xlab)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(18, 38), sharey=True)\n",
    "\n",
    "    # Track 1: Gamma\n",
    "    plot_track(\n",
    "        axs[0],\n",
    "        {\n",
    "            \"GR_EDTC\": to_num(df, \"GR_EDTC\"),\n",
    "            \"HCGR\":    to_num(df, \"HCGR\"),\n",
    "            \"HSGR\":    to_num(df, \"HSGR\"),\n",
    "        },\n",
    "        \"Track 1: Gamma\",\n",
    "        xlab=\"GAPI\",\n",
    "    )\n",
    "\n",
    "    # Track 2: Resistivity\n",
    "    plot_track(\n",
    "        axs[1],\n",
    "        {\"AF90\": to_num(df, \"AF90\"), \"AT90\": to_num(df, \"AT90\")},\n",
    "        \"Track 2: Resistivity\",\n",
    "        xlab=\"ohm-m\",\n",
    "        xscale=\"log\",\n",
    "    )\n",
    "\n",
    "    # Track 3: Porosity equivalents (apples-to-apples)\n",
    "    plot_track(\n",
    "        axs[2],\n",
    "        {\"NPOR\": npor, \"PHI_DEN\": phi_d, \"PHI_Sonic\": phi_s},\n",
    "        \"Track 3: Porosity equivalents\",\n",
    "        xlab=\"v/v\",\n",
    "        invert_x=False,\n",
    "    )\n",
    "\n",
    "    # Track 4: NMR vs conventional porosity\n",
    "    plot_track(\n",
    "        axs[3],\n",
    "        {\n",
    "            \"PHIT_NMR\": to_num(df, \"PHIT_NMR\"),\n",
    "            #\"PHIE_NMR\": to_num(df, \"PHIE_NMR\"),\n",
    "            \"NPOR\": npor,\n",
    "            \"PHI_DEN\": phi_d,\n",
    "        },\n",
    "        \"Track 4: NMR vs conventional\",\n",
    "        xlab=\"v/v\",\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=170)\n",
    "    plt.close(fig)\n",
    "    print(\"Wrote:\", out_png)\n",
    "\n",
    "def main():\n",
    "    las = lasio.read(\"./01_NGS_NGS_MAIN_AIT_TLD_MCFL_CNL_046PUP_2062-10520.las\")\n",
    "    df = las.df().copy()\n",
    "    df.index.name = \"DEPT\"\n",
    "\n",
    "    # Choose a zoom window if you want (optional)\n",
    "    # dmin, dmax = 10000, 10500\n",
    "    dmin, dmax = None, None\n",
    "\n",
    "    plot_tracks(df, out_png=\"qc_merge_tracks.png\", dmin=dmin, dmax=dmax)\n",
    "\n",
    "    # Build series to compare to GR_EDTC (use derivatives for bed-edge timing)\n",
    "    npor = to_num(df, \"NPOR\")\n",
    "    rhoz = to_num(df, \"RHOZ\")\n",
    "    dtco = to_num(df, \"DTCO\")\n",
    "\n",
    "    other = {\n",
    "        # gamma companions\n",
    "        \"HCGR\": to_num(df, \"HCGR\"),\n",
    "        \"HSGR\": to_num(df, \"HSGR\"),\n",
    "\n",
    "        # resistivity\n",
    "        \"AF90\": to_num(df, \"AF90\"),\n",
    "        \"AT90\": to_num(df, \"AT90\"),\n",
    "\n",
    "        # porosity equivalents (better than raw RHOB/DT)\n",
    "        \"NPOR\": npor,\n",
    "        \"PHI_DEN\": density_porosity(rhoz) if rhoz is not None else None,\n",
    "        \"PHI_Sonic\": sonic_porosity(dtco) if dtco is not None else None,\n",
    "\n",
    "        # NMR\n",
    "        \"PHIT_NMR\": to_num(df, \"PHIT_NMR\"),\n",
    "        #\"PHIE_NMR\": to_num(df, \"PHIE_NMR\"),\n",
    "    }\n",
    "\n",
    "    rep = qc_run_alignment(df, ref_curve=\"GR_EDTC\", other_series=other, step=0.5, max_lag_ft=10)\n",
    "    rep = rep.sort_values(\"abs_corr_score\", ascending=False)\n",
    "    rep.to_csv(\"qc_merge_alignment_vs_GR_EDTC.csv\", index=False)\n",
    "    print(\"Wrote: qc_merge_alignment_vs_GR_EDTC.csv\")\n",
    "    print(rep.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500671c-ac3a-4750-8f19-5788fd2dc983",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## main.py\n",
    "\n",
    "### ./apps/merge_gui/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd259455-dfa7-4030-a69f-eb500d8d0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from PySide6.QtWidgets import QApplication\n",
    "\n",
    "MERGED_LAS = \"/Users/craig/Documents/petro_suite2/data/outputs/merged/Merged_Well_Log_Bakken_Bakken_renamed.las\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    from PySide6.QtWidgets import QApplication\n",
    "    from apps.merge_gui.ui_main_window import MainWindow\n",
    "\n",
    "    app = QApplication(sys.argv)\n",
    "\n",
    "    w = MainWindow()\n",
    "    w.show()\n",
    "\n",
    "    sys.exit(app.exec())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e91b7-9478-4212-9dc0-67f6bf7c7c7b",
   "metadata": {},
   "source": [
    "## state.py\n",
    "\n",
    "### ./petrocore/workflow/state.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda8d07-840f-4cb6-bd97-62fd5be392f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "print(\">>> LOADING state.py from:\", __file__)\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from petrocore.models.dataset import Dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WorkflowState:\n",
    "    # -------------------------\n",
    "    # Data sources (one or both may be used)\n",
    "    # -------------------------\n",
    "    dataset: Optional[Dataset] = None               # preferred (for workflow)\n",
    "    merged_df: Optional[pd.DataFrame] = None        # optional (merge/QC legacy)\n",
    "\n",
    "    # -------------------------\n",
    "    # Active working frame\n",
    "    # -------------------------\n",
    "    analysis_df: Optional[pd.DataFrame] = None      # what PlotsPanel reads\n",
    "\n",
    "    # -------------------------\n",
    "    # Depth / ZOI\n",
    "    # -------------------------\n",
    "    depth_limits: Optional[Tuple[float, float]] = None\n",
    "    zoi_depth_range: Optional[Tuple[float, float]] = None\n",
    "\n",
    "    # -------------------------\n",
    "    # Workflow config + execution\n",
    "    # -------------------------\n",
    "    params: Dict[str, Any] = field(default_factory=dict)\n",
    "    enabled_steps: Dict[str, bool] = field(default_factory=dict)\n",
    "    registry: Any = None\n",
    "\n",
    "    # -------------------------\n",
    "    # Tops overlays (optional)\n",
    "    # -------------------------\n",
    "    tops_df: pd.DataFrame = field(\n",
    "        default_factory=lambda: pd.DataFrame(columns=[\"Name\", \"Depth\", \"Color\"])\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # UI helpers\n",
    "    # -------------------------\n",
    "    plot_title: str = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f99d1-9c6f-44ef-b237-9e129e3688d3",
   "metadata": {},
   "source": [
    "## plots_panel.py\n",
    "\n",
    "### ./apps/merge_gui/ui_panels/plots_panel.py\n",
    "\n",
    "**python -m apps.merge_gui.main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d95355-d8fa-4895-b52b-02d71bc591c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW VERSION\n",
    "\n",
    "\n",
    "\n",
    "# apps/merge_gui/ui_panels/plots_panel.py\n",
    "from __future__ import annotations\n",
    "\n",
    "print(\">>> LOADING plots_panel.py from:\", __file__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyqtgraph as pg\n",
    "\n",
    "from PySide6.QtCore import Qt, QSettings, QTimer, Signal\n",
    "from PySide6.QtWidgets import (\n",
    "    QLabel,\n",
    "    QSizePolicy,\n",
    "    QSlider,\n",
    "    QMessageBox,\n",
    "    QFileDialog,\n",
    "    QWidget,\n",
    "    QVBoxLayout,\n",
    "    QHBoxLayout,\n",
    "    QTabWidget,\n",
    "    QFormLayout,\n",
    "    QPushButton,\n",
    "    QDoubleSpinBox,\n",
    "    QSpinBox,\n",
    "    QGridLayout,   # ✅ add this\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_qtagg import FigureCanvasQTAgg as FigureCanvas\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Small utilities\n",
    "# =============================================================================\n",
    "def compute_sal_kppm_from_rw75(Rw75):\n",
    "    \"\"\"\n",
    "    SAL in KPPM from your equation:\n",
    "      SAL = (10**((3.562 - log10(Rw75 - 0.0123))/0.955))/1000\n",
    "    \"\"\"\n",
    "    Rw75 = np.asarray(Rw75, dtype=float)\n",
    "    # guard: Rw75 must be > 0.0123 for (Rw75 - 0.0123) > 0\n",
    "    x = np.clip(Rw75 - 0.0123, 1e-6, None)\n",
    "    SAL   = (10 ** ((3.562 - np.log10(x)) / 0.955)) / 1000.0\n",
    "    print(\"SAL =\", SAL)\n",
    " \n",
    "    return SAL\n",
    "\n",
    "\n",
    "def compute_bdacy(T_F, Rw):\n",
    "    \"\"\"\n",
    "    Bdacy from SCA2006-29 (3) formula you pasted:\n",
    "      Bdacy = (1-0.83*exp(-exp(-2.38+(42.17/TC))/Rw))*(-3.16+1.59*log(TC))**2\n",
    "    \"\"\"\n",
    "    Rw = np.asarray(Rw, dtype=float)\n",
    "    TC = (float(T_F) - 32.0) / 1.8\n",
    "\n",
    "    # guards\n",
    "    TC_safe = max(TC, 1e-6)\n",
    "    Rw_safe = np.clip(Rw, 1e-6, None)\n",
    "\n",
    "    term1 = (1.0 - 0.83 * np.exp(-np.exp(-2.38 + (42.17 / TC_safe)) / Rw_safe))\n",
    "    term2 = (-3.16 + 1.59 * np.log(TC_safe)) ** 2\n",
    "    return term1 * term2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def write_constants_to_file(file_path, *, m_cem, n_sat, Rw, mslope, Bdacy):\n",
    "    \"\"\"\n",
    "    Write Pickett / saturation constants to a text file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = Path(file_path)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(\"# Pickett / Saturation Parameters\\n\")\n",
    "            f.write(\"# -------------------------------\\n\")\n",
    "            f.write(f\"m_cem  : {m_cem:.3f}\\n\")\n",
    "            f.write(f\"n_sat  : {n_sat:.3f}\\n\")\n",
    "            f.write(f\"Rw     : {Rw:.5f}\\n\")\n",
    "            f.write(f\"mslope : {mslope:.3f}\\n\")\n",
    "            f.write(f\"Bdacy  : {Bdacy:.4f}\\n\")\n",
    "\n",
    "        print(f\"[PARAMS] Constants written to {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[PARAMS] Error writing constants: {e}\")\n",
    "\n",
    "\n",
    "def _first_present(cols, candidates):\n",
    "    s = set(cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def first_present(cols, candidates):\n",
    "    return _first_present(cols, candidates)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_dspin(minv, maxv, step, decimals, value, *, keyboard_tracking=False):\n",
    "    sb = QDoubleSpinBox()\n",
    "    #sb = QSlider()\n",
    "    sb.setRange(minv, maxv)\n",
    "    sb.setSingleStep(step)\n",
    "    sb.setDecimals(decimals)\n",
    "\n",
    "    # Key behavior: don’t leave half-typed values hanging\n",
    "    sb.setKeyboardTracking(keyboard_tracking)  # False = updates on commit (Enter/focus-out)\n",
    "\n",
    "    # Optional: nicer typing\n",
    "    sb.setAccelerated(True)\n",
    "\n",
    "    try:\n",
    "        sb.setValue(float(value))\n",
    "    except Exception:\n",
    "        sb.setValue(float(minv))\n",
    "\n",
    "    return sb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Professional float slider classes\n",
    "# =============================================================================\n",
    "\n",
    "from PySide6.QtCore import Qt, Signal, QEvent\n",
    "from PySide6.QtWidgets import (\n",
    "    QWidget, QHBoxLayout, QLabel, QSlider, QInputDialog, QSizePolicy\n",
    ")\n",
    "\n",
    "\n",
    "class FloatSlider(QSlider):\n",
    "    \"\"\"Mac trackpad-friendly slider with SHIFT fine control.\"\"\"\n",
    "    def __init__(self, orientation, parent=None):\n",
    "        super().__init__(orientation, parent)\n",
    "        self._fine_div = 10  # SHIFT => 10x finer\n",
    "\n",
    "    def wheelEvent(self, e):\n",
    "        # angleDelta works for mouse wheels; pixelDelta often used by trackpads on macOS\n",
    "        delta = e.angleDelta().y()\n",
    "        if delta == 0:\n",
    "            delta = e.pixelDelta().y()\n",
    "\n",
    "        if delta == 0:\n",
    "            return\n",
    "\n",
    "        step = self.singleStep()\n",
    "        if e.modifiers() & Qt.ShiftModifier:\n",
    "            step = max(1, step // self._fine_div)\n",
    "\n",
    "        direction = 1 if delta > 0 else -1\n",
    "        self.setValue(self.value() + direction * step)\n",
    "        e.accept()\n",
    "\n",
    "\n",
    "class FloatSliderRow(QWidget):\n",
    "    \"\"\"\n",
    "    Slider row:\n",
    "      - Expanding slider + live value label\n",
    "      - SHIFT + trackpad scroll = fine adjust\n",
    "      - Double-click on slider OR label = type exact value\n",
    "    \"\"\"\n",
    "    valueChangedFloat = Signal(float)\n",
    "\n",
    "    def __init__(self, min_val, max_val, step, decimals, value, parent=None, label_width=70):\n",
    "        super().__init__(parent)\n",
    "\n",
    "        self.decimals = int(decimals)\n",
    "        self.scale = 10 ** self.decimals\n",
    "\n",
    "        self.min_i  = int(round(min_val * self.scale))\n",
    "        self.max_i  = int(round(max_val * self.scale))\n",
    "        self.step_i = max(1, int(round(step * self.scale)))\n",
    "\n",
    "        self.slider = FloatSlider(Qt.Horizontal)\n",
    "        self.slider.setMinimum(self.min_i)\n",
    "        self.slider.setMaximum(self.max_i)\n",
    "        self.slider.setSingleStep(self.step_i)\n",
    "        self.slider.setPageStep(max(1, (self.max_i - self.min_i) // 50))\n",
    "        #self.slider.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)\n",
    "\n",
    "        self.slider.setMinimumWidth(120)\n",
    "        self.slider.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)\n",
    "\n",
    "\n",
    "        # NOTE: we use self.label (not self.value_label)\n",
    "        self.label = QLabel(\"\")\n",
    "        self.label.setFixedWidth(label_width)\n",
    "\n",
    "        lay = QHBoxLayout(self)\n",
    "        lay.setContentsMargins(0, 0, 0, 0)\n",
    "        lay.addWidget(self.slider, stretch=1)\n",
    "        lay.addWidget(self.label, stretch=0)\n",
    "\n",
    "        self.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)\n",
    "\n",
    "        # Trackpad-friendly: capture double-clicks even when they land on child widgets\n",
    "        self.slider.installEventFilter(self)\n",
    "        self.label.installEventFilter(self)\n",
    "\n",
    "        self.set_value(value)\n",
    "\n",
    "        self.slider.valueChanged.connect(self._sync)\n",
    "        self.slider.valueChanged.connect(lambda _: self.valueChangedFloat.emit(self.value()))\n",
    "\n",
    "    def eventFilter(self, obj, event):\n",
    "        # Catch double-click on slider/label and forward to our handler\n",
    "        if event.type() == QEvent.MouseButtonDblClick:\n",
    "            self._prompt_value()\n",
    "            return True\n",
    "        return super().eventFilter(obj, event)\n",
    "\n",
    "    def _sync(self, v_i: int):\n",
    "        self.label.setText(f\"{v_i / self.scale:.{self.decimals}f}\")\n",
    "\n",
    "    def value(self) -> float:\n",
    "        return self.slider.value() / self.scale\n",
    "\n",
    "    def set_value(self, v: float):\n",
    "        v_i = int(round(v * self.scale))\n",
    "        v_i = max(self.min_i, min(self.max_i, v_i))\n",
    "        self.slider.setValue(v_i)\n",
    "        self._sync(v_i)\n",
    "\n",
    "    def _prompt_value(self):\n",
    "        v0 = self.value()\n",
    "        v, ok = QInputDialog.getDouble(\n",
    "            self,\n",
    "            \"Set value\",\n",
    "            \"Value:\",\n",
    "            v0,\n",
    "            self.min_i / self.scale,\n",
    "            self.max_i / self.scale,\n",
    "            self.decimals,\n",
    "        )\n",
    "        if ok:\n",
    "            self.set_value(v)\n",
    "            self.valueChangedFloat.emit(self.value())\n",
    "\n",
    "\n",
    "\n",
    "def get_col(df: pd.DataFrame, col: str):\n",
    "    \"\"\"Return numeric numpy array for df[col] if present; else None.\"\"\"\n",
    "    if df is None or col is None:\n",
    "        return None\n",
    "    if col not in df.columns:\n",
    "        return None\n",
    "    return pd.to_numeric(df[col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "\n",
    "def _pretty(k: str) -> str:\n",
    "    return k.replace(\"_\", \" \").title()\n",
    "\n",
    "\n",
    "def vsh_from_gr(gr, gr_clean=25.0, gr_shale=160.0, clip=True):\n",
    "    \"\"\"\n",
    "    Linear Vsh from gamma ray (GR or CGR) endpoints.\n",
    "    Returns numpy array, NaN where gr is not finite.\n",
    "    \"\"\"\n",
    "    gr = np.asarray(gr, dtype=float)\n",
    "    denom = (gr_shale - gr_clean)\n",
    "    if denom == 0:\n",
    "        v = np.full_like(gr, np.nan, dtype=float)\n",
    "    else:\n",
    "        v = (gr - gr_clean) / denom\n",
    "\n",
    "    if clip:\n",
    "        v = np.clip(v, 0.0, 1.0)\n",
    "\n",
    "    v[~np.isfinite(gr)] = np.nan\n",
    "    return v\n",
    "\n",
    "\n",
    "def vsh_from_gamma(df: pd.DataFrame, params: dict):\n",
    "    \"\"\"\n",
    "    Prefer CGR if available, else GR, then compute VSH_GR using vsh_from_gr().\n",
    "    Returns: (vsh_array or None, gamma_col_used or None)\n",
    "    \"\"\"\n",
    "    # 1) honor curve-picker selections\n",
    "    gamma_col = params.get(\"cgr_curve\")\n",
    "    if not gamma_col or gamma_col not in df.columns:\n",
    "        gamma_col = params.get(\"gr_curve\")\n",
    "\n",
    "    # 2) fallback list (CGR first)\n",
    "    if not gamma_col or gamma_col not in df.columns:\n",
    "        gamma_col = _first_present(df.columns, [\"HCGR\", \"CGR\", \"ECGR\", \"GR_EDTC\", \"HSGR\", \"GR\", \"SGR\", \"HGR\"])\n",
    "\n",
    "    if not gamma_col or gamma_col not in df.columns:\n",
    "        return None, None\n",
    "\n",
    "    gr_clean = float(params.get(\"gr_clean\", 25.0))\n",
    "    gr_shale = float(params.get(\"gr_shale\", 160.0))\n",
    "\n",
    "    g = pd.to_numeric(df[gamma_col], errors=\"coerce\").to_numpy(float)\n",
    "    return vsh_from_gr(g, gr_clean=gr_clean, gr_shale=gr_shale, clip=True), gamma_col\n",
    "\n",
    "\n",
    "def vsh_from_triangle(x, y, A, B, C, clip=True):\n",
    "    \"\"\"\n",
    "    Vsh = barycentric weight of shale vertex B for points (x,y)\n",
    "    A=matrix, B=shale, C=fluid.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "\n",
    "    out = np.full_like(x, np.nan, dtype=float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if not np.any(m):\n",
    "        return out\n",
    "\n",
    "    Ax, Ay = A\n",
    "    Bx, By = B\n",
    "    Cx, Cy = C\n",
    "\n",
    "    denom = (By - Cy) * (Ax - Cx) + (Cx - Bx) * (Ay - Cy)\n",
    "    if not np.isfinite(denom) or abs(denom) < 1e-12:\n",
    "        return out\n",
    "\n",
    "    # barycentric weight for shale vertex B\n",
    "    lamB = ((Cy - Ay) * (x[m] - Cx) + (Ax - Cx) * (y[m] - Cy)) / denom\n",
    "    if clip:\n",
    "        lamB = np.clip(lamB, 0.0, 1.0)\n",
    "\n",
    "    out[m] = lamB\n",
    "    return out\n",
    "\n",
    "\n",
    "def vsh_from_nd_triangle(df: pd.DataFrame, params: dict):\n",
    "    \"\"\"\n",
    "    VSH from neutron-density triangle using barycentric shale weight.\n",
    "    Uses params:\n",
    "      neut_matrix, den_matrix, neut_shale, den_shale, neut_fl, den_fl\n",
    "    Curve choices:\n",
    "      tnph_curve, rhob_curve\n",
    "    \"\"\"\n",
    "    tnph = params.get(\"tnph_curve\")\n",
    "    rhob = params.get(\"rhob_curve\")\n",
    "\n",
    "    if (not tnph) or (tnph not in df.columns):\n",
    "        tnph = _first_present(df.columns, [\"TNPH\", \"NPHI\", \"CNL\", \"NPOR\"])\n",
    "    if (not rhob) or (rhob not in df.columns):\n",
    "        rhob = _first_present(df.columns, [\"RHOZ\", \"RHOB\"])\n",
    "\n",
    "    if (tnph is None) or (rhob is None):\n",
    "        return None\n",
    "\n",
    "    x = pd.to_numeric(df[tnph], errors=\"coerce\").to_numpy(float)\n",
    "    y = pd.to_numeric(df[rhob], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "    A = (float(params.get(\"neut_matrix\", -0.04)), float(params.get(\"den_matrix\", 2.65)))\n",
    "    B = (float(params.get(\"neut_shale\", 0.49)),   float(params.get(\"den_shale\", 2.65)))\n",
    "    C = (float(params.get(\"neut_fl\", 1.0)),       float(params.get(\"den_fl\", 1.10)))\n",
    "\n",
    "    return vsh_from_triangle(x, y, A, B, C, clip=True)\n",
    "\n",
    "\n",
    "def vsh_from_dt_triangle(df: pd.DataFrame, params: dict):\n",
    "    \"\"\"\n",
    "    VSH from DT-RHOB triangle using barycentric shale weight.\n",
    "    Uses params:\n",
    "      dt_matrix, den_matrix, dt_shale, den_shale, dt_fl, den_fl\n",
    "    Curve choices:\n",
    "      dtco_curve, rhob_curve\n",
    "    \"\"\"\n",
    "    rhob = params.get(\"rhob_curve\")\n",
    "    if (not rhob) or (rhob not in df.columns):\n",
    "        rhob = _first_present(df.columns, [\"RHOZ\", \"RHOB\"])\n",
    "\n",
    "    dtco = params.get(\"dtco_curve\")\n",
    "    if (not dtco) or (dtco not in df.columns):\n",
    "        dtco = _first_present(df.columns, [\"DTCO\", \"DTC\", \"AC\"])\n",
    "\n",
    "    if (rhob is None) or (dtco is None):\n",
    "        return None\n",
    "\n",
    "    # x=DT, y=RHOB to match your crossplot axes\n",
    "    x = pd.to_numeric(df[dtco], errors=\"coerce\").to_numpy(float)\n",
    "    y = pd.to_numeric(df[rhob], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "    A = (float(params.get(\"dt_matrix\", 55.5)), float(params.get(\"den_matrix\", 2.65)))\n",
    "    B = (float(params.get(\"dt_shale\", 90.0)),  float(params.get(\"den_shale\", 2.65)))\n",
    "    C = (float(params.get(\"dt_fl\", 188.0)),    float(params.get(\"den_fl\", 1.10)))\n",
    "\n",
    "    return vsh_from_triangle(x, y, A, B, C, clip=True)\n",
    "\n",
    "\n",
    "def compute_sw_products(\n",
    "    df: pd.DataFrame,\n",
    "    rt_col: str,\n",
    "    phit_col: str,\n",
    "    qv_col: str | None,\n",
    "    vsh_col: str | None,\n",
    "    m_cem: float,\n",
    "    n_sat: float,\n",
    "    rw: float,\n",
    "    mslope: float,\n",
    "    B: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Vectorized SW/BVW/BVO + MSTAR_APP + fitted MSTAR line.\n",
    "    Returns dict of numpy arrays aligned to df.index.\n",
    "    \"\"\"\n",
    "    rt   = pd.to_numeric(df[rt_col], errors=\"coerce\").to_numpy(float)\n",
    "    phit = pd.to_numeric(df[phit_col], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "    if qv_col and (qv_col in df.columns):\n",
    "        qv = pd.to_numeric(df[qv_col], errors=\"coerce\").to_numpy(float)\n",
    "    else:\n",
    "        qv = np.zeros_like(phit)\n",
    "\n",
    "    if vsh_col and (vsh_col in df.columns):\n",
    "        vsh = pd.to_numeric(df[vsh_col], errors=\"coerce\").to_numpy(float)\n",
    "    else:\n",
    "        vsh = np.full_like(phit, np.nan)\n",
    "\n",
    "    n = len(df)\n",
    "    BVW = np.full(n, np.nan, float)\n",
    "    SWT = np.full(n, np.nan, float)\n",
    "    BVO = np.full(n, np.nan, float)\n",
    "    MSTAR_APP = np.full(n, np.nan, float)\n",
    "    MSTAR = np.full(n, np.nan, float)\n",
    "\n",
    "    m = np.isfinite(rt) & np.isfinite(phit) & (rt > 0) & (phit > 0)\n",
    "\n",
    "    # Archie-style BVW\n",
    "    BVW_m = phit[m] * ((1.0 / (phit[m] ** m_cem)) * (rw / rt[m])) ** (1.0 / n_sat)\n",
    "    BVW_m = np.minimum(BVW_m, phit[m])  # clamp BVW <= PHIT\n",
    "\n",
    "    SWT_m = np.where(phit[m] > 0, BVW_m / phit[m], np.nan)\n",
    "    BVO_m = phit[m] * (1.0 - SWT_m)\n",
    "\n",
    "    # Apparent M*:\n",
    "    # MSTARAPP = log10(rw/(Rt*(1+rw*B*Qv))) / log10(PHIT)\n",
    "    denom = rt[m] * (1.0 + rw * B * qv[m])\n",
    "    ok = np.isfinite(denom) & (denom > 0) & np.isfinite(phit[m]) & (phit[m] > 0) & (phit[m] != 1.0)\n",
    "\n",
    "    ms_app = np.full(np.sum(m), np.nan, float)\n",
    "    \n",
    "    ms_app[ok] = np.log10(rw / denom[ok]) / np.log10(phit[m][ok])\n",
    "\n",
    "    BVW[m] = BVW_m\n",
    "    SWT[m] = SWT_m\n",
    "    BVO[m] = BVO_m\n",
    "    MSTAR_APP[m] = ms_app\n",
    "\n",
    "    # fitted M* line\n",
    "    if np.any(np.isfinite(vsh)):\n",
    "        MSTAR = vsh * mslope + m_cem\n",
    "\n",
    "    return dict(BVW=BVW, SWT=SWT, BVO=BVO, MSTAR_APP=MSTAR_APP, MSTAR=MSTAR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ParamStore:\n",
    "    \"\"\"\n",
    "    Read/write params from controller.state.params and persist via QSettings.\n",
    "    \"\"\"\n",
    "    def __init__(self, controller, org=\"petro_suite3\", app=\"merge_gui\"):\n",
    "        self.controller = controller\n",
    "        self.settings = QSettings(org, app)\n",
    "\n",
    "    def get(self, key, default=None):\n",
    "        p = getattr(self.controller.state, \"params\", None)\n",
    "        if not isinstance(p, dict):\n",
    "            self.controller.state.params = {}\n",
    "            p = self.controller.state.params\n",
    "        if key in p:\n",
    "            return p[key]\n",
    "        return self.settings.value(key, default)\n",
    "\n",
    "    def set(self, key, value):\n",
    "        p = getattr(self.controller.state, \"params\", None)\n",
    "        if not isinstance(p, dict):\n",
    "            self.controller.state.params = {}\n",
    "            p = self.controller.state.params\n",
    "        p[key] = value\n",
    "        self.settings.setValue(key, value)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PlotsPanel\n",
    "# =============================================================================\n",
    "class PlotsPanel(QWidget):\n",
    "    def __init__(self, controller):\n",
    "        super().__init__()\n",
    "        self.controller = controller\n",
    "        self.param_store = ParamStore(controller)\n",
    "\n",
    "        self.tabs = QTabWidget()\n",
    "\n",
    "        self.settings = QSettings(\"CrestedButtePetro\", \"PetroSuite6\")\n",
    "        \n",
    "        # -------------------------\n",
    "        # Depth tracks (PyQtGraph)\n",
    "        # -------------------------\n",
    "        self.tracks = pg.GraphicsLayoutWidget()\n",
    "        self.tabs.addTab(self.tracks, \"Depth Tracks\")\n",
    "        self.track_items = {}\n",
    "        self._build_tracks()\n",
    "\n",
    "        # -------------------------\n",
    "        # Matplotlib figures/canvases (create ONCE)\n",
    "        # -------------------------\n",
    "        self.fig_nd, self.ax_nd = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        self.canvas_nd = FigureCanvas(self.fig_nd)\n",
    "\n",
    "        self.fig_gr, self.ax_gr = plt.subplots(1, 1, figsize=(6, 3))\n",
    "        self.canvas_gr = FigureCanvas(self.fig_gr)\n",
    "\n",
    "        self.fig_dt, self.ax_dt = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        self.canvas_dt = FigureCanvas(self.fig_dt)\n",
    "\n",
    "        # SW tab figure (Matplotlib) 2x2 mosaic\n",
    "        self.fig_sw = plt.figure(figsize=(11, 7))\n",
    "        axs = self.fig_sw.subplot_mosaic(\n",
    "            [\n",
    "                [\"left\",   \"middle\"],\n",
    "                [\"right\",  \"cbw\"   ],\n",
    "            ]\n",
    "        )\n",
    "        self.ax_sw_left  = axs[\"left\"]    # BVW/BVO depth plot\n",
    "        self.ax_sw_mid   = axs[\"middle\"]  # Pickett\n",
    "        self.ax_sw_right = axs[\"right\"]   # Vsh vs Mstar\n",
    "        self.ax_sw_cbw   = axs[\"cbw\"]     # Vsh_HL vs CBW\n",
    "        self.canvas_sw = FigureCanvas(self.fig_sw)\n",
    "\n",
    "        # -------------------------\n",
    "        # Tabs (canvas + controls)\n",
    "        # -------------------------\n",
    "        self.tabs.addTab(self._build_crossplots_tab(), \"Crossplots + Histogram\")\n",
    "        self.tabs.addTab(self._build_sw_tab(), \"Sw / Pickett / M* / CBW\")\n",
    "\n",
    "        layout = QVBoxLayout(self)\n",
    "        layout.addWidget(self.tabs)\n",
    "        \n",
    "        self._sw_recompute_timer = QTimer(self)\n",
    "        self._sw_recompute_timer.setSingleShot(True)\n",
    "        self._sw_recompute_timer.timeout.connect(self._on_compute_sw_clicked)\n",
    "\n",
    "        self._vsh_recompute_timer = QTimer()\n",
    "        self._vsh_recompute_timer.setSingleShot(True)\n",
    "        \n",
    "        def _do_vsh_live_update():\n",
    "            try:\n",
    "                if hasattr(self, \"_recompute_vsh_only\"):\n",
    "                    self._recompute_vsh_only()\n",
    "                fn = getattr(self, \"_refresh_vsh_depth_plot\", None)\n",
    "                if callable(fn):\n",
    "                    fn()\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        self._vsh_recompute_timer.timeout.connect(_do_vsh_live_update)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Public API\n",
    "    # =============================================================================\n",
    "    def update_all(self, state):\n",
    "        self.update_depth_plot(state)\n",
    "        self.update_nd_crossplot(state)\n",
    "        self.update_gr_hist(state)\n",
    "        self.update_dt_rhob_crossplot(state)\n",
    "        self.update_sw_tab(state)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Param update\n",
    "    # =============================================================================\n",
    "    def _set_param_and_refresh(self, key, value):\n",
    "        self.param_store.set(key, value)\n",
    "        # keep state.params in sync with param_store (so live recompute sees slider changes)\n",
    "        try:\n",
    "            state = getattr(self.controller, \"state\", None)\n",
    "            if state is not None:\n",
    "                params = getattr(state, \"params\", None)\n",
    "                if isinstance(params, dict):\n",
    "                    params[key] = value\n",
    "                else:\n",
    "                    state.params = {key: value}\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        # If SW/M* parameters change, recompute derived curves (debounced for sliders)\n",
    "        if key.startswith(\"sw.\") or key == \"cbw_intercept\":\n",
    "            if key in (\"sw.rw\", \"sw.B\", \"sw.m_cem\", \"sw.n_sat\", \"sw.mslope\", \"cbw_intercept\"):\n",
    "                # debounce recompute to avoid hammering while user drags a slider\n",
    "                if hasattr(self, \"_sw_recompute_timer\") and self._sw_recompute_timer is not None:\n",
    "                    self._sw_recompute_timer.start(150)  # ms\n",
    "                else:\n",
    "                    self._on_compute_sw_clicked()\n",
    "    \n",
    "        if hasattr(self.controller, \"update_plots\"):\n",
    "            self.controller.update_plots()\n",
    "        else:\n",
    "            self.controller.refresh_plots()\n",
    "\n",
    "        # keep the VSH depth plot in sync\n",
    "        try:\n",
    "            self._refresh_vsh_depth_plot()\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        # --- keep VSH depth plot in sync (crossplots tab)\n",
    "        try:\n",
    "            fn = getattr(self, \"_refresh_vsh_depth_plot\", None)\n",
    "            if callable(fn):\n",
    "                fn()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # --- Live VSH update when adjusting crossplot parameters\n",
    "        try:\n",
    "            tabs = getattr(self, \"tabs\", None)\n",
    "            on_cross = True\n",
    "            if tabs is not None:\n",
    "                tab_name = tabs.tabText(tabs.currentIndex()).strip().lower()\n",
    "                on_cross = (\"cross\" in tab_name)\n",
    "\n",
    "            if on_cross and hasattr(self, \"_recompute_vsh_only\"):\n",
    "                self._recompute_vsh_only()\n",
    "\n",
    "            fn = getattr(self, \"_refresh_vsh_depth_plot\", None)\n",
    "            if callable(fn):\n",
    "                fn()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def _on_export_df_clicked(self):\n",
    "    \n",
    "        state = getattr(self, \"state\", None)\n",
    "        if state is None:\n",
    "            QMessageBox.warning(self, \"Export DF\", \"State object not found.\")\n",
    "            return\n",
    "    \n",
    "        # --- Prefer depth-windowed view ---\n",
    "        df = getattr(state, \"analysis_df_view\", None)\n",
    "        if df is None or df.empty:\n",
    "            df = getattr(state, \"analysis_df\", None)\n",
    "    \n",
    "        if df is None or df.empty:\n",
    "            QMessageBox.warning(self, \"Export DF\", \"No dataframe available to export.\")\n",
    "            return\n",
    "    \n",
    "        # --- Ask where to save ---\n",
    "        file_name, _ = QFileDialog.getSaveFileName(\n",
    "            self,\n",
    "            \"Export DataFrame\",\n",
    "            \"qt_export.csv\",\n",
    "            \"CSV Files (*.csv);;Excel Files (*.xlsx)\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        print(\"EXPORT state id:\", id(state))\n",
    "\n",
    "        if not file_name:\n",
    "            return\n",
    "    \n",
    "        try:\n",
    "            out = df.copy()\n",
    "    \n",
    "            # Insert depth as first column\n",
    "            if out.index.name is None:\n",
    "                out.insert(0, \"DEPT\", out.index.astype(float))\n",
    "            else:\n",
    "                out.insert(0, out.index.name, out.index.astype(float))\n",
    "    \n",
    "            if file_name.endswith(\".xlsx\"):\n",
    "                out.to_excel(file_name, index=False)\n",
    "            else:\n",
    "                out.to_csv(file_name, index=False)\n",
    "    \n",
    "            QMessageBox.information(self, \"Export DF\", f\"Saved:\\n{file_name}\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            QMessageBox.critical(self, \"Export DF\", str(e))\n",
    "    \n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Buttons\n",
    "    # =============================================================================\n",
    "    def _recompute_swb_only(self):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "    \n",
    "        state = getattr(self.controller, \"state\", None)\n",
    "        if state is None:\n",
    "            return\n",
    "    \n",
    "        df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            return\n",
    "    \n",
    "        # If already present, don’t redo (optional)\n",
    "        if any(c.strip().upper() == \"SWB\" for c in df.columns):\n",
    "            return\n",
    "    \n",
    "        cbw_col  = next((c for c in [\"CBW\"] if c in df.columns), None)\n",
    "        phit_col = next((c for c in [\"PHIT_NMR\", \"TCMR\", \"PHIT\", \"NMR_PHIT\"] if c in df.columns), None)\n",
    "        if not (cbw_col and phit_col):\n",
    "            return\n",
    "    \n",
    "        cbw  = pd.to_numeric(df[cbw_col], errors=\"coerce\").to_numpy(dtype=float, copy=True)\n",
    "        phit = pd.to_numeric(df[phit_col], errors=\"coerce\").to_numpy(dtype=float, copy=True)\n",
    "    \n",
    "        swb = np.full_like(phit, np.nan, dtype=float)\n",
    "        m = np.isfinite(cbw) & np.isfinite(phit) & (phit > 0.01)\n",
    "        swb[m] = cbw[m] / phit[m]\n",
    "        swb = np.clip(swb, 0.0, 1.0)\n",
    "    \n",
    "        # ---------------------------------------------------\n",
    "        # Robust median filter (removes spikes)\n",
    "        # ---------------------------------------------------\n",
    "    \n",
    "        window = 3   # must be odd; adjust based on sample rate\n",
    "    \n",
    "        swb_series = pd.Series(swb)\n",
    "    \n",
    "        swb_med = swb_series.rolling(\n",
    "            window=window,\n",
    "            center=True,\n",
    "            min_periods=1\n",
    "        ).median()\n",
    "    \n",
    "        # ---------------------------------------------------\n",
    "        # Optional light mean smoothing (cosmetic)\n",
    "        # ---------------------------------------------------\n",
    "    \n",
    "        swb_smooth = swb_med.rolling(\n",
    "            window=3,\n",
    "            center=True,\n",
    "            min_periods=1\n",
    "        ).mean()\n",
    "    \n",
    "        df[\"SWB\"] = swb_smooth.to_numpy(dtype=float)\n",
    "    \n",
    "\n",
    "    \n",
    "    def _recompute_vsh_only(self):\n",
    "        \"\"\"Recompute VSH_GR / VSH_ND / VSH_DT / VSH_HL using current params, no button needed.\"\"\"\n",
    "        state = getattr(self.controller, \"state\", None)\n",
    "        if state is None:\n",
    "            return\n",
    "    \n",
    "        df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            return\n",
    "    \n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "    \n",
    "        # ---- VSH from Gamma (CGR preferred)\n",
    "        vsh_gr, gamma_used = vsh_from_gamma(df, params)\n",
    "        if vsh_gr is not None:\n",
    "            df[\"VSH_GR\"] = pd.to_numeric(vsh_gr, errors=\"coerce\")\n",
    "    \n",
    "        # ---- VSH_ND\n",
    "        vsh_nd = vsh_from_nd_triangle(df, params)\n",
    "        if vsh_nd is not None:\n",
    "            df[\"VSH_ND\"] = pd.to_numeric(vsh_nd, errors=\"coerce\")\n",
    "    \n",
    "        # ---- VSH_DT\n",
    "        vsh_dt = vsh_from_dt_triangle(df, params)\n",
    "        if vsh_dt is not None:\n",
    "            df[\"VSH_DT\"] = pd.to_numeric(vsh_dt, errors=\"coerce\")\n",
    "    \n",
    "        # ---- HL combine (use finite-only)\n",
    "        base_cols = [c for c in [\"VSH_GR\", \"VSH_ND\", \"VSH_DT\"] if c in df.columns]\n",
    "        good_cols = []\n",
    "        for c in base_cols:\n",
    "            arr = pd.to_numeric(df[c], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            if np.isfinite(arr).any():\n",
    "                good_cols.append(c)\n",
    "    \n",
    "        if len(good_cols) == 0:\n",
    "            hl = np.full(len(df), np.nan, dtype=float)\n",
    "        elif len(good_cols) == 1:\n",
    "            hl = pd.to_numeric(df[good_cols[0]], errors=\"coerce\").to_numpy(dtype=float)\n",
    "        elif len(good_cols) == 2:\n",
    "            a = pd.to_numeric(df[good_cols[0]], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            b = pd.to_numeric(df[good_cols[1]], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            hl = 0.5 * (a + b)\n",
    "        else:\n",
    "            a, b, c = (pd.to_numeric(df[col], errors=\"coerce\").to_numpy(dtype=float) for col in good_cols[:3])\n",
    "            P = np.stack([0.5 * (a + b), 0.5 * (a + c), 0.5 * (b + c)], axis=0)\n",
    "            hl = np.nanmedian(P, axis=0)\n",
    "    \n",
    "        hl = np.clip(pd.to_numeric(hl, errors=\"coerce\"), 0.0, 1.0)\n",
    "        if \"VSH_GR\" in df.columns:\n",
    "            hl = pd.Series(hl, index=df.index).fillna(pd.to_numeric(df[\"VSH_GR\"], errors=\"coerce\")).to_numpy(dtype=float)\n",
    "    \n",
    "        df[\"VSH_HL\"] = hl\n",
    "    \n",
    "        # write back\n",
    "        state.analysis_df = df\n",
    "    \n",
    "        # keep view in sync (if you have a separate view df)\n",
    "        dfV = getattr(state, \"analysis_df_view\", None)\n",
    "        if dfV is not None and not dfV.empty:\n",
    "            vsh_cols = [c for c in df.columns if \"VSH\" in c.upper()]\n",
    "            for c in vsh_cols:\n",
    "                dfV[c] = pd.to_numeric(df[c], errors=\"coerce\").reindex(dfV.index)\n",
    "            state.analysis_df_view = dfV\n",
    "\n",
    "        self._recompute_swb_only()\n",
    "\n",
    "\n",
    "\n",
    "    def _on_compute_vsh_clicked(self):\n",
    "        state = getattr(self.controller, \"state\", None)\n",
    "        if state is None:\n",
    "            return\n",
    "    \n",
    "        df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            return\n",
    "    \n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "    \n",
    "        # ---- VSH from Gamma (CGR preferred)\n",
    "        vsh_gr, gamma_used = vsh_from_gamma(df, params)\n",
    "        if vsh_gr is not None:\n",
    "            df[\"VSH_GR\"] = pd.to_numeric(vsh_gr, errors=\"coerce\")\n",
    "            print(f\"[VSH] gamma used -> {gamma_used}\")\n",
    "        else:\n",
    "            # ensure column doesn't linger from a previous run\n",
    "            if \"VSH_GR\" in df.columns:\n",
    "                df[\"VSH_GR\"] = pd.to_numeric(df[\"VSH_GR\"], errors=\"coerce\")\n",
    "    \n",
    "        # ---- VSH_ND (only if TNPH + RHOB exist)\n",
    "        vsh_nd = vsh_from_nd_triangle(df, params)\n",
    "        if vsh_nd is not None:\n",
    "            df[\"VSH_ND\"] = pd.to_numeric(vsh_nd, errors=\"coerce\")\n",
    "    \n",
    "        # ---- VSH_DT (only if RHOB + DT exist)\n",
    "        vsh_dt = vsh_from_dt_triangle(df, params)\n",
    "        if vsh_dt is not None:\n",
    "            df[\"VSH_DT\"] = pd.to_numeric(vsh_dt, errors=\"coerce\")\n",
    "    \n",
    "        # ---- Robust HL combine (median of pairwise averages), but only using usable curves\n",
    "        base_cols = [c for c in [\"VSH_GR\", \"VSH_ND\", \"VSH_DT\"] if c in df.columns]\n",
    "    \n",
    "        good_cols = []\n",
    "        for c in base_cols:\n",
    "            arr = pd.to_numeric(df[c], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            if np.isfinite(arr).any():\n",
    "                good_cols.append(c)\n",
    "    \n",
    "        print(\"[VSH_HL] candidates:\", base_cols)\n",
    "        print(\"[VSH_HL] usable (finite):\", good_cols)\n",
    "    \n",
    "        if len(good_cols) == 0:\n",
    "            df[\"VSH_HL\"] = np.nan\n",
    "    \n",
    "        elif len(good_cols) == 1:\n",
    "            df[\"VSH_HL\"] = pd.to_numeric(df[good_cols[0]], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    \n",
    "        elif len(good_cols) == 2:\n",
    "            a = pd.to_numeric(df[good_cols[0]], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            b = pd.to_numeric(df[good_cols[1]], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            df[\"VSH_HL\"] = 0.5 * (a + b)\n",
    "    \n",
    "        else:\n",
    "            a, b, c = (pd.to_numeric(df[col], errors=\"coerce\").to_numpy(dtype=float) for col in good_cols[:3])\n",
    "            P = np.stack([0.5 * (a + b), 0.5 * (a + c), 0.5 * (b + c)], axis=0)\n",
    "            df[\"VSH_HL\"] = np.nanmedian(P, axis=0)\n",
    "    \n",
    "        # clip to [0,1]\n",
    "        df[\"VSH_HL\"] = np.clip(pd.to_numeric(df[\"VSH_HL\"], errors=\"coerce\").to_numpy(dtype=float), 0.0, 1.0)\n",
    "    \n",
    "        # fill remaining NaNs from GR if available\n",
    "        if \"VSH_GR\" in df.columns:\n",
    "            df[\"VSH_HL\"] = pd.Series(df[\"VSH_HL\"], index=df.index).fillna(\n",
    "                pd.to_numeric(df[\"VSH_GR\"], errors=\"coerce\")\n",
    "            ).to_numpy(dtype=float)\n",
    "    \n",
    "        # ---- write back\n",
    "        state.analysis_df = df\n",
    "    \n",
    "        # ---- ensure view gets the new columns BEFORE plotting\n",
    "        try:\n",
    "            if hasattr(self.controller, \"rebuild_view\"):\n",
    "                self.controller.rebuild_view()\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        # If your app keeps a separate df view object, mirror VSH columns into it\n",
    "        try:\n",
    "            dfA = getattr(state, \"analysis_df\", None)\n",
    "            dfV = getattr(state, \"analysis_df_view\", None)\n",
    "            if dfA is not None and dfV is not None and not dfV.empty:\n",
    "                vsh_cols = [c for c in dfA.columns if \"VSH\" in c.upper()]\n",
    "                for c in vsh_cols:\n",
    "                    dfV[c] = pd.to_numeric(dfA[c], errors=\"coerce\").reindex(dfV.index)\n",
    "                state.analysis_df_view = dfV\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        # ---- refresh UI plots\n",
    "        try:\n",
    "            self.controller.refresh_plots()\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        # ---- refresh VSH depth plot (single call)\n",
    "        try:\n",
    "            fn = getattr(self, \"_refresh_vsh_depth_plot\", None)\n",
    "            if callable(fn):\n",
    "                fn()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _on_compute_sw_clicked(self):\n",
    "        w = self.focusWidget()\n",
    "        if w is not None:\n",
    "            try:\n",
    "                w.clearFocus()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        state = getattr(self.controller, \"state\", None)\n",
    "        if state is None:\n",
    "            return\n",
    "\n",
    "        df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            return\n",
    "\n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "\n",
    "        # --- Resolve Rt curve\n",
    "        rt_col = params.get(\"rt_curve\")\n",
    "        if (not rt_col) or (rt_col not in df.columns):\n",
    "            rt_col = _first_present(df.columns, [\"AT90\", \"AF90\", \"AO90\", \"ILD\", \"RT\"])\n",
    "\n",
    "        # --- Resolve PHIT curve (allow chartbook/nmr fallback)\n",
    "        phit_col = params.get(\"phit_curve\")\n",
    "        if (not phit_col) or (phit_col not in df.columns):\n",
    "            phit_col = \"PHIT\" if \"PHIT\" in df.columns else _first_present(df.columns, [\"PHIT_CHART\", \"PHIT_NMR\", \"TCMR\", \"MPHIS\"])\n",
    "\n",
    "        if (rt_col is None) or (phit_col is None):\n",
    "            print(\"[SW] Need Rt + PHIT (or PHIT_CHART/PHIT_NMR) to compute SW/BVW/M*\")\n",
    "            return\n",
    "\n",
    "        # --- Optional Qv\n",
    "        qv_col = \"Qv\" if \"Qv\" in df.columns else (\"QV\" if \"QV\" in df.columns else None)\n",
    "\n",
    "        # --- Parameters\n",
    "        m_cem  = float(params.get(\"sw.m_cem\", 2.0))\n",
    "        n_sat  = float(params.get(\"sw.n_sat\", 2.0))\n",
    "        rw     = float(params.get(\"sw.rw\", 0.03))\n",
    "        mslope = float(params.get(\"sw.mslope\", 1.0))\n",
    "        B      = float(params.get(\"sw.B\", 0.0))\n",
    "\n",
    "        # Vsh column for fitted M*\n",
    "        vsh_col = \"VSH_HL\" if \"VSH_HL\" in df.columns else None\n",
    "\n",
    "        out = compute_sw_products(\n",
    "            df=df,\n",
    "            rt_col=rt_col,\n",
    "            phit_col=phit_col,\n",
    "            qv_col=qv_col,\n",
    "            vsh_col=vsh_col,\n",
    "            m_cem=m_cem,\n",
    "            n_sat=n_sat,\n",
    "            rw=rw,\n",
    "            mslope=mslope,\n",
    "            B=B,\n",
    "        )\n",
    "\n",
    "        df[\"PHIT_USED\"] = pd.to_numeric(df[phit_col], errors=\"coerce\")  # debug\n",
    "        df[\"BVW\"] = out[\"BVW\"]\n",
    "        df[\"SWT\"] = out[\"SWT\"]\n",
    "        df[\"BVO\"] = out[\"BVO\"]\n",
    "        df[\"MSTAR_APP\"] = out[\"MSTAR_APP\"]\n",
    "        df[\"MSTAR_FIT\"] = out[\"MSTAR\"]\n",
    "\n",
    "        state.analysis_df = df\n",
    "        if hasattr(self.controller, \"rebuild_view\"):\n",
    "            self.controller.rebuild_view()\n",
    "        self.controller.refresh_plots()\n",
    "\n",
    "\n",
    "    def _on_compute_ws_clicked(self):\n",
    "        # Ensure any active editor commits its text to the widget value\n",
    "        w = self.focusWidget()\n",
    "        if w is not None:\n",
    "            try:\n",
    "                w.clearFocus()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        state = getattr(self.controller, \"state\", None)\n",
    "        if state is None:\n",
    "            return\n",
    "    \n",
    "        df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            return\n",
    "    \n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "    \n",
    "        # --- Resolve Rt\n",
    "        rt_col = params.get(\"rt_curve\")\n",
    "        if (not rt_col) or (rt_col not in df.columns):\n",
    "            for c in [\"AT90\", \"AF90\", \"ILD\", \"RT\"]:\n",
    "                if c in df.columns:\n",
    "                    rt_col = c\n",
    "                    break\n",
    "        if rt_col is None:\n",
    "            print(\"[WS] No Rt curve found\")\n",
    "            return\n",
    "    \n",
    "        # --- Resolve PHIT\n",
    "        phit_col = params.get(\"phit_curve\")\n",
    "        if (not phit_col) or (phit_col not in df.columns):\n",
    "            if \"PHIT\" in df.columns:\n",
    "                phit_col = \"PHIT\"\n",
    "            else:\n",
    "                for c in [\"PHIT_CHART\", \"PHIT_NMR\", \"TCMR\", \"MPHIS\"]:\n",
    "                    if c in df.columns:\n",
    "                        phit_col = c\n",
    "                        break\n",
    "        if phit_col is None:\n",
    "            print(\"[WS] No PHIT curve found\")\n",
    "            return\n",
    "    \n",
    "        # Standardize PHIT\n",
    "        if phit_col != \"PHIT\":\n",
    "            df[\"PHIT\"] = pd.to_numeric(df[phit_col], errors=\"coerce\")\n",
    "            phit_col = \"PHIT\"\n",
    "    \n",
    "        # --- Resolve Qv (optional; if missing -> 0)\n",
    "        qv_col = \"Qv\" if \"Qv\" in df.columns else (\"QV\" if \"QV\" in df.columns else None)\n",
    "\n",
    "\n",
    "        # --- VSH required\n",
    "        vsh_col = \"VSH_HL\" if \"VSH_HL\" in df.columns else None\n",
    "        if vsh_col is None:\n",
    "            print(\"[WS] Need VSH_HL (compute Vsh first) to build CBWapp and MSTAR for Waxman-Smits\")\n",
    "            return\n",
    "        \n",
    "        # --- CBW_Int from SW-tab param \"cbw_intercept\"\n",
    "        CBW_Int = float(params.get(\"cbw_intercept\", 0.15))  # <-- uses your existing UI key\n",
    "        Vsh = pd.to_numeric(df[vsh_col], errors=\"coerce\").to_numpy(float)\n",
    "        Vsh = np.clip(Vsh, 0.0, 1.0)\n",
    "        \n",
    "        df[\"CBWapp\"] = np.clip(Vsh * CBW_Int, 0.0, 1.0)\n",
    "        cbw = \"CBWapp\"\n",
    "       \n",
    "        # --- Params\n",
    "        m_cem  = float(params.get(\"sw.m_cem\", 2.0))     # intercept\n",
    "        n_sat  = float(params.get(\"sw.n_sat\", 2.0))     # saturation exponent\n",
    "        rw     = float(params.get(\"sw.rw\", 0.03))\n",
    "        B      = float(params.get(\"sw.B\", 0.0))\n",
    "        mslope = float(params.get(\"sw.mslope\", 1.0))    # M* slope vs Vsh\n",
    "\n",
    "        # --- recompute SAL, Qv (Hill/Shirley/Klein), and B (Bdacy) because Rw can vary\n",
    "        T_F    = float(params.get(\"sw.T_F\", 150.0))\n",
    "        den_fl = float(params.get(\"sw.den_fl\", 1.0))\n",
    "        \n",
    "        # Rw can be a curve in df or a scalar param\n",
    "        rw_curve = params.get(\"sw.rw_curve\", None)\n",
    "        if isinstance(rw_curve, str) and rw_curve in df.columns:\n",
    "            Rw_arr = pd.to_numeric(df[rw_curve], errors=\"coerce\").to_numpy(float)\n",
    "            Rw_arr = np.where(np.isfinite(Rw_arr), Rw_arr, rw)  # fall back to scalar rw where missing\n",
    "        else:\n",
    "            Rw_arr = np.full(len(df), rw, dtype=float)\n",
    "        \n",
    "        Rw_arr = np.clip(Rw_arr, 1e-6, None)\n",
    "        \n",
    "        # Rw75\n",
    "        Rw75 = ((T_F + 6.77) * Rw_arr) / (75.0 + 6.77)\n",
    "        \n",
    "        # SAL (KPPM)\n",
    "        x = np.clip(Rw75 - 0.0123, 1e-6, None)\n",
    "        SAL = (10 ** ((3.562 - np.log10(x)) / 0.955)) / 1000.0\n",
    "        df[\"SAL\"] = SAL\n",
    "\n",
    "\n",
    "        # Ensure PHIT is sane for ratios\n",
    "        PHIT = pd.to_numeric(df[phit_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "        \n",
    "        # If CBWapp was computed earlier as an array, use that array directly.\n",
    "        # If you stored it in df, coerce it like this:\n",
    "        cbw = pd.to_numeric(df[\"CBWapp\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "        \n",
    "        PHIT = np.clip(PHIT, 0.0, None)\n",
    "        cbw  = np.clip(cbw,  0.0, None)\n",
    "        \n",
    "        PHIT_safe = np.clip(PHIT, 1e-6, None)\n",
    "        Swb = np.clip(cbw / PHIT_safe, 0.0, 1.0)\n",
    "        df[\"Swb\"] = Swb\n",
    "\n",
    "        print(\"Swb range:\", np.nanmin(Swb), np.nanmax(Swb))\n",
    "  \n",
    "        \n",
    "        # Qv from Swb (Hill, Shirley & Klein)\n",
    "        if \"Swb\" in df.columns:\n",
    "            Swb = pd.to_numeric(df[\"Swb\"], errors=\"coerce\").to_numpy(float)\n",
    "            denom = (0.6425 / np.sqrt(np.clip(den_fl * SAL, 1e-12, None)) + 0.22)\n",
    "            df[\"Qv\"] = np.clip(Swb / denom, 0.0, 5.0)\n",
    "            qv_col = \"Qv\"\n",
    "        else:\n",
    "            print(\"[WS] Swb missing -> cannot compute Qv from HSK; using existing Qv/0\")\n",
    "        \n",
    "        # Bdacy array\n",
    "        TC = (T_F - 32.0) / 1.8\n",
    "        TC = max(TC, 1e-6)\n",
    "        term1 = (1.0 - 0.83 * np.exp(-np.exp(-2.38 + (42.17 / TC)) / Rw_arr))\n",
    "        term2 = (-3.16 + 1.59 * np.log(TC)) ** 2\n",
    "        Bdacy_arr = term1 * term2\n",
    "        df[\"Bdacy\"] = Bdacy_arr\n",
    "        \n",
    "        # B is constant → take robust scalar (median)\n",
    "        B = float(np.nanmedian(Bdacy_arr))\n",
    "        params[\"sw.B\"] = B  # optional\n",
    "        print(f\"[WS] Using B (Bdacy median) = {B:.4g}\")\n",
    "\n",
    "        # --- After Bdacy is computed and B chosen\n",
    "        Bdacy_scalar = float(np.nanmedian(df[\"Bdacy\"]))\n",
    "        \n",
    "        # Pull constants from params (already resolved earlier)\n",
    "        m_cem  = float(params.get(\"sw.m_cem\", 2.0))\n",
    "        n_sat  = float(params.get(\"sw.n_sat\", 2.0))\n",
    "        Rw     = float(params.get(\"sw.rw\", 0.03))\n",
    "        mslope = float(params.get(\"sw.mslope\", 1.0))\n",
    "        \n",
    "        # Write constants file\n",
    "        file_path = \"./apps/merge_gui/data/Pickett.txt\"\n",
    "        \n",
    "        write_constants_to_file(\n",
    "            file_path,\n",
    "            m_cem=m_cem,\n",
    "            n_sat=n_sat,\n",
    "            Rw=Rw,\n",
    "            mslope=mslope,\n",
    "            Bdacy=Bdacy_scalar,\n",
    "        )\n",
    "     \n",
    "        # --- arrays\n",
    "        Rt   = pd.to_numeric(df[rt_col], errors=\"coerce\").to_numpy(float)\n",
    "        PHIT = pd.to_numeric(df[phit_col], errors=\"coerce\").to_numpy(float)\n",
    "        Qv   = pd.to_numeric(df[qv_col], errors=\"coerce\").to_numpy(float) if qv_col else np.zeros_like(PHIT)\n",
    "        Vsh  = pd.to_numeric(df[vsh_col], errors=\"coerce\").to_numpy(float)\n",
    "        #cbw  = pd.to_numeric(df[cbw_col], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "        \n",
    "        # --- build MSTAR (your fitted line)\n",
    "        MSTAR = Vsh * mslope + m_cem\n",
    "        df[\"MSTAR\"] = MSTAR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        # --- compute Sw (old iterative)\n",
    "        Sw_cp = waxsmits_it(Rt, PHIT, MSTAR, Qv, rw, B, n_sat=n_sat, n_steps=100)\n",
    "        '''\n",
    "       \n",
    "        \n",
    "        \n",
    "        from petrocore.workflow.waxman_smits import waxman_smits_sw_iterative\n",
    "        # or: from petrocore.workflows.waxman_smits import waxman_smits_sw_iterative  (match your folder name)   \n",
    "        \n",
    "        # --- compute Sw using workflow iterative solver (vectorized, tighter)\n",
    "        Sw_cp = waxman_smits_sw_iterative(\n",
    "            rt=Rt,\n",
    "            phit=PHIT,\n",
    "            qv=Qv,\n",
    "            rw=float(np.nanmedian(Rw_arr)),   # solver currently expects scalar rw; see note below\n",
    "            m=MSTAR,                          # <-- depth-varying cementation exponent (array)\n",
    "            n=n_sat,\n",
    "            B=float(B),\n",
    "            max_iter=60,\n",
    "            tol=1e-6,\n",
    "            sw0=None,\n",
    "        )\n",
    "        Sw_cp = np.clip(Sw_cp, 1e-4, 1.0)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # ensure cbw is not negative (optional, but helps)\n",
    "        cbw = np.clip(cbw, 0.0, None)\n",
    "        \n",
    "        df[\"SW_CP\"]   = Sw_cp\n",
    "        df[\"BVWT_CP\"] = PHIT * Sw_cp\n",
    "        BVWT_CP = df[\"BVWT_CP\"]\n",
    "        #df[\"BVWe_CP\"] = np.clip(df[\"BVWT_CP\"].to_numpy(float) - cbw, 0.0, None)\n",
    "        df[\"BVWe_CP\"] = pd.Series(np.clip(PHIT * Sw_cp - cbw, 0.0, None), index=df.index)\n",
    "        BVWe_CP = df[\"BVWe_CP\"]\n",
    " \n",
    "        PHIT = np.clip(PHIT, 0.0, None)   # before computing PHIE\n",
    "        PHIE = PHIT - cbw\n",
    "        PHIE = np.clip(PHIE, 0.0, None)\n",
    "        PHIE = np.minimum(PHIE, PHIT)     # now PHIT is non-negative, safe\n",
    "        df[\"PHIE\"] = PHIE\n",
    "        df[\"PHIT\"] = PHIT                # also store the cleaned PHIT\n",
    "\n",
    " \n",
    "        print(\"PHIT range:\", np.nanmin(PHIT), np.nanmax(PHIT))\n",
    "        print(\"PHIE range:\", np.nanmin(PHIE), np.nanmax(PHIE))\n",
    "        print(\"CBW range:\", np.nanmin(cbw), np.nanmax(cbw))\n",
    "        print(\"BVWT_CP range:\", np.nanmin(BVWT_CP), np.nanmax(BVWT_CP))\n",
    "        print(\"BVWe_CP range:\", np.nanmin(BVWe_CP), np.nanmax(BVWe_CP))\n",
    "        print(\"MSTAR range:\", np.nanmin(MSTAR), np.nanmax(MSTAR))\n",
    "        print(\"Qv range:\", np.nanmin(Qv), np.nanmax(Qv))\n",
    "        \n",
    "        \n",
    "        state.analysis_df = df\n",
    "    \n",
    "        # keep view in sync\n",
    "        if hasattr(self.controller, \"rebuild_view\"):\n",
    "            self.controller.rebuild_view()\n",
    "    \n",
    "        # refresh\n",
    "        if hasattr(self.controller, \"update_plots\"):\n",
    "            self.controller.update_plots()\n",
    "        else:\n",
    "            self.controller.refresh_plots()\n",
    "\n",
    "\n",
    "    \n",
    "    # ✅ ADD IT HERE\n",
    "    def _on_final_zone_plot_clicked(self):\n",
    "        state = getattr(self.controller, \"state\", None)\n",
    "        if state is None:\n",
    "            return\n",
    "\n",
    "        df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            print(\"[ZonePlot] No analysis_df\")\n",
    "            return\n",
    "\n",
    "        units = getattr(state, \"units_map\", {}) or {}\n",
    "\n",
    "        # depth window (use current view if present)\n",
    "        top  = getattr(state, \"depth_top\", None)\n",
    "        base = getattr(state, \"depth_base\", None)\n",
    "\n",
    "        if top is None or base is None:\n",
    "            print(\"[ZonePlot] No depth window set\")\n",
    "            return\n",
    "\n",
    "        from petrocore.viz.zone_template_plot import launch_zone_plot\n",
    "\n",
    "        title = getattr(state, \"well_name\", \"Zone Plot\")\n",
    "\n",
    "        fig = launch_zone_plot(\n",
    "            df,\n",
    "            units,\n",
    "            top,\n",
    "            base,\n",
    "            title=title,\n",
    "            depth_col=\"DEPT\",\n",
    "            show=True,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Tab builders (Matplotlib + controls)\n",
    "    # =============================================================================\n",
    "    def _build_crossplots_tab(self):\n",
    "        \"\"\"\n",
    "        One tab that contains:\n",
    "          - N–D crossplot (canvas_nd)\n",
    "          - DTCO–RHOB crossplot (canvas_dt)\n",
    "          - GR histogram (canvas_gr)\n",
    "        with one shared control panel.\n",
    "    \n",
    "        Layout requested:\n",
    "          - LEFT: stack 2 crossplots + 1 histogram vertically\n",
    "          - MIDDLE: depth plot showing all VSH* curves + VSH_HL\n",
    "          - RIGHT: one shared control panel\n",
    "        \"\"\"\n",
    "        from PySide6.QtWidgets import QWidget, QHBoxLayout, QVBoxLayout, QFormLayout, QPushButton, QSplitter\n",
    "        from PySide6.QtCore import Qt\n",
    "        import pyqtgraph as pg\n",
    "        import numpy as np\n",
    "    \n",
    "        w = QWidget()\n",
    "        lay = QHBoxLayout(w)\n",
    "    \n",
    "        # ---------------------------------------------------------------------\n",
    "        # LEFT: stacked plots (ND, DT, GR)\n",
    "        # ---------------------------------------------------------------------\n",
    "        plots = QWidget()\n",
    "        vlay = QVBoxLayout(plots)\n",
    "        vlay.setContentsMargins(0, 0, 0, 0)\n",
    "        vlay.setSpacing(6)\n",
    "    \n",
    "        vlay.addWidget(self.canvas_nd, stretch=1)\n",
    "        vlay.addWidget(self.canvas_dt, stretch=1)\n",
    "        vlay.addWidget(self.canvas_gr, stretch=1)\n",
    "    \n",
    "        # ---------------------------------------------------------------------\n",
    "        # MIDDLE: VSH depth plot\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Create once, reuse\n",
    "        if not hasattr(self, \"vsh_depth_plot\") or self.vsh_depth_plot is None:\n",
    "            self.vsh_depth_plot = pg.PlotWidget()\n",
    "            self.vsh_depth_plot.invertY(True)  # depth increases downward\n",
    "            self.vsh_depth_plot.showGrid(x=True, y=True, alpha=0.5)\n",
    "            self.vsh_depth_plot.setLabel(\"bottom\", \"Vsh\")\n",
    "            self.vsh_depth_plot.setLabel(\"left\", \"Depth\")\n",
    "            try:\n",
    "                self.vsh_depth_plot.addLegend(offset=(10, 10))\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "        # Splitter between stacked plots and VSH depth plot\n",
    "        split = QSplitter(Qt.Horizontal)\n",
    "        split.addWidget(plots)\n",
    "        split.addWidget(self.vsh_depth_plot)\n",
    "        split.setStretchFactor(0, 3)  # stacked plots\n",
    "        split.setStretchFactor(1, 2)  # VSH depth plot\n",
    "    \n",
    "        lay.addWidget(split, stretch=5)\n",
    "    \n",
    "        # ---------------------------------------------------------------------\n",
    "        # RIGHT: controls (one panel)\n",
    "        # ---------------------------------------------------------------------\n",
    "        ctrl = QWidget()\n",
    "        form = QFormLayout(ctrl)\n",
    "        form.setFieldGrowthPolicy(QFormLayout.ExpandingFieldsGrow)\n",
    "        lay.addWidget(ctrl, stretch=1)\n",
    "    \n",
    "        # --- Vsh button (same as ND tab)\n",
    "        btn_vsh = QPushButton(\"2) Compute Vsh HL\")\n",
    "        btn_vsh.clicked.connect(self._on_compute_vsh_clicked)\n",
    "        form.addRow(btn_vsh)\n",
    "    \n",
    "        # --- ND triangle controls (same keys you already use)\n",
    "        defaults_nd = dict(\n",
    "            neut_shale=0.32, den_shale=2.65,\n",
    "            neut_matrix=0.02, den_matrix=2.65,\n",
    "            neut_fl=1.0, den_fl=1.1\n",
    "        )\n",
    "        spec_nd = {\n",
    "            \"neut_matrix\": (-0.15, 1.20, 0.01, 3),\n",
    "            \"den_matrix\":  (1.00,  3.00, 0.01, 3),   # removed (redundant with Rho_mat) \n",
    "            \"neut_shale\":  (-0.15, 1.20, 0.01, 3),\n",
    "            \"den_shale\":   (1.00,  3.00, 0.01, 3),   # removed (redundant with Rho_sh)\n",
    "           # \"neut_fl\":     (-0.15, 1.50, 0.01, 3),\n",
    "            # \"den_fl\":      (0.80,  2.00, 0.01, 3),\n",
    "        }\n",
    "        for k, (mn, mx, st, dec) in spec_nd.items():\n",
    "            val = self.param_store.get(k, defaults_nd[k])\n",
    "            row = FloatSliderRow(mn, mx, st, dec, float(val))\n",
    "            row.valueChangedFloat.connect(lambda v, kk=k: self._set_param_and_refresh(kk, float(v)))\n",
    "            form.addRow(_pretty(k), row)\n",
    "    \n",
    "        # --- DT triangle controls (same keys you already use)\n",
    "        for k, label, rng, dflt, step, dec in [\n",
    "            (\"dt_matrix\", \"DT_mat\", (40, 120), 55.5, 0.5, 1),\n",
    "            (\"dt_shale\",  \"DT_sh\",  (40, 140), 90.0, 0.5, 1),\n",
    "            # (\"dt_fl\",     \"DT fluid (us/ft)\",  (100, 300), 188.0, 1.0, 1),\n",
    "            #(\"den_matrix\", \"Rho_mat\", (1.5, 3.0), 2.65, 0.01, 3),\n",
    "            #(\"den_shale\",  \"Rho_sh\",  (1.5, 3.0), 2.65, 0.01, 3),\n",
    "            # (\"den_fl\",    \"RHOB fluid (g/cc)\", (0.8, 2.0), 1.10, 0.01, 3),\n",
    "        ]:\n",
    "            val = float(self.param_store.get(k, dflt))\n",
    "            row = FloatSliderRow(rng[0], rng[1], step, dec, float(val))\n",
    "            row.valueChangedFloat.connect(lambda v, kk=k: self._set_param_and_refresh(kk, float(v)))\n",
    "            form.addRow(label, row)\n",
    "    \n",
    "        # --- GR histogram controls (same keys you already use)\n",
    "        gr_clean = float(self.param_store.get(\"gr_clean\", 25))\n",
    "        gr_shale = float(self.param_store.get(\"gr_shale\", 160))\n",
    "    \n",
    "        row_clean = FloatSliderRow(0, 300, 1, 0, gr_clean, label_width=40)\n",
    "        row_shale = FloatSliderRow(0, 300, 1, 0, gr_shale, label_width=40)\n",
    "    \n",
    "        row_clean.valueChangedFloat.connect(lambda v: self._set_param_and_refresh(\"gr_clean\", float(v)))\n",
    "        row_shale.valueChangedFloat.connect(lambda v: self._set_param_and_refresh(\"gr_shale\", float(v)))\n",
    "    \n",
    "        form.addRow(\"GR cl\", row_clean)\n",
    "        form.addRow(\"GR sh\", row_shale)\n",
    "    \n",
    "        # ---------------------------------------------------------------------\n",
    "        # Local helper to refresh the VSH depth plot (called by your refresh flow)\n",
    "        # ---------------------------------------------------------------------\n",
    "        def _refresh_vsh_depth_plot_local():\n",
    "            import numpy as np\n",
    "            import pandas as pd\n",
    "            import pyqtgraph as pg   # <-- important\n",
    "        \n",
    "            p = getattr(self, \"vsh_depth_plot\", None)\n",
    "            if p is None:\n",
    "                return\n",
    "        \n",
    "            state = getattr(self.controller, \"state\", None)\n",
    "            if state is None:\n",
    "                return\n",
    "        \n",
    "            df = getattr(state, \"analysis_df_view\", None)\n",
    "            if df is None or df.empty:\n",
    "                df = getattr(state, \"analysis_df\", None)\n",
    "            if df is None or df.empty:\n",
    "                return\n",
    "        \n",
    "            # depth\n",
    "            depth_col = None\n",
    "            for c in [\"DEPTH\", \"DEPT\", \"MD\", \"TVD\", \"TVDSS\"]:\n",
    "                if c in df.columns:\n",
    "                    depth_col = c\n",
    "                    break\n",
    "        \n",
    "            if depth_col is not None:\n",
    "                depth = pd.to_numeric(df[depth_col], errors=\"coerce\").to_numpy(dtype=float, copy=True)\n",
    "            else:\n",
    "                depth = pd.to_numeric(df.index, errors=\"coerce\").to_numpy(dtype=float, copy=True)\n",
    "        \n",
    "            good = np.isfinite(depth)\n",
    "            if not good.any():\n",
    "                p.clear()\n",
    "                p.setTitle(\"VSH: depth not numeric\")\n",
    "                return\n",
    "        \n",
    "            d = depth[good].astype(float, copy=True)\n",
    "\n",
    "\n",
    "            \n",
    "            vsh_cols = [c for c in df.columns if \"VSH\" in c.upper()]\n",
    "            if \"VSH_HL\" in vsh_cols:\n",
    "                vsh_cols = [c for c in vsh_cols if c != \"VSH_HL\"] + [\"VSH_HL\"]\n",
    "        \n",
    "            p.clear()\n",
    "            p.showGrid(x=True, y=True, alpha=0.35)\n",
    "        \n",
    "            pen_map = {\n",
    "                \"VSH_GR\": pg.mkPen(\"g\", width=2),\n",
    "                \"VSH_ND\": pg.mkPen(\"w\", width=2),\n",
    "                \"VSH_DT\": pg.mkPen(\"b\", width=2),\n",
    "                \"VSH_HL\": pg.mkPen(\"r\", width=6),\n",
    "            }\n",
    "        \n",
    "            any_plotted = False\n",
    "\n",
    "\n",
    "\n",
    "             # --- Plot Swb/SWB (yellow) ---\n",
    "            swb_col = next((c for c in df.columns if c.strip().upper() == \"SWB\"), None)\n",
    "            \n",
    "            if swb_col is not None:\n",
    "                swb_all = pd.to_numeric(df[swb_col], errors=\"coerce\").to_numpy(dtype=float, copy=True)\n",
    "                swb = swb_all[good]\n",
    "                m = np.isfinite(swb) & np.isfinite(d)\n",
    "                if m.any():\n",
    "                    p.plot(\n",
    "                        np.ascontiguousarray(swb[m], dtype=np.float64),\n",
    "                        np.ascontiguousarray(d[m], dtype=np.float64),\n",
    "                        name=swb_col,  # will show \"Swb\" in legend\n",
    "                        pen=pg.mkPen(\"y\", width=2),\n",
    "                    )\n",
    "                    any_plotted = True\n",
    "            else:\n",
    "                print(\"DEBUG: no Swb/SWB column found. Available:\", [c for c in df.columns if \"SW\" in c.upper()])\n",
    " \n",
    " \n",
    "            \n",
    "            # plot VSH\n",
    "            for col in vsh_cols:\n",
    "                x = pd.to_numeric(df[col], errors=\"coerce\").to_numpy(dtype=float, copy=True)[good]\n",
    "                if np.isfinite(x).any():\n",
    "                    p.plot(x, d, name=col, pen=pen_map.get(col, None))\n",
    "                    any_plotted = True\n",
    "\n",
    "            \n",
    "            if any_plotted:\n",
    "                p.setXRange(0.0, 1.0, padding=0.02)\n",
    "                p.setYRange(float(np.nanmin(d)), float(np.nanmax(d)), padding=0.0)\n",
    "                p.invertY(True)\n",
    "        \n",
    "                        \n",
    "    \n",
    "        self._refresh_vsh_depth_plot = _refresh_vsh_depth_plot_local\n",
    "\n",
    "        # Force an initial draw after the widget is constructed\n",
    "        QTimer.singleShot(0, self._refresh_vsh_depth_plot)\n",
    "\n",
    "\n",
    "        return w\n",
    "    \n",
    "    \n",
    "\n",
    "    def _build_sw_tab(self):\n",
    "        ps = getattr(self, \"param_store\", {}) or {}\n",
    "    \n",
    "        # defaults\n",
    "        m_cem   = float(ps.get(\"sw.m_cem\", 1.9))\n",
    "        n_sat   = float(ps.get(\"sw.n_sat\", 2.0))\n",
    "        rw      = float(ps.get(\"sw.rw\", 0.023))\n",
    "        mslope  = float(ps.get(\"sw.mslope\", 1.0))\n",
    "        cbw_int = float(ps.get(\"cbw_intercept\", 0.15))\n",
    "    \n",
    "        w = QWidget()\n",
    "        lay = QHBoxLayout(w)\n",
    "    \n",
    "        lay.addWidget(self.canvas_sw, stretch=4)\n",
    "    \n",
    "        ctrl = QWidget()\n",
    "        form = QFormLayout(ctrl)\n",
    "        \n",
    "       \n",
    "        # critical: makes the FIELD (slider column) actually grow\n",
    "        form.setFieldGrowthPolicy(QFormLayout.ExpandingFieldsGrow)\n",
    "    \n",
    "        lay.addWidget(ctrl, stretch=1)\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # Red instruction label\n",
    "        # ------------------------------------------\n",
    "        note = QLabel(\n",
    "            \"<b>Parameter Controls</b><br>\"\n",
    "            \"• Drag slider for coarse adjustment<br>\"\n",
    "            \"• Two-finger scroll for fine adjustment<br>\"\n",
    "            \"• SHIFT + scroll for ultra-fine adjustment<br>\"\n",
    "            \"• Double-tap slider or value to enter exact number\"\n",
    "        )\n",
    "        \n",
    "        note.setStyleSheet(\"\"\"\n",
    "            QLabel {\n",
    "                color: red;\n",
    "                font-size: 11px;\n",
    "                background-color: #f8f8f8;\n",
    "                border: 1px solid #dddddd;\n",
    "                border-radius: 4px;\n",
    "                padding: 8px;\n",
    "            }\n",
    "        \"\"\")\n",
    "        \n",
    "        note.setWordWrap(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        form.addRow(note)\n",
    "\n",
    "        \n",
    "    \n",
    "        # ---- rows (Techlog/IP behavior)\n",
    "        row_m = FloatSliderRow(1.00, 3.00, 0.01, 2, m_cem)\n",
    "        row_m.valueChangedFloat.connect(lambda v: self._set_param_and_refresh(\"sw.m_cem\", float(v)))\n",
    "        form.addRow(\"m (cementation)\", row_m)\n",
    "    \n",
    "        row_n = FloatSliderRow(1.00, 3.00, 0.01, 2, n_sat)\n",
    "        row_n.valueChangedFloat.connect(lambda v: self._set_param_and_refresh(\"sw.n_sat\", float(v)))\n",
    "        form.addRow(\"n (saturation)\", row_n)\n",
    "    \n",
    "        row_rw = FloatSliderRow(0.001, 0.20, 0.001, 4, rw)\n",
    "        row_rw.valueChangedFloat.connect(lambda v: self._set_param_and_refresh(\"sw.rw\", float(v)))\n",
    "        form.addRow(\"Rw\", row_rw)\n",
    "    \n",
    "        row_ms = FloatSliderRow(0.01, 4.00, 0.01, 2, mslope)\n",
    "        row_ms.valueChangedFloat.connect(lambda v: self._set_param_and_refresh(\"sw.mslope\", float(v)))\n",
    "        form.addRow(\"M* slope\", row_ms)\n",
    "    \n",
    "        row_cbw = FloatSliderRow(0.0, 0.5, 0.01, 3, cbw_int)\n",
    "        row_cbw.valueChangedFloat.connect(lambda v: self._set_param_and_refresh(\"cbw_intercept\", float(v)))\n",
    "        form.addRow(\"CBW Intercept\", row_cbw)\n",
    "    \n",
    "        # ---- buttons (keep compact)\n",
    "        btn_sw = QPushButton(\"3) Compute General Sw / BVW / M*\")\n",
    "        btn_sw.clicked.connect(self._on_compute_sw_clicked)\n",
    "        btn_sw.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n",
    "        form.addRow(btn_sw)\n",
    "    \n",
    "        btn_ws = QPushButton(\"4) Compute Waxman–Smits Sw\")\n",
    "        btn_ws.clicked.connect(self._on_compute_ws_clicked)\n",
    "        btn_ws.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n",
    "        form.addRow(btn_ws)\n",
    "    \n",
    "        btn_export = QPushButton(\"Export DF\")\n",
    "        btn_export.clicked.connect(self._on_export_df_clicked)\n",
    "        btn_export.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n",
    "        form.addRow(btn_export)\n",
    "    \n",
    "        btn_zone = QPushButton(\"Final Zone Plot - not working\")\n",
    "        btn_zone.clicked.connect(self._on_final_zone_plot_clicked)\n",
    "        btn_zone.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n",
    "        form.addRow(btn_zone)\n",
    "    \n",
    "        return w\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Tracks (build)\n",
    "    # =============================================================================\n",
    "    def _build_tracks(self):\n",
    "        self.tracks.clear()\n",
    "\n",
    "        gr  = self.tracks.addPlot(row=0, col=0, title=\"GR/CGR\")\n",
    "        por = self.tracks.addPlot(row=0, col=1, title=\"Porosity\")\n",
    "        rt  = self.tracks.addPlot(row=0, col=2, title=\"Rt\")\n",
    "        nmr = self.tracks.addPlot(row=0, col=3, title=\"NMR\")\n",
    "        bvw = self.tracks.addPlot(row=0, col=4, title=\"BVW\")\n",
    "\n",
    "        por.setYLink(gr)\n",
    "        rt.setYLink(gr)\n",
    "        nmr.setYLink(gr)\n",
    "        bvw.setYLink(gr)\n",
    "\n",
    "\n",
    "        for p in (gr, por, rt, nmr,bvw):\n",
    "            p.invertY(True)\n",
    "            p.showGrid(x=True, y=True, alpha=0.6)\n",
    "\n",
    "        self.track_items = {\"gr\": gr, \"por\": por, \"rt\": rt, \"nmr\": nmr,\"bvw\":bvw}\n",
    "\n",
    "    # =============================================================================\n",
    "    # Depth plot update (GR/CGR fills + VSH_HL overlay + NMR fills)\n",
    "    # =============================================================================\n",
    "    def update_depth_plot(self, state):\n",
    "        # Clear tracks\n",
    "        for pi in self.track_items.values():\n",
    "            pi.clear()\n",
    "\n",
    "        # Prefer depth-windowed view if present\n",
    "        df = getattr(state, \"analysis_df_view\", None)\n",
    "        if df is None or df.empty:\n",
    "            df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            return\n",
    "\n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "\n",
    "        depth = df.index.to_numpy(dtype=float)\n",
    "\n",
    "        def arr(name: str | None):\n",
    "            if not name or name not in df.columns:\n",
    "                return None\n",
    "            return pd.to_numeric(df[name], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "        def get_curve(*keys, fallbacks=None):\n",
    "            for k in keys:\n",
    "                v = params.get(k)\n",
    "                if v:\n",
    "                    return v\n",
    "            if fallbacks:\n",
    "                return first_present(df.columns, fallbacks)\n",
    "            return None\n",
    "\n",
    "        # Resolve curves\n",
    "        gr_curve   = get_curve(\"gr_curve\",  fallbacks=[\"HSGR\", \"GR\", \"SGR\", \"GR_EDTC\", \"HGR\",\"EGR\"])\n",
    "        cgr_curve  = get_curve(\"cgr_curve\", fallbacks=[\"HCGR\", \"CGR\", \"ECGR\", \"GR_EDTC\"])\n",
    "\n",
    "        tnph_curve = get_curve(\"tnph_curve\", fallbacks=[\"TNPH\", \"NPOR\", \"NPHI\", \"CNL\"])\n",
    "        rhob_curve = get_curve(\"rhob_curve\", fallbacks=[\"RHOB\", \"RHOZ\"])\n",
    "        \n",
    "        rt_curve   = get_curve(\"rt_curve\",   fallbacks=[\"AT90\", \"AF90\", \"AO90\", \"ILD\", \"RT\"])\n",
    "\n",
    "        phit_nmr_curve = get_curve(\"tcmr_curve\", fallbacks=[\"PHIT_NMR\", \"TCMR\", \"MPHS\"])\n",
    "        phie_nmr_curve = get_curve(\"cmrp_curve\", fallbacks=[\"PHIE_NMR\", \"CMRP_3MS\", \"CMRP3MS\", \"CMRP\",\"MPHI\"])\n",
    "        bvie_curve     = get_curve(\"bvie_curve\", fallbacks=[\"BVIE\", \"BVI_E\", \"BVI\",\"MBVI\"])\n",
    "\n",
    "        # Helper to plot a curve on a track\n",
    "        def plot(track_key: str, x, pen, xlim=None, invert_x=False, logx=False):\n",
    "            if x is None:\n",
    "                return\n",
    "            m = np.isfinite(x) & np.isfinite(depth)\n",
    "            if logx:\n",
    "                m = m & (x > 0)\n",
    "            if not np.any(m):\n",
    "                return\n",
    "            pi = self.track_items[track_key]\n",
    "            pi.setLogMode(x=bool(logx), y=False)\n",
    "            pi.getViewBox().invertX(invert_x)\n",
    "            pi.plot(x[m], depth[m], pen=pen)\n",
    "            if xlim is not None:\n",
    "                pi.setXRange(xlim[0], xlim[1], padding=0.0)\n",
    "\n",
    "        # -------------------------\n",
    "        # Track 1: GR/CGR + fills\n",
    "        # -------------------------\n",
    "        pi_gr = self.track_items[\"gr\"]\n",
    "        x_gr  = arr(gr_curve)\n",
    "        x_cgr = arr(cgr_curve)\n",
    "\n",
    "        if (x_gr is not None) and (x_cgr is not None):\n",
    "            m = np.isfinite(depth) & np.isfinite(x_gr) & np.isfinite(x_cgr)\n",
    "            if np.any(m):\n",
    "                d = depth[m].astype(float)\n",
    "                grv  = np.clip(x_gr[m].astype(float),  0.0, 200.0)\n",
    "                cgrv = np.clip(x_cgr[m].astype(float), 0.0, 200.0)\n",
    "\n",
    "                c_gr  = pi_gr.plot(grv,  d, pen=pg.mkPen(\"m\", width=2), name=\"GR\")\n",
    "                c_cgr = pi_gr.plot(cgrv, d, pen=pg.mkPen(\"g\", width=2), name=\"CGR\")\n",
    "\n",
    "                zerogr = np.zeros_like(grv)\n",
    "                c_zero = pi_gr.plot(zerogr, d, pen=pg.mkPen(\"w\", width=1), name=\"0\")\n",
    "\n",
    "                # Between GR and CGR (magenta)\n",
    "                pi_gr.addItem(pg.FillBetweenItem(c_gr, c_cgr, brush=pg.mkBrush(\"magenta\")))\n",
    "                # From 0 to CGR (green)\n",
    "                pi_gr.addItem(pg.FillBetweenItem(c_zero, c_cgr, brush=pg.mkBrush(\"g\")))\n",
    "\n",
    "                pi_gr.setXRange(0, 200, padding=0.0)\n",
    "        else:\n",
    "            plot(\"gr\", x_gr,  pg.mkPen(\"m\", width=2), xlim=(0, 200))\n",
    "            plot(\"gr\", x_cgr, pg.mkPen(\"g\", width=2), xlim=(0, 200))\n",
    "\n",
    "\n",
    "        # Overlay VSH_HL (scaled just for the GR track visual)\n",
    "        vsh = get_col(df, \"VSH_HL\")\n",
    "        if vsh is not None:\n",
    "            plot(\"gr\", vsh * 200.0, pg.mkPen(\"brown\", width=6), xlim=(0, 200))\n",
    "            \n",
    "        pi.addLegend()\n",
    "\n",
    "        # -------------------------\n",
    "        # Track 2: Porosity overlays (+ Chartbook PHIT if present)\n",
    "        # -------------------------\n",
    "        plot(\"por\", arr(tnph_curve),     pg.mkPen(\"g\", width=1), xlim=(-0.15, 0.45), invert_x=True)\n",
    "        plot(\"por\", arr(phit_nmr_curve), pg.mkPen(\"k\", width=1), xlim=(-0.15, 0.45), invert_x=True)\n",
    "        plot(\"por\", arr(phie_nmr_curve), pg.mkPen(\"b\", width=1), xlim=(-0.15, 0.45), invert_x=True)\n",
    "\n",
    "        # Optional Chartbook total porosity (already in porosity units)\n",
    "        x_phit_chart = get_col(df, \"PHIT_CHART\")  # assumes this returns a numpy array aligned to df index\n",
    "        if x_phit_chart is not None:\n",
    "            plot(\"por\", x_phit_chart, pg.mkPen(\"y\", width=2), xlim=(-0.15, 0.45), invert_x=True)\n",
    "        \n",
    "        # Density porosity from RHOB (need numeric array, not the curve name)\n",
    "        rhob_x = arr(rhob_curve)  # <-- this is the key fix\n",
    "        if rhob_x is not None:\n",
    "            rho_ma = 2.71\n",
    "            rho_fl = 1.10\n",
    "            phid = (rho_ma - rhob_x) / (rho_ma - rho_fl)\n",
    "        \n",
    "            # optional clipping to keep it sane for display\n",
    "            phid = np.clip(phid, -0.15, 0.45)\n",
    "        \n",
    "            plot(\"por\", phid, pg.mkPen(\"r\", width=2), xlim=(-0.15, 0.45), invert_x=True)\n",
    "\n",
    "        \n",
    "            \n",
    "        pi.addLegend()\n",
    "\n",
    "        # -------------------------\n",
    "        # Track 3: Rt (log x)\n",
    "        # -------------------------\n",
    "        plot(\"rt\", arr(rt_curve), pg.mkPen(\"lightgray\", width=2, style=Qt.DashLine), xlim=(0.2, 20), invert_x=False, logx=False)\n",
    "        pi.addLegend()\n",
    "\n",
    "        # -------------------------\n",
    "        # Track 4: NMR fills (PHIT_NMR, PHIE_NMR, BVIE)\n",
    "        # -------------------------\n",
    "        pi_nmr = self.track_items[\"nmr\"]\n",
    "        x_phit = arr(phit_nmr_curve)\n",
    "        x_phie = arr(phie_nmr_curve)\n",
    "        x_bvie = arr(bvie_curve)\n",
    "\n",
    "        if (x_phit is not None) and (x_phie is not None) and (x_bvie is not None):\n",
    "            m = np.isfinite(depth) & np.isfinite(x_phit) & np.isfinite(x_phie) & np.isfinite(x_bvie)\n",
    "            if np.any(m):\n",
    "                d = depth[m].astype(float)\n",
    "                phit = np.clip(x_phit[m].astype(float), 0.0, 0.30)\n",
    "                phie = np.clip(x_phie[m].astype(float), 0.0, 0.30)\n",
    "                bvie = np.clip(x_bvie[m].astype(float), 0.0, 0.30)\n",
    "\n",
    "                # enforce ordering: phit >= phie >= bvie\n",
    "                phie = np.minimum(phie, phit)\n",
    "                bvie = np.minimum(bvie, phie)\n",
    "\n",
    "                c_phit = pi_nmr.plot(phit, d, pen=pg.mkPen(\"gray\", width=1), name=\"PHIT_NMR\")\n",
    "                c_phie = pi_nmr.plot(phie, d, pen=pg.mkPen(\"y\", width=1), name=\"PHIE_NMR\")\n",
    "                c_bvie = pi_nmr.plot(bvie, d, pen=pg.mkPen(\"b\", width=1), name=\"BVIE\")\n",
    "\n",
    "                zero = np.zeros_like(bvie)\n",
    "                c_zero = pi_nmr.plot(zero, d, pen=pg.mkPen(\"w\", width=1), name=\"0\")\n",
    "\n",
    "                pi_nmr.addItem(pg.FillBetweenItem(c_phit, c_phie, brush=pg.mkBrush(\"gray\")))  # CBW\n",
    "                pi_nmr.addItem(pg.FillBetweenItem(c_phie, c_bvie, brush=pg.mkBrush(\"y\")))     # FFI\n",
    "                pi_nmr.addItem(pg.FillBetweenItem(c_bvie, c_zero, brush=pg.mkBrush(\"b\")))     # BVI\n",
    "\n",
    "                pi_nmr.getViewBox().invertX(True)\n",
    "                pi_nmr.setXRange(0.0, 0.30, padding=0.0)\n",
    "\n",
    "  \n",
    "        pi.addLegend()\n",
    "\n",
    "\n",
    "\n",
    "        # -------------------------\n",
    "        # Track 5: BVW fills (PHIT, PHIE, BVWe_CP)  [NO NMR]\n",
    "        # -------------------------\n",
    "        pi_bvw = self.track_items[\"bvw\"]\n",
    "        \n",
    "        # pull curves from analysis df\n",
    "        x_phit = get_col(df, \"PHIT\")        # standardized PHIT (fraction)\n",
    "        x_phie = get_col(df, \"PHIE\")        # PHIT - CBWapp (clipped >=0 in your compute)\n",
    "        x_bvwe = get_col(df, \"BVWe_CP\")     # PHIT*Sw - CBWapp (clipped >=0)\n",
    "        \n",
    "        # only proceed if present\n",
    "        if (x_phit is not None) and (x_phie is not None) and (x_bvwe is not None):\n",
    "            m = np.isfinite(depth) & np.isfinite(x_phit) & np.isfinite(x_phie) & np.isfinite(x_bvwe)\n",
    "            if np.any(m):\n",
    "                d = depth[m].astype(float)\n",
    "        \n",
    "                phit = np.clip(x_phit[m].astype(float), 0.0, 0.30)\n",
    "                phie = np.clip(x_phie[m].astype(float), 0.0, 0.30)\n",
    "                bvwe = np.clip(x_bvwe[m].astype(float), 0.0, 0.30)\n",
    "        \n",
    "                # enforce ordering: PHIT >= PHIE >= BVWe\n",
    "                phie = np.minimum(phie, phit)\n",
    "                bvwe = np.minimum(bvwe, phie)\n",
    "        \n",
    "                # boundary curves (these show in legend)\n",
    "                c_phit = pi_bvw.plot(phit, d, pen=pg.mkPen(\"yellow\", width=1), name=\"PHIT\")\n",
    "                c_phie = pi_bvw.plot(phie, d, pen=pg.mkPen(\"green\", width=1), name=\"PHIE\")\n",
    "                c_bvwe = pi_bvw.plot(bvwe, d, pen=pg.mkPen(\"dodgerblue\", width=1), name=\"BVWe_CP\")\n",
    "        \n",
    "                # baseline\n",
    "                zero = np.zeros_like(bvwe)\n",
    "                c_zero = pi_bvw.plot(zero, d, pen=pg.mkPen(\"w\", width=1), name=\"0\")\n",
    "        \n",
    "                # fills\n",
    "                pi_bvw.addItem(pg.FillBetweenItem(c_phit, c_phie, brush=pg.mkBrush(\"gray\")))   # CBW = PHIT-PHIE\n",
    "                pi_bvw.addItem(pg.FillBetweenItem(c_phie, c_bvwe, brush=pg.mkBrush(\"green\"))) # BVWe in PHIE\n",
    "                pi_bvw.addItem(pg.FillBetweenItem(c_bvwe, c_zero, brush=pg.mkBrush(\"dodgerblue\")))   # remaining PHIE\n",
    "        \n",
    "                # axis config\n",
    "                pi_bvw.getViewBox().invertX(True)\n",
    "                pi_bvw.setXRange(0.0, 0.30, padding=0.0)\n",
    "\n",
    "        \n",
    "        #pi_bvw.clear()\n",
    "        if getattr(pi_bvw, \"legend\", None) is None:\n",
    "            pi_bvw.addLegend(offset=(10, 10))\n",
    "\n",
    "        \n",
    "\n",
    "        # -------------------------\n",
    "        # Tops overlay (filtered to current interval + does NOT expand view)\n",
    "        # -------------------------\n",
    "        tops_df = getattr(state, \"tops_df\", None)\n",
    "        if tops_df is not None and len(tops_df) > 0:\n",
    "        \n",
    "            # Find a depth column robustly\n",
    "            depth_col = None\n",
    "            for c in (\"Depth\", \"DEPT\", \"dept\", \"TopDepth\", \"Top_Depth\", \"MD\", \"TVD\"):\n",
    "                if c in tops_df.columns:\n",
    "                    depth_col = c\n",
    "                    break\n",
    "        \n",
    "            # Optional name/label column\n",
    "            name_col = None\n",
    "            for c in (\"Top\", \"Name\", \"Horizon\", \"Marker\", \"Pick\", \"Formation\"):\n",
    "                if c in tops_df.columns:\n",
    "                    name_col = c\n",
    "                    break\n",
    "        \n",
    "            if depth_col is not None:\n",
    "                # Current plotted interval from df (view)\n",
    "                ylo = float(np.nanmin(depth))\n",
    "                yhi = float(np.nanmax(depth))\n",
    "        \n",
    "                # Add only tops inside interval\n",
    "                for _, row in tops_df.iterrows():\n",
    "                    d = row.get(depth_col, None)\n",
    "                    if d is None or pd.isna(d):\n",
    "                        continue\n",
    "                    try:\n",
    "                        y = float(d)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        \n",
    "                    # ✅ filter to interval\n",
    "                    if not (ylo <= y <= yhi):\n",
    "                        continue\n",
    "        \n",
    "                    label = None\n",
    "                    if name_col is not None:\n",
    "                        v = row.get(name_col, None)\n",
    "                        if v is not None and not pd.isna(v):\n",
    "                            label = str(v)\n",
    "        \n",
    "                    for pi in self.track_items.values():\n",
    "                        pi.addLine(y=y, pen=pg.mkPen(\"r\", width=2))\n",
    "        \n",
    "                    # Put label only on GR track\n",
    "                    if label:\n",
    "                        gr_pi = self.track_items[\"gr\"]\n",
    "                        x_left = gr_pi.viewRange()[0][0]\n",
    "                        t = pg.TextItem(label, anchor=(0, 1))\n",
    "                        t.setPos(x_left, y)\n",
    "                        gr_pi.addItem(t)\n",
    "        \n",
    "        # ✅ Lock Y-range AFTER tops so nothing expands the view\n",
    "        ymin = float(np.nanmin(depth))\n",
    "        ymax = float(np.nanmax(depth))\n",
    "        for pi in self.track_items.values():\n",
    "            pi.enableAutoRange(axis=pg.ViewBox.YAxis, enable=False)\n",
    "            # With invertY(True), setYRange still expects (min,max); this keeps it pinned\n",
    "            pi.setYRange(ymin, ymax, padding=0.0)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    # Matplotlib updates\n",
    "    # =============================================================================\n",
    "    def update_nd_crossplot(self, state):\n",
    "        ax = self.ax_nd\n",
    "        canvas = self.canvas_nd\n",
    "        ax.clear()\n",
    "\n",
    "        df = getattr(state, \"analysis_df_view\", None)\n",
    "        if df is None or df.empty:\n",
    "            df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            canvas.draw_idle()\n",
    "            return\n",
    "\n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "\n",
    "        tnph = params.get(\"tnph_curve\")\n",
    "        rhob = params.get(\"rhob_curve\")\n",
    "        if (not tnph) or (tnph not in df.columns):\n",
    "            tnph = _first_present(df.columns, [\"TNPH\", \"NPHI\", \"CNL\", \"NPOR\"])\n",
    "        if (not rhob) or (rhob not in df.columns):\n",
    "            rhob = _first_present(df.columns, [\"RHOZ\", \"RHOB\"])\n",
    "\n",
    "        if tnph and rhob and (tnph in df.columns) and (rhob in df.columns):\n",
    "            x = pd.to_numeric(df[tnph], errors=\"coerce\").to_numpy(float)\n",
    "            y = pd.to_numeric(df[rhob], errors=\"coerce\").to_numpy(float)\n",
    "            m = np.isfinite(x) & np.isfinite(y)\n",
    "            ax.scatter(x[m], y[m], s=6, color='red', alpha=0.6, label=\"Well data\")\n",
    "            ax.set_xlabel(\"NPOR (V/V)\")\n",
    "            ax.set_ylabel(\"RHOB (g/cc)\")\n",
    "            ax.invert_yaxis()\n",
    "            ax.grid(True, alpha=0.25)\n",
    "\n",
    "        # optional chartbook points\n",
    "\n",
    "\n",
    "        '''\n",
    "        df_chart = getattr(state, \"chartbook_df\", None)\n",
    "        if df_chart is not None and not df_chart.empty:\n",
    "            if (\"Neutron\" in df_chart.columns) and (\"RHOB\" in df_chart.columns):\n",
    "                cx = pd.to_numeric(df_chart[\"Neutron\"], errors=\"coerce\").to_numpy(float)\n",
    "                cy = pd.to_numeric(df_chart[\"RHOB\"], errors=\"coerce\").to_numpy(float)\n",
    "                cm = np.isfinite(cx) & np.isfinite(cy)\n",
    "                ax.scatter(cx[cm], cy[cm], s=10, alpha=0.7, c=\"m\", label=\"SLB chartbook\")'''\n",
    "\n",
    "\n",
    "        file_path = \"./apps/merge_gui/data/cnl_chart_1pt1.xlsx\"\n",
    "        spine = pd.read_excel(file_path, index_col=False)\n",
    "        if spine is not None and not spine.empty:\n",
    "            if (\"Neutron\" in spine.columns) and (\"RHOB\" in spine.columns):\n",
    "                cx = pd.to_numeric(spine[\"Neutron\"], errors=\"coerce\").to_numpy(float)\n",
    "                cy = pd.to_numeric(spine[\"RHOB\"], errors=\"coerce\").to_numpy(float)\n",
    "                cm = np.isfinite(cx) & np.isfinite(cy)\n",
    "                #ax.plot(cx[cm], cy[cm],  alpha=0.7, c=\"k\", label=\"SLB chartbook\")\n",
    "                ax.scatter(cx[cm], cy[cm], s=8, alpha=0.7, c=\"k\", label=\"SLB chartbook\")\n",
    "\n",
    "        # triangle\n",
    "        neut_shale  = float(params.get(\"neut_shale\",  0.49))\n",
    "        den_shale   = float(params.get(\"den_shale\",   2.65))\n",
    "        neut_matrix = float(params.get(\"neut_matrix\", -0.04))\n",
    "        den_matrix  = float(params.get(\"den_matrix\",  2.65))\n",
    "        neut_fl     = float(params.get(\"neut_fl\",     1.0))\n",
    "        den_fl      = float(params.get(\"den_fl\",      1.1))\n",
    "\n",
    "        A = (neut_matrix, den_matrix)\n",
    "        B = (neut_shale,  den_shale)\n",
    "        C = (neut_fl,     den_fl)\n",
    "\n",
    "        ax.plot([A[0], B[0], C[0], A[0]], [A[1], B[1], C[1], A[1]], lw=2, color='m', alpha=0.9, label=\"Shale triangle\")\n",
    "        ax.text(A[0], A[1], \" Mat\", color='blue', fontsize=9, va=\"top\")\n",
    "        ax.text(B[0], B[1], \" Sh\",  color='blue', fontsize=9, va=\"top\")\n",
    "        ax.text(C[0], C[1], \" Fl\",  color='blue', fontsize=9, va=\"center\")\n",
    "        ax.legend(loc=\"best\", fontsize=9)\n",
    "\n",
    "        canvas.draw_idle()\n",
    "\n",
    "    def update_gr_hist(self, state):\n",
    "        ax = self.ax_gr\n",
    "        canvas = self.canvas_gr\n",
    "        ax.clear()\n",
    "\n",
    "        df = getattr(state, \"analysis_df_view\", None)\n",
    "        if df is None or df.empty:\n",
    "            df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            canvas.draw_idle()\n",
    "            return\n",
    "\n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "\n",
    "        gr = params.get(\"gr_curve\")\n",
    "        if (not gr) or (gr not in df.columns):\n",
    "            gr = _first_present(df.columns, [\"GR\", \"HSGR\", \"SGR\", \"GR_EDTC\", \"HCGR\", \"CGR\", \"ECGR\"])\n",
    "\n",
    "        if not gr or gr not in df.columns:\n",
    "            ax.text(0.5, 0.5, \"No GR curve found\", ha=\"center\", va=\"center\")\n",
    "            canvas.draw_idle()\n",
    "            return\n",
    "\n",
    "        gr_clean = float(params.get(\"gr_clean\", 25.0))\n",
    "        gr_shale = float(params.get(\"gr_shale\", 160.0))\n",
    "\n",
    "        g = pd.to_numeric(df[gr], errors=\"coerce\").to_numpy(float)\n",
    "        g = g[np.isfinite(g)]\n",
    "        if g.size:\n",
    "            ax.hist(g, bins=60, alpha=0.85, color='green')\n",
    "            ax.axvline(gr_clean, lw=2, linestyle=\"--\",color='blue' ,label=f\"GR clean = {gr_clean:g}\")\n",
    "            ax.axvline(gr_shale, lw=2, linestyle=\"--\", color='brown',label=f\"GR shale = {gr_shale:g}\")\n",
    "            ax.axvspan(gr_clean,gr_shale,color='yellow',alpha=0.1)\n",
    "            ax.set_xlabel(gr)\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            ax.grid(True, alpha=0.25)\n",
    "            ax.legend(loc=\"best\", fontsize=9)\n",
    "\n",
    "        canvas.draw_idle()\n",
    "\n",
    "    def update_dt_rhob_crossplot(self, state):\n",
    "        ax = self.ax_dt\n",
    "        canvas = self.canvas_dt\n",
    "        ax.clear()\n",
    "\n",
    "        df = getattr(state, \"analysis_df_view\", None)\n",
    "        if df is None or df.empty:\n",
    "            df = getattr(state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            canvas.draw_idle()\n",
    "            return\n",
    "\n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "\n",
    "        rhob = params.get(\"rhob_curve\")\n",
    "        if (not rhob) or (rhob not in df.columns):\n",
    "            rhob = _first_present(df.columns, [\"RHOZ\", \"RHOB\"])\n",
    "\n",
    "        dtco = params.get(\"dtco_curve\")\n",
    "        if (not dtco) or (dtco not in df.columns):\n",
    "            dtco = _first_present(df.columns, [\"DTCO\", \"DTC\", \"AC\"])\n",
    "\n",
    "        if not rhob or not dtco or (rhob not in df.columns) or (dtco not in df.columns):\n",
    "            ax.text(0.5, 0.5, \"Need RHOB and DTCO\", ha=\"center\", va=\"center\")\n",
    "            canvas.draw_idle()\n",
    "            return\n",
    "\n",
    "        x = pd.to_numeric(df[dtco], errors=\"coerce\").to_numpy(float)   # DT on X\n",
    "        y = pd.to_numeric(df[rhob], errors=\"coerce\").to_numpy(float)   # RHOB on Y\n",
    "        m = np.isfinite(x) & np.isfinite(y)\n",
    "        ax.scatter(x[m], y[m], s=6, color='dodgerblue', alpha=1, label=\"Well data\")\n",
    "\n",
    "        ax.set_xlabel(\"DTCO (µs/ft)\")\n",
    "        ax.set_ylabel(\"RHOB (g/cc)\")\n",
    "        ax.set_xlim(40, 189)\n",
    "        ax.set_ylim(3.0, 1.0)\n",
    "        ax.grid(True, alpha=0.25)\n",
    "\n",
    "        den_matrix = float(params.get(\"den_matrix\", 2.65))\n",
    "        den_shale  = float(params.get(\"den_shale\",  2.65))\n",
    "        den_fl     = float(params.get(\"den_fl\",     1.10))\n",
    "\n",
    "        dt_matrix  = float(params.get(\"dt_matrix\", 55.5))\n",
    "        dt_shale   = float(params.get(\"dt_shale\",  90.0))\n",
    "        dt_fl      = float(params.get(\"dt_fl\",    188.0))\n",
    "\n",
    "        A = (dt_matrix, den_matrix)\n",
    "        B = (dt_shale,  den_shale)\n",
    "        C = (dt_fl,     den_fl)\n",
    "\n",
    "        ax.plot([A[0], B[0], C[0], A[0]], [A[1], B[1], C[1], A[1]], lw=2, alpha=0.9, color='m', label=\"Shale triangle\")\n",
    "        ax.text(A[0], A[1], \" Mat\", color='blue', fontsize=9, va=\"top\")\n",
    "        ax.text(B[0], B[1], \" Sh\",  color='blue', fontsize=9, va=\"top\")\n",
    "        ax.text(C[0], C[1], \" Fl\",  color='blue', fontsize=9, va=\"center\")\n",
    "\n",
    "        ax.legend(loc=\"best\", fontsize=9)\n",
    "        canvas.draw_idle()\n",
    "\n",
    "        \n",
    "    def update_sw_tab(self, state):\n",
    "        axL = self.ax_sw_left\n",
    "        axM = self.ax_sw_mid\n",
    "        axR = self.ax_sw_right\n",
    "        axC = self.ax_sw_cbw\n",
    "\n",
    "        axL.clear(); axM.clear(); axR.clear(); axC.clear()\n",
    "\n",
    "        df_view = getattr(state, \"analysis_df_view\", None)\n",
    "        df_full = getattr(state, \"analysis_df\", None)\n",
    "        df = df_view if (df_view is not None and not df_view.empty) else df_full\n",
    "        if df is None or df.empty:\n",
    "            self.canvas_sw.draw_idle()\n",
    "            return\n",
    "\n",
    "        # If view is missing required columns (common), fall back to full\n",
    "        if (df is df_view) and (df_full is not None) and (not df_full.empty):\n",
    "            need_cols = [\"VSH_HL\", \"CBW\"]\n",
    "            if any(c not in df.columns for c in need_cols) and all(c in df_full.columns for c in need_cols):\n",
    "                df = df_full\n",
    "\n",
    "        params = getattr(state, \"params\", {}) or {}\n",
    "        if not isinstance(params, dict):\n",
    "            params = {}\n",
    "\n",
    "        # Resolve RT + PHIT\n",
    "        rt_col = params.get(\"rt_curve\")\n",
    "        if (not rt_col) or (rt_col not in df.columns):\n",
    "            rt_col = _first_present(df.columns, [\"AT90\", \"AF90\", \"AO90\", \"ILD\", \"RT\"])\n",
    "\n",
    "        phit_col = params.get(\"phit_curve\")\n",
    "        if (not phit_col) or (phit_col not in df.columns):\n",
    "            phit_col = \"PHIT\" if \"PHIT\" in df.columns else _first_present(df.columns, [\"PHIT_CHART\", \"PHIT_NMR\", \"TCMR\", \"MPHS\"])\n",
    "\n",
    "        if (rt_col is None) or (phit_col is None):\n",
    "            axL.text(0.5, 0.5, \"Need PHIT (or PHIT_CHART/PHIT_NMR) and Rt\", ha=\"center\", va=\"center\")\n",
    "            self.canvas_sw.draw_idle()\n",
    "            return\n",
    "\n",
    "        y = df.index.to_numpy(float)\n",
    "        phit = pd.to_numeric(df[phit_col], errors=\"coerce\").to_numpy(float)\n",
    "        rt = pd.to_numeric(df[rt_col], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "        bvw = pd.to_numeric(df[\"BVW\"], errors=\"coerce\").to_numpy(float) if \"BVW\" in df.columns else None\n",
    "        bvo = pd.to_numeric(df[\"BVO\"], errors=\"coerce\").to_numpy(float) if \"BVO\" in df.columns else None\n",
    "        ms_app = pd.to_numeric(df[\"MSTAR_APP\"], errors=\"coerce\").to_numpy(float) if \"MSTAR_APP\" in df.columns else None\n",
    "        vsh = pd.to_numeric(df[\"VSH_HL\"], errors=\"coerce\").to_numpy(float) if \"VSH_HL\" in df.columns else None\n",
    "\n",
    "        # ---- Left: BVW/BVO depth plot\n",
    "        axL.set_title(\"Bulk Volume Plot\")\n",
    "        axL.plot(phit, y, \"-r\", lw=1, label=\"PHIT\")\n",
    "\n",
    "        if bvw is not None:\n",
    "            axL.plot(bvw, y, \"-k\", lw=1, label=\"BVW\")\n",
    "            axL.fill_betweenx(y, phit, bvw, where=np.isfinite(phit) & np.isfinite(bvw), color = \"green\", alpha=0.35, label=\"BVO\")\n",
    "            axL.fill_betweenx(y, bvw, 0, where=np.isfinite(bvw), color = \"cyan\", alpha=0.35, label=\"BVW fill\")\n",
    "\n",
    "        axL.set_xlim(0.5, 0.0)\n",
    "        axL.set_ylim(np.nanmax(y), np.nanmin(y))\n",
    "        axL.set_xlabel(\"BVO/BVW\")\n",
    "        axL.set_ylabel(\"Depth\")\n",
    "        axL.grid(True, alpha=0.25)\n",
    "        axL.legend(loc=\"best\", fontsize=9)\n",
    "\n",
    "        # ---- Right: VSH vs MSTAR_APP + fitted line\n",
    "        axR.set_title(\"Vsh_HL vs. Mstar_Apparent\")\n",
    "        if (vsh is None) or (ms_app is None):\n",
    "            axR.text(0.5, 0.5, \"Need VSH_HL and MSTAR_APP\\n(Compute Vsh + Sw first)\", ha=\"center\", va=\"center\")\n",
    "        else:\n",
    "            m = np.isfinite(vsh) & np.isfinite(ms_app)\n",
    "            axR.plot(vsh[m], ms_app[m], \"r.\", ms=3)\n",
    "\n",
    "            m_cem  = float(params.get(\"sw.m_cem\", 2.0))\n",
    "            mslope = float(params.get(\"sw.mslope\", 1.0))\n",
    "            xline = np.linspace(0.0, 1.0, 200)\n",
    "            axR.plot(xline, xline * mslope + m_cem, \"k-\", lw=2)\n",
    "\n",
    "        axR.set_xlim(0.0, 1.0)\n",
    "        axR.set_ylim(0.0, 7.0)\n",
    "        axR.set_xlabel(\"Vsh_HL [v/v]\")\n",
    "        axR.set_ylabel(\"Mstar Apparent\")\n",
    "        axR.grid(True, alpha=0.25)\n",
    "\n",
    "        # ---- Middle: Pickett plot + Sw lines\n",
    "        axM.set_title(\"Pickett Plot\")\n",
    "        m = np.isfinite(rt) & (rt > 0) & np.isfinite(phit) & (phit > 0)\n",
    "        axM.loglog(rt[m], phit[m], \"r.\", ms=3)\n",
    "\n",
    "        axM.set_xlim(0.01, 1000)\n",
    "        axM.set_ylim(0.01, 1.0)\n",
    "        axM.set_xlabel(f\"{rt_col} [ohm-m]\")\n",
    "        axM.set_ylabel(\"PHIT [v/v]\")\n",
    "        axM.grid(True, which=\"both\", alpha=0.25)\n",
    "\n",
    "        m_cem = float(params.get(\"sw.m_cem\", 2.0))\n",
    "        n_sat = float(params.get(\"sw.n_sat\", 2.0))\n",
    "        rw    = float(params.get(\"sw.rw\", 0.03))\n",
    "\n",
    "        sw_plot = (1.0, 0.8, 0.6, 0.4, 0.2)\n",
    "        phit_plot = np.array([0.01, 1.0])\n",
    "        for sw in sw_plot:\n",
    "            rt_line = (rw / (sw ** n_sat)) * (1.0 / (phit_plot ** m_cem))\n",
    "            axM.loglog(rt_line, phit_plot, lw=2, label=f\"Sw {int(sw*100)}%\")\n",
    "        axM.legend(loc=\"best\", fontsize=8)\n",
    "\n",
    "        # ---- CBW vs VSH_HL (bottom-right)\n",
    "        axC.set_title(\"Vsh_HL vs. CBW\", color=\"blue\")\n",
    "\n",
    "        cbw_col = _first_present(df.columns, [\"CBW\", \"CBWapp\", \"CBWa\", \"CBW_NMR\", \"CBWAPP\"])\n",
    "        if (vsh is None) or (cbw_col is None):\n",
    "            axC.text(0.5, 0.5, \"Need VSH_HL + CBW\\n(Compute Vsh first)\", ha=\"center\", va=\"center\")\n",
    "        else:\n",
    "            cbw = pd.to_numeric(df[cbw_col], errors=\"coerce\").to_numpy(float)\n",
    "            mm = np.isfinite(vsh) & np.isfinite(cbw)\n",
    "            axC.plot(vsh[mm], cbw[mm], \"r.\", ms=3)\n",
    "\n",
    "            cbw_int = float(params.get(\"cbw_intercept\", 0.15))\n",
    "            x = np.linspace(0.0, 1.0, 200)\n",
    "            axC.plot(x, x * cbw_int, \"k-\", lw=2)\n",
    "\n",
    "            axC.set_xlim(0.0, 1.0)\n",
    "            axC.set_ylim(0.0, 0.5)\n",
    "            axC.set_xlabel(\"Vsh_HL [v/v]\", color=\"blue\")\n",
    "            axC.set_ylabel(f\"{cbw_col} [v/v]\", color=\"blue\")\n",
    "            axC.grid(True, alpha=0.25)\n",
    "\n",
    "        self.fig_sw.tight_layout()\n",
    "        self.canvas_sw.draw_idle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a612b00-a612-4871-98fa-eef77b11fb3c",
   "metadata": {},
   "source": [
    "## ./workflow/waxman_smits_sw_iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398518c-f7e1-4847-8de9-d4e7782a7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# petrocore/workflows/waxman_smits.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Config\n",
    "# -------------------------------------------------\n",
    "@dataclass(frozen=True)\n",
    "class WaxmanSmitsConfig:\n",
    "    rt_col: str = \"RT\"          # you will pass your family-winner (AT90/ILD/RT)\n",
    "    phit_col: str = \"PHIT\"\n",
    "    qv_col: str = \"Qv\"\n",
    "\n",
    "    out_sw: str = \"SW_WS\"\n",
    "    out_bvwt: str = \"BVWT_WS\"\n",
    "    out_bvwe: str = \"BVWe_WS\"\n",
    "\n",
    "    cbw_col: str = \"CBWapp\"     # for BVWe = PHIT*Sw - CBWapp\n",
    "    qv_cap: float = 5.0         # keep consistent with your Qv cap\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Core physics pieces (transparent)\n",
    "# -------------------------------------------------\n",
    "def _b_waxman_smits(temperature_f: float) -> float:\n",
    "    \"\"\"\n",
    "    B parameter (brine conductivity factor) for Waxman–Smits.\n",
    "    Many implementations use a temperature-dependent B.\n",
    "    If you already use a different B model in petrocore, swap this.\n",
    "    Here we provide a simple, common approximation.\n",
    "\n",
    "    IMPORTANT: If you have a preferred equation, tell me and I'll drop it in.\n",
    "    \"\"\"\n",
    "    # A mild, usable default (not high-stakes accurate).\n",
    "    # You can replace with your existing petrocore B(T) function.\n",
    "    # Typical B is ~4–6 (1/ohm·m)/meq/cc depending on units.\n",
    "    # We'll keep it as a parameter in the solver anyway.\n",
    "    return 4.8\n",
    "\n",
    "\n",
    "def _safe_clip(a: np.ndarray, lo: float, hi: float) -> np.ndarray:\n",
    "    return np.clip(a, lo, hi)\n",
    "\n",
    "def _as_depth_array(x, npts: int, name: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize x to a 1D float array of length npts.\n",
    "\n",
    "    Accepts:\n",
    "      - scalar -> broadcast to length npts\n",
    "      - array-like length npts -> returned as float 1D\n",
    "\n",
    "    Raises:\n",
    "      ValueError if array-like is wrong length.\n",
    "    \"\"\"\n",
    "    if np.isscalar(x) or (isinstance(x, np.ndarray) and x.ndim == 0):\n",
    "        return np.full(npts, float(x), dtype=float)\n",
    "\n",
    "    arr = np.asarray(x, dtype=float).reshape(-1)\n",
    "    if arr.size != npts:\n",
    "        raise ValueError(f\"{name} must be scalar or length {npts}; got length {arr.size}\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "'''MAIN PROGRAM'''\n",
    "def waxman_smits_sw_iterative(\n",
    "    rt: np.ndarray,\n",
    "    phit: np.ndarray,\n",
    "    qv: np.ndarray,\n",
    "    rw: float,\n",
    "    *,\n",
    "    m,\n",
    "    n: float,\n",
    "    B: float,\n",
    "    max_iter: int = 60,\n",
    "    tol: float = 1e-6,\n",
    "    sw0: Optional[np.ndarray] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Iterative Waxman–Smits water saturation solve for Sw.\n",
    "\n",
    "    Uses the common form:\n",
    "      1/Rt = (1/F) * (Sw^n) * ( (1/Rw) + B*Qv/Sw )\n",
    "\n",
    "    where F = a / phit^m (Archie formation factor), often a=1.\n",
    "    Rearranged and solved by fixed-point iteration.\n",
    "\n",
    "    Notes:\n",
    "      - This is deliberately written clearly (not \"clever\") for transparency.\n",
    "      - Handles NaNs and invalid inputs safely.\n",
    "    \"\"\"\n",
    "    rt = np.asarray(rt, dtype=float)\n",
    "    phit = np.asarray(phit, dtype=float)\n",
    "    qv = np.asarray(qv, dtype=float)\n",
    "\n",
    "    npts = len(rt)\n",
    "    sw = np.full(npts, np.nan, dtype=float)\n",
    "\n",
    "    # normalize m to depth array\n",
    "    m_arr = _as_depth_array(m, npts, \"m\")\n",
    "\n",
    "    # masks\n",
    "    ok = (\n",
    "        np.isfinite(rt) & (rt > 0) &\n",
    "        np.isfinite(phit) & (phit > 0) &\n",
    "        np.isfinite(qv) & (qv >= 0) &\n",
    "        np.isfinite(m_arr) & (m_arr > 0)\n",
    "    )\n",
    "    if not np.any(ok):\n",
    "        return sw\n",
    "\n",
    "    # formation factor (assume a=1): F = 1 / phit^m\n",
    "    F = 1.0 / np.power(phit[ok], m_arr[ok])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # initial guess\n",
    "    if sw0 is None:\n",
    "        swk = np.full(ok.sum(), 0.6, dtype=float)\n",
    "    else:\n",
    "        sw0 = np.asarray(sw0, dtype=float)\n",
    "        swk = sw0[ok].copy()\n",
    "        swk[~np.isfinite(swk)] = 0.6\n",
    "        swk = _safe_clip(swk, 1e-4, 1.0)\n",
    "\n",
    "    inv_rt = 1.0 / rt[ok]\n",
    "    inv_rw = 1.0 / rw\n",
    "    qv_ok = qv[ok]\n",
    "\n",
    "    # fixed-point iteration\n",
    "    for _ in range(max_iter):\n",
    "        # conductivity term: inv_rw + B*Qv/Sw\n",
    "        term = inv_rw + (B * qv_ok / np.maximum(swk, 1e-6))\n",
    "\n",
    "        # target: inv_rt = (1/F) * Sw^n * term\n",
    "        # => Sw_new = [ inv_rt * F / term ]^(1/n)\n",
    "        rhs = (inv_rt * F) / np.maximum(term, 1e-12)\n",
    "        sw_new = np.power(np.maximum(rhs, 1e-12), 1.0 / n)\n",
    "\n",
    "        sw_new = _safe_clip(sw_new, 1e-4, 1.0)\n",
    "\n",
    "        # check convergence\n",
    "        if np.nanmax(np.abs(sw_new - swk)) < tol:\n",
    "            swk = sw_new\n",
    "            break\n",
    "        swk = sw_new\n",
    "\n",
    "    sw[ok] = swk\n",
    "    return sw\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Workflow wrapper\n",
    "# -------------------------------------------------\n",
    "def compute_waxman_smits(\n",
    "    *,\n",
    "    analysis_df: pd.DataFrame,\n",
    "    cfg: WaxmanSmitsConfig,\n",
    "    rw: float,\n",
    "    m,\n",
    "    n: float,\n",
    "    B: Optional[float] = None,\n",
    "    temperature_f: Optional[float] = None,\n",
    "    logger: Optional[Callable[[str], None]] = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      SW_WS, BVWT_WS, BVWe_WS\n",
    "    to analysis_df.\n",
    "\n",
    "    BVWT_WS = PHIT * SW_WS\n",
    "    BVWe_WS = PHIT * SW_WS - CBWapp\n",
    "    \"\"\"\n",
    "    if analysis_df is None or analysis_df.empty:\n",
    "        raise ValueError(\"analysis_df is empty.\")\n",
    "\n",
    "    for c in (cfg.rt_col, cfg.phit_col, cfg.qv_col):\n",
    "        if c not in analysis_df.columns:\n",
    "            raise ValueError(f\"Missing required column in analysis_df: '{c}'\")\n",
    "\n",
    "    if cfg.cbw_col not in analysis_df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing CBW column '{cfg.cbw_col}'. Run CBW step first or change cfg.cbw_col.\"\n",
    "        )\n",
    "\n",
    "    if not np.isfinite(rw) or rw <= 0:\n",
    "        raise ValueError(\"rw must be > 0.\")\n",
    "\n",
    "\n",
    "    m_arr = _as_depth_array(m, len(analysis_df), \"m\")\n",
    "    if not np.all(np.isfinite(m_arr)) or np.nanmin(m_arr) <= 0:\n",
    "        raise ValueError(\"m must be > 0 everywhere (scalar or depth array).\")\n",
    "\n",
    "\n",
    "    if not np.isfinite(n) or n <= 0:\n",
    "        raise ValueError(\"n must be > 0.\")\n",
    "\n",
    "    if B is None:\n",
    "        if temperature_f is None:\n",
    "            # reasonable fallback constant if you don't provide temperature\n",
    "            B = 4.8\n",
    "        else:\n",
    "            B = _b_waxman_smits(float(temperature_f))\n",
    "\n",
    "    df = analysis_df.copy()\n",
    "\n",
    "    rt = pd.to_numeric(df[cfg.rt_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    phit = pd.to_numeric(df[cfg.phit_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    qv = pd.to_numeric(df[cfg.qv_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    qv = np.clip(qv, 0.0, cfg.qv_cap)\n",
    "\n",
    "    sw = waxman_smits_sw_iterative(\n",
    "        rt=rt,\n",
    "        phit=phit,\n",
    "        qv=qv,\n",
    "        rw=rw,\n",
    "        m=m_arr,\n",
    "        n=n,\n",
    "        B=float(B),\n",
    "        max_iter=60,\n",
    "        tol=1e-6,\n",
    "        sw0=None,\n",
    "    )\n",
    "\n",
    "    bvwt = np.clip(phit * sw, 0.0, 1.0)\n",
    "\n",
    "    cbwapp = pd.to_numeric(df[cfg.cbw_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    bvwe = np.clip(bvwt - cbwapp, 0.0, 1.0)\n",
    "\n",
    "    df[cfg.out_sw] = sw\n",
    "    df[cfg.out_bvwt] = bvwt\n",
    "    df[cfg.out_bvwe] = bvwe\n",
    "\n",
    "    n_valid = int(np.isfinite(sw).sum())\n",
    "\n",
    "    report = (\n",
    "        \"=== WAXMAN–SMITS Sw ===\\n\"\n",
    "        f\"Rt column     : {cfg.rt_col}\\n\"\n",
    "        f\"PHIT column   : {cfg.phit_col}\\n\"\n",
    "        f\"Qv column     : {cfg.qv_col}\\n\"\n",
    "        f\"CBW column    : {cfg.cbw_col}\\n\"\n",
    "\n",
    "        f\"rw, m, n      : {rw:g}, {float(np.nanmin(m_arr)):g}..{float(np.nanmax(m_arr)):g}, {n:g}\\n\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        f\"B             : {float(B):g}\\n\"\n",
    "        f\"Outputs       : {cfg.out_sw}, {cfg.out_bvwt}, {cfg.out_bvwe}\\n\"\n",
    "        f\"Valid Sw      : {n_valid:,} / {len(df):,}\\n\"\n",
    "    )\n",
    "\n",
    "    if logger:\n",
    "        logger(report)\n",
    "\n",
    "    return {\"analysis_df\": df, \"report\": report}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f683b-ed5d-4f5e-832f-6adacdf276f7",
   "metadata": {},
   "source": [
    "## Curve_Picker_Panel.py\n",
    "\n",
    "### ./apps/merge_gui/ui_panels/curve_picker_panel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2537b60-ee32-428a-ad03-7ef7bfe20df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##CurvePickerPanel\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "\n",
    "from PySide6.QtWidgets import QWidget, QFormLayout, QComboBox, QPushButton, QLabel\n",
    "from PySide6.QtWidgets import QComboBox  # already imported, just FYI\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CHARTBOOK_FILES = {\n",
    "    \"TNPH ρf=1.00 (SLB)\": \"TNPH_1pt0.xlsx\",\n",
    "    \"CNL ρf=1.0 (SLB)\"  : \"CNL_1pt0.xlsx\",\n",
    "    \"CNL ρf=1.1 (SLB)\"  : \"CNL_1pt1.xlsx\",\n",
    "    \"TNPH ρf=1.19 (SLB)\": \"TNPH_1pt19.xlsx\",\n",
    "}\n",
    "DEFAULT_CHARTBOOK_KEY = \"TNPH ρf=1.19 (SLB)\"\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_CANDIDATES: Dict[str, List[str]] = {\n",
    "    \"gr_curve\":   [\"GR_EDTC\", \"HSGR\", \"GR\", \"SGR\", \"HCGR\"],\n",
    "    \"cgr_curve\":  [\"HCGR\", \"CGR\", \"GR_EDTC\"],\n",
    "    \"rhob_curve\": [\"RHOZ\", \"RHOB\"],\n",
    "    \"tnph_curve\": [\"NPOR\", \"TNPH\", \"NPHI\", \"CNL\"],\n",
    "    \"rt_curve\":   [\"AT90\", \"AF90\", \"AT60\", \"AF60\", \"AT30\", \"AF30\", \"AT20\", \"AF20\", \"ILD\", \"RT\"],\n",
    "    \"dtco_curve\": [\"DTCO\", \"DTC\", \"AC\"],\n",
    "    \"pef_curve\":  [\"PEFZ\", \"PEF\"],\n",
    "    \"tcmr_curve\": [\"PHIT_NMR\", \"TCMR\"],\n",
    "    \"cmrp_curve\": [\"PHIE_NMR\", \"CMRP_3MS\", \"CMRP3MS\", \"CMRP\"],\n",
    "    \"cbw_curve\":  [\"CBW\"],\n",
    "    \"ffi_curve\":  [\"FFI\", \"CMFF\"],\n",
    "    \"bvie_curve\": [\"BVIE\", \"BVI_E\"],\n",
    "}\n",
    "\n",
    "\n",
    "def _charts_dir() -> Path:\n",
    "    # curve_picker_panel.py is .../apps/merge_gui/ui_panels/\n",
    "    # charts live in .../apps/merge_gui/data/\n",
    "    here = Path(__file__).resolve()\n",
    "    return (here.parents[1] / \"data\")  # ui_panels -> merge_gui -> data\n",
    "\n",
    "def _chart_path(filename: str) -> str:\n",
    "    p = _charts_dir() / filename\n",
    "    return str(p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def first_present(cols: List[str], candidates: List[str]) -> Optional[str]:\n",
    "    s = set(cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _norm(x, xmin, xmax):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return (x - xmin) / (xmax - xmin)\n",
    "\n",
    "def build_chart_payload(df_chart: pd.DataFrame,\n",
    "                        neutron_col=\"Neutron\",\n",
    "                        rhob_col=\"RHOB\",\n",
    "                        por_col=\"Porosity\",\n",
    "                        rhoma_col=\"Rho_Matrix\",\n",
    "                        cnl_min=-0.05, cnl_max=0.60,\n",
    "                        rhob_min=1.90, rhob_max=3.00):\n",
    "    cnl = pd.to_numeric(df_chart[neutron_col], errors=\"coerce\").to_numpy(float)\n",
    "    rho = pd.to_numeric(df_chart[rhob_col], errors=\"coerce\").to_numpy(float)\n",
    "    por = pd.to_numeric(df_chart[por_col], errors=\"coerce\").to_numpy(float)\n",
    "    rma = pd.to_numeric(df_chart[rhoma_col], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "    m = np.isfinite(cnl) & np.isfinite(rho) & np.isfinite(por) & np.isfinite(rma)\n",
    "    cnl, rho, por, rma = cnl[m], rho[m], por[m], rma[m]\n",
    "\n",
    "    X = np.column_stack([\n",
    "        _norm(cnl, cnl_min, cnl_max),\n",
    "        _norm(rho, rhob_min, rhob_max),\n",
    "    ])\n",
    "    tree = cKDTree(X)\n",
    "\n",
    "    return dict(tree=tree, por=por, rma=rma,\n",
    "                cnl_min=cnl_min, cnl_max=cnl_max,\n",
    "                rhob_min=rhob_min, rhob_max=rhob_max)\n",
    "\n",
    "\n",
    "\n",
    "def chartbook_knn(payload, tnph, rhob, k=3, eps=1e-6):\n",
    "    \"\"\"\n",
    "    kNN inverse-distance weighted interpolation on digitized ND chartbook.\n",
    "\n",
    "    Returns full-length arrays:\n",
    "      phit_est, rhomaa_est\n",
    "    with NaN where TNPH or RHOB are not finite.\n",
    "    \"\"\"\n",
    "\n",
    "    tnph = np.asarray(tnph, dtype=float)\n",
    "    rhob = np.asarray(rhob, dtype=float)\n",
    "\n",
    "    n = len(tnph)\n",
    "    phit = np.full(n, np.nan, dtype=float)\n",
    "    rhomaa = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    # Only query where both logs are finite\n",
    "    valid = np.isfinite(tnph) & np.isfinite(rhob)\n",
    "    if not np.any(valid):\n",
    "        return phit, rhomaa\n",
    "\n",
    "    # ---- Clip to chart limits (prevents wild extrapolation)\n",
    "    tn = np.clip(tnph[valid], payload[\"cnl_min\"], payload[\"cnl_max\"])\n",
    "    rb = np.clip(rhob[valid], payload[\"rhob_min\"], payload[\"rhob_max\"])\n",
    "\n",
    "    # ---- Normalize to chart space\n",
    "    q = np.column_stack([\n",
    "        _norm(tn, payload[\"cnl_min\"], payload[\"cnl_max\"]),\n",
    "        _norm(rb, payload[\"rhob_min\"], payload[\"rhob_max\"]),\n",
    "    ])\n",
    "\n",
    "    # ---- KDTree query\n",
    "    dist, idx = payload[\"tree\"].query(q, k=k, workers=-1)\n",
    "\n",
    "    # Ensure 2D outputs if k == 1\n",
    "    if k == 1:\n",
    "        dist = dist[:, None]\n",
    "        idx = idx[:, None]\n",
    "\n",
    "    # ---- Inverse-distance weights\n",
    "    w = 1.0 / np.maximum(dist, eps)\n",
    "\n",
    "    por_n = payload[\"por\"][idx]\n",
    "    rma_n = payload[\"rma\"][idx]\n",
    "\n",
    "    phit_valid = (w * por_n).sum(axis=1) / w.sum(axis=1)\n",
    "    rhomaa_valid = (w * rma_n).sum(axis=1) / w.sum(axis=1)\n",
    "\n",
    "    # ---- Insert back into full-length arrays\n",
    "    phit[valid] = phit_valid\n",
    "    rhomaa[valid] = rhomaa_valid\n",
    "\n",
    "    return phit, rhomaa\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CurvePickerPanel(QWidget):\n",
    "    \"\"\"\n",
    "    Dropdowns for selecting the active curve per family.\n",
    "    Stores selections in controller.state.params and refreshes plots.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, controller, candidates: Dict[str, List[str]] | None = None):\n",
    "        super().__init__()\n",
    "        self.controller = controller\n",
    "        self.candidates = candidates or DEFAULT_CANDIDATES\n",
    "\n",
    "        self.form = QFormLayout(self)\n",
    "        self.info = QLabel(\"Load a dataset to populate curve lists.\")\n",
    "        self.form.addRow(self.info)\n",
    "\n",
    "        self.combos: Dict[str, QComboBox] = {}\n",
    "\n",
    "        \n",
    "        self.btn_autopick = QPushButton(\"Auto-pick from candidates\")\n",
    "        self.btn_apply = QPushButton(\"Apply + Refresh Plots\")\n",
    "        self.btn_chartbook_phi = QPushButton(\"1) Calc Chartbook PHI (NEUT–RHOB)\")\n",
    "        \n",
    "        # --- Chartbook fluid density selector (SLB digitized charts)\n",
    "        self.cb_chartbook = QComboBox()\n",
    "        for k in CHARTBOOK_FILES.keys():\n",
    "            self.cb_chartbook.addItem(k, userData=k)\n",
    "        # default selection\n",
    "        idx0 = self.cb_chartbook.findText(DEFAULT_CHARTBOOK_KEY)\n",
    "        self.cb_chartbook.setCurrentIndex(idx0 if idx0 >= 0 else 0)\n",
    "        \n",
    "        self.btn_autopick.clicked.connect(self.autopick)\n",
    "        self.btn_apply.clicked.connect(self.apply_to_state)\n",
    "        self.btn_chartbook_phi.clicked.connect(self.calc_chartbook_phi)\n",
    "        \n",
    "        # When density changes, rebuild payload (but don’t recompute curves until button press)\n",
    "        self.cb_chartbook.currentIndexChanged.connect(self._on_chartbook_changed)\n",
    "        \n",
    "        self.form.addRow(self.btn_autopick)\n",
    "        self.form.addRow(\"Chartbook (fluid density)\", self.cb_chartbook)\n",
    "        self.form.addRow(self.btn_chartbook_phi)\n",
    "        self.form.addRow(self.btn_apply)\n",
    "        \n",
    "\n",
    "    def _on_chartbook_changed(self, *_):\n",
    "        # Drop cached payload so next compute rebuilds with selected SLB chart\n",
    "        if hasattr(self, \"_chartbook_payload\"):\n",
    "            delattr(self, \"_chartbook_payload\")\n",
    "        self.info.setText(\"Chartbook selection changed. Click '1) Calc Chartbook PHI' to recompute.\")\n",
    "\n",
    "\n",
    "\n",
    "    def populate_from_columns(self, columns: List[str]):\n",
    "        # wipe existing combo rows\n",
    "        for cb in self.combos.values():\n",
    "            cb.blockSignals(True)\n",
    "            cb.clear()\n",
    "            cb.blockSignals(False)\n",
    "        self.combos = {}\n",
    "    \n",
    "        # Fixed rows in the form:\n",
    "        #   Row 0: info label\n",
    "        #   Tail rows (bottom, always kept):\n",
    "        #     - Auto-pick button\n",
    "        #     - Chartbook dropdown\n",
    "        #     - Chartbook compute button\n",
    "        #     - Apply button\n",
    "        FIXED_TAIL_ROWS = 4\n",
    "        KEEP_ROWS = 1 + FIXED_TAIL_ROWS  # info + tail controls\n",
    "    \n",
    "        # Remove any previously inserted curve-combo rows (rows between info and tail controls)\n",
    "        while self.form.rowCount() > KEEP_ROWS:\n",
    "            self.form.removeRow(1)\n",
    "    \n",
    "        self.info.setText(f\"{len(columns)} curves available. Choose active curves:\")\n",
    "    \n",
    "        # Insert curve-family combos above the fixed tail controls\n",
    "        insert_at = self.form.rowCount() - FIXED_TAIL_ROWS\n",
    "    \n",
    "        for key in self.candidates.keys():\n",
    "            cb = QComboBox()\n",
    "            cb.addItem(\"(none)\", userData=None)\n",
    "            for col in columns:\n",
    "                cb.addItem(col, userData=col)\n",
    "    \n",
    "            cb.currentIndexChanged.connect(self._on_change)\n",
    "            self.combos[key] = cb\n",
    "    \n",
    "            self.form.insertRow(insert_at, key, cb)\n",
    "            insert_at += 1  # next combo goes under the previous one\n",
    "    \n",
    "        # Set initial picks without triggering lots of refreshes\n",
    "        for cb in self.combos.values():\n",
    "            cb.blockSignals(True)\n",
    "        self.autopick()\n",
    "        for cb in self.combos.values():\n",
    "            cb.blockSignals(False)\n",
    "    \n",
    "        # One clean update at end\n",
    "        self.apply_to_state()\n",
    "\n",
    "\n",
    "\n",
    "    def _columns_from_state(self) -> List[str]:\n",
    "        # Prefer Dataset if present\n",
    "        ds = getattr(self.controller.state, \"dataset\", None)\n",
    "        if ds is not None:\n",
    "            return list(ds.data.columns)\n",
    "\n",
    "        # fallback: analysis_df if present\n",
    "        df = getattr(self.controller.state, \"analysis_df\", None)\n",
    "        if df is not None:\n",
    "            return list(df.columns)\n",
    "\n",
    "        return []\n",
    "\n",
    "    def autopick(self):\n",
    "        cols = self._columns_from_state()\n",
    "        if not cols:\n",
    "            return\n",
    "\n",
    "        for key, cand in self.candidates.items():\n",
    "            cb = self.combos.get(key)\n",
    "            if cb is None:\n",
    "                continue\n",
    "\n",
    "            pick = first_present(cols, cand)\n",
    "            if pick is None:\n",
    "                cb.setCurrentIndex(0)\n",
    "            else:\n",
    "                idx = cb.findText(pick)\n",
    "                cb.setCurrentIndex(idx if idx >= 0 else 0)\n",
    "\n",
    "    def _on_change(self, *_):\n",
    "        # If you prefer “Apply only”, comment this out\n",
    "        self.apply_to_state()\n",
    "\n",
    "    def apply_to_state(self):\n",
    "        p = getattr(self.controller.state, \"params\", None)\n",
    "        if p is None:\n",
    "            self.controller.state.params = {}\n",
    "            p = self.controller.state.params\n",
    "\n",
    "        for key, cb in self.combos.items():\n",
    "            val = cb.currentData()\n",
    "            if val is None:\n",
    "                p.pop(key, None)\n",
    "            else:\n",
    "                p[key] = val\n",
    "\n",
    "        # refresh plots using new active curves\n",
    "        if hasattr(self.controller, \"update_plots\"):\n",
    "            self.controller.update_plots()\n",
    "        else:\n",
    "            self.controller.refresh_plots()\n",
    "\n",
    "\n",
    "    def calc_chartbook_phi(self):\n",
    "        df = getattr(self.controller.state, \"analysis_df\", None)\n",
    "        if df is None or df.empty:\n",
    "            self.info.setText(\"No analysis_df loaded.\")\n",
    "            return\n",
    "    \n",
    "        p = getattr(self.controller.state, \"params\", {}) or {}\n",
    "        rhob_curve = p.get(\"rhob_curve\") or first_present(list(df.columns), self.candidates[\"rhob_curve\"])\n",
    "        tnph_curve = p.get(\"tnph_curve\") or first_present(list(df.columns), self.candidates[\"tnph_curve\"])\n",
    "    \n",
    "        if not rhob_curve or not tnph_curve:\n",
    "            self.info.setText(\"Need RHOB and TNPH families present to compute Chartbook PHI.\")\n",
    "            return\n",
    "    \n",
    "        rhob = pd.to_numeric(df[rhob_curve], errors=\"coerce\").to_numpy(float)\n",
    "        tnph = pd.to_numeric(df[tnph_curve], errors=\"coerce\").to_numpy(float)\n",
    "    \n",
    "        # Cache payload (build once)\n",
    "        if not hasattr(self, \"_chartbook_payload\"):\n",
    "            key = DEFAULT_CHARTBOOK_KEY\n",
    "            if hasattr(self, \"cb_chartbook\") and self.cb_chartbook is not None:\n",
    "                try:\n",
    "                    key = self.cb_chartbook.currentData() or DEFAULT_CHARTBOOK_KEY\n",
    "                except RuntimeError:\n",
    "                    key = DEFAULT_CHARTBOOK_KEY\n",
    "        \n",
    "            rel_name = CHARTBOOK_FILES.get(key, CHARTBOOK_FILES[DEFAULT_CHARTBOOK_KEY])\n",
    "            file_path = _chart_path(rel_name)\n",
    "        \n",
    "            \n",
    "            df_chart = pd.read_excel(file_path, index_col=False)\n",
    "            #self.controller.state.chartbook_df = df_chart   # <-- ADD THIS LINE\n",
    "            self.controller.state.chartbook_df = df_chart\n",
    "            self._chartbook_payload = build_chart_payload(df_chart)\n",
    "\n",
    "\n",
    "        \n",
    "        phit, rhomaa = chartbook_knn(self._chartbook_payload, tnph, rhob, k=3)\n",
    "    \n",
    "        df[\"PHIT_CHART\"] = phit\n",
    "        df[\"RHOMAA_CHART\"] = rhomaa\n",
    "        self.controller.state.analysis_df = df\n",
    "    \n",
    "        # Make new curves visible in dropdowns + refresh plots\n",
    "        self.populate_from_columns(list(df.columns))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1ad18-97da-4006-8963-db22e6cf1c79",
   "metadata": {},
   "source": [
    "## ui_main_window.py\n",
    "\n",
    "### ./apps/merge_gui/ui_main_window.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627f1c1-129e-4382-8c3e-92d75a7efe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apps/merge_gui/ui_main_window.py\n",
    "from __future__ import annotations\n",
    "\n",
    "print(\">>> LOADING ui_main_window.py <<<\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import lasio\n",
    "\n",
    "from PySide6.QtCore import Qt\n",
    "from PySide6.QtGui import QAction\n",
    "from PySide6.QtWidgets import (\n",
    "    QMainWindow,\n",
    "    QDockWidget,\n",
    "    QTextEdit,\n",
    "    QFileDialog,\n",
    ")\n",
    "\n",
    "from petrocore.models.dataset import Dataset\n",
    "from petrocore.workflow.state import WorkflowState\n",
    "from apps.merge_gui.controllers.workflow_controller import WorkflowController\n",
    "\n",
    "from apps.merge_gui.ui_panels.depth_panel import DepthRangePanel\n",
    "from apps.merge_gui.ui_panels.tops_panel import TopsPanel\n",
    "from apps.merge_gui.ui_panels.plots_panel import PlotsPanel\n",
    "from apps.merge_gui.ui_panels.curve_picker_panel import CurvePickerPanel\n",
    "from apps.merge_gui.controllers.workflow_controller import WorkflowController\n",
    "from apps.merge_gui.ui_panels.plots_panel import PlotsPanel\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Families + diagnostics\n",
    "# -----------------------------\n",
    "def default_families_map():\n",
    "    return {\n",
    "        \"GR\":   [\"GR_EDTC\",\"HSGR\", \"GR\", \"SGR\", \"HGR\"],\n",
    "        \"CGR\":  [\"HCGR\", \"CGR\"],\n",
    "        \"RHOB\": [\"RHOZ\", \"RHOB\"],\n",
    "        \"TNPH\": [\"TNPH\", \"NPHI\", \"CNL\", \"NPOR\"],\n",
    "        \"RT\":   [\"AT90\", \"AF90\", \"AO90\", \"ILD\", \"RT\",\"RD\"],\n",
    "        \"TCMR\": [\"PHIT_NMR\", \"TCMR\", \"MPHIS\"],\n",
    "        \"CMRP\": [\"PHIE_NMR\", \"CMRP_3MS\", \"CMRP3MS\", \"CMRP\", \"MPHI\"],\n",
    "        \"CBW\":  [\"CBW\"],\n",
    "        \"BVIE\": [\"BVIE\", \"BVI_E\",\"MBVI\"],\n",
    "        \"FFI\":  [\"FFI\", \"CMFF\",\"MFFI\"],\n",
    "        \"PEF\":  [\"PEFZ\", \"PEF\",\"PE\"],\n",
    "        \"DTCO\": [\"DTCO\", \"DT\", \"AC\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def quick_curve_audit(df: pd.DataFrame):\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    rt_cands = [\"AT90\", \"AF90\", \"AO90\", \"ILD\", \"RT\", \"RES\", \"RLA1\", \"RDEP\", \"RD\", \"RXOZ\", \"RXO8\"]\n",
    "    nmr_cands = [\n",
    "        \"PHIT_NMR\", \"PHIE_NMR\", \"TCMR\", \"CMRP_3MS\", \"CMRP3MS\", \"CMRP\",\n",
    "        \"CBW\", \"FFI\", \"BVI\",\"MBVI\", \"BVIE\", \"MPHI\", \"MPHIS\"\n",
    "    ]\n",
    "\n",
    "    def present(cands):\n",
    "        return [c for c in cands if c in cols]\n",
    "\n",
    "    print(\"\\n=== CURVE AUDIT ===\")\n",
    "    print(\"Columns:\", len(cols))\n",
    "    print(\"Rt present:\", present(rt_cands))\n",
    "    print(\"NMR present:\", present(nmr_cands))\n",
    "\n",
    "\n",
    "def resolve_family_names(df: pd.DataFrame, families_map: dict):\n",
    "    cols = set(df.columns)\n",
    "    resolved = {}\n",
    "    for fam, cands in families_map.items():\n",
    "        hit = next((c for c in cands if c in cols), None)\n",
    "        resolved[fam] = hit\n",
    "\n",
    "    print(\"\\n=== RESOLVED FAMILY CURVES (key) ===\")\n",
    "    for fam in (\"RT\", \"TCMR\", \"CMRP\", \"CBW\", \"FFI\", \"BVIE\"):\n",
    "        print(f\"{fam:5s} -> {resolved.get(fam)}\")\n",
    "\n",
    "    return resolved\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main window\n",
    "# -----------------------------\n",
    "class MainWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Petro Workflow GUI\")\n",
    "\n",
    "        self.state = WorkflowState()\n",
    "\n",
    "        self.console = QTextEdit()\n",
    "        self.console.setReadOnly(True)\n",
    "\n",
    "        # 1) Create controller first (plots_panel can be None for the moment)\n",
    "        self.controller = WorkflowController(self.state, None, self.console)\n",
    "\n",
    "        # 2) Now create PlotsPanel with controller\n",
    "        self.plots_panel = PlotsPanel(self.controller)\n",
    "        self.setCentralWidget(self.plots_panel)\n",
    "\n",
    "        # 3) Attach plots_panel back to controller\n",
    "        self.controller.plots_panel = self.plots_panel\n",
    "\n",
    "        # 4) Other panels that need controller\n",
    "        self.curve_picker = CurvePickerPanel(self.controller)\n",
    "\n",
    "        self.resize(1400, 800)\n",
    "\n",
    "        self._build_menu()\n",
    "        self._build_docks()\n",
    "\n",
    "\n",
    "    # -------------------------\n",
    "    # UI docks\n",
    "    # -------------------------\n",
    "    def _build_docks(self):\n",
    "        # ---- Curves dock (left) ----\n",
    "        dock_curves = QDockWidget(\"Curves\", self)\n",
    "        dock_curves.setWidget(self.curve_picker)\n",
    "        self.addDockWidget(Qt.LeftDockWidgetArea, dock_curves)\n",
    "    \n",
    "        # ---- Controls + Tops as TABBED dock ----\n",
    "        self.depth_panel = DepthRangePanel(self.controller)\n",
    "        self.tops_panel = TopsPanel(self.controller)\n",
    "    \n",
    "        dock_controls = QDockWidget(\"Controls / Tops\", self)\n",
    "        dock_controls.setAllowedAreas(Qt.LeftDockWidgetArea | Qt.RightDockWidgetArea)\n",
    "    \n",
    "        from PySide6.QtWidgets import QTabWidget\n",
    "        tabs = QTabWidget()\n",
    "        tabs.addTab(self.depth_panel, \"Depth\")\n",
    "        tabs.addTab(self.tops_panel, \"Tops\")\n",
    "    \n",
    "        dock_controls.setWidget(tabs)\n",
    "        self.addDockWidget(Qt.LeftDockWidgetArea, dock_controls)\n",
    "    \n",
    "        # ---- Log / console at bottom ----\n",
    "        dock_log = QDockWidget(\"Log\", self)\n",
    "        dock_log.setWidget(self.console)\n",
    "        dock_log.setFeatures(QDockWidget.DockWidgetMovable | QDockWidget.DockWidgetClosable)\n",
    "\n",
    "        self.addDockWidget(Qt.BottomDockWidgetArea, dock_log)\n",
    "    \n",
    "        # ---- Let Qt auto-arrange nicely ----\n",
    "        self.resizeDocks([dock_curves, dock_controls], [200, 260], Qt.Horizontal)\n",
    "        self.resizeDocks([dock_log], [140], Qt.Vertical)\n",
    "\n",
    "    # -------------------------\n",
    "    # Menu\n",
    "    # -------------------------\n",
    "    def _build_menu(self):\n",
    "        menubar = self.menuBar()\n",
    "        file_menu = menubar.addMenu(\"&File\")\n",
    "\n",
    "        act_open = QAction(\"Open LAS…\", self)\n",
    "        act_open.setShortcut(\"Ctrl+O\")\n",
    "        act_open.triggered.connect(self._on_open_las)\n",
    "        file_menu.addAction(act_open)\n",
    "\n",
    "        act_tops_core = QAction(\"Open Tops (Core)…\", self)\n",
    "        act_tops_core.triggered.connect(self._on_open_tops_core)\n",
    "        file_menu.addAction(act_tops_core)\n",
    "\n",
    "        file_menu.addSeparator()\n",
    "\n",
    "        act_quit = QAction(\"Quit\", self)\n",
    "        act_quit.setShortcut(\"Ctrl+Q\")\n",
    "        act_quit.triggered.connect(self.close)\n",
    "        file_menu.addAction(act_quit)\n",
    "\n",
    "    # -------------------------\n",
    "    # File -> Open LAS\n",
    "    # -------------------------\n",
    "    def _on_open_las(self):\n",
    "        path, _ = QFileDialog.getOpenFileName(\n",
    "            self,\n",
    "            \"Open LAS\",\n",
    "            \"\",\n",
    "            \"LAS files (*.las *.LAS);;All files (*.*)\"\n",
    "        )\n",
    "        if not path:\n",
    "            return\n",
    "        self.open_las_file(path)\n",
    "\n",
    "    def open_las_file(self, path: str):\n",
    "        las = lasio.read(path)\n",
    "\n",
    "        # ---- WELL name from LAS header (preferred over filename) ----\n",
    "        try:\n",
    "            well_name = str(las.well.WELL.value).strip()\n",
    "        except Exception:\n",
    "            well_name = \"\"\n",
    "        \n",
    "        if well_name:\n",
    "            self.controller.state.well_name = well_name\n",
    "            self.console.append(f\"[WELL] LAS WELL = {well_name}\")\n",
    "        else:\n",
    "            self.controller.state.well_name = \"\"\n",
    "            self.console.append(\"[WELL] LAS WELL not found; will fallback to filename\")\n",
    "\n",
    "        \n",
    "        df = las.df()\n",
    "        df.index = df.index.astype(float)\n",
    "\n",
    "        # Diagnostics\n",
    "        quick_curve_audit(df)\n",
    "        resolve_family_names(df, default_families_map())\n",
    "\n",
    "        ds = Dataset(\n",
    "            data=df,\n",
    "            families_map=default_families_map(),\n",
    "            name=os.path.basename(path),\n",
    "        )\n",
    "\n",
    "        # send dataset into workflow\n",
    "        self.controller.set_dataset(ds)\n",
    "\n",
    "        # update curve picker\n",
    "        self.curve_picker.populate_from_columns(list(df.columns))\n",
    "\n",
    "        self.console.append(f\"Loaded LAS: {os.path.basename(path)}\")\n",
    "\n",
    "        # after controller.set_dataset(ds)\n",
    "        dmin, dmax = float(df.index.min()), float(df.index.max())\n",
    "        self.depth_panel.set_depth_limits(dmin, dmax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------\n",
    "    # File -> Open Tops (Core)\n",
    "    # -------------------------\n",
    "\n",
    "    def _infer_well_name_for_tops(self) -> str:\n",
    "        # 1) Prefer LAS header-derived well name\n",
    "        w = getattr(self.controller.state, \"well_name\", \"\") or \"\"\n",
    "        w = str(w).strip()\n",
    "        if w:\n",
    "            return w\n",
    "    \n",
    "        # 2) Fallback: LAS base filename\n",
    "        ds = getattr(self.controller.state, \"dataset\", None)\n",
    "        if ds is None:\n",
    "            return \"\"\n",
    "        name = str(getattr(ds, \"name\", \"\")).strip()\n",
    "        if name.lower().endswith(\".las\"):\n",
    "            name = name[:-4]\n",
    "        return name\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _on_open_tops_core(self):\n",
    "        path, _ = QFileDialog.getOpenFileName(\n",
    "            self,\n",
    "            \"Open Tops (Core) Excel\",\n",
    "            \"\",\n",
    "            \"Excel files (*.xlsx *.xls);;All files (*.*)\"\n",
    "        )\n",
    "        if not path:\n",
    "            return\n",
    "\n",
    "        well_key = self._infer_well_name_for_tops()\n",
    "        self.open_tops_core_xlsx(path, well_key)\n",
    "\n",
    "    def open_tops_core_xlsx(self, tops_xlsx_path: str, well_key: str):\n",
    "        \"\"\"\n",
    "        Reads master tops file with columns:\n",
    "          - Welll Name  (note 3 'l' per your file)\n",
    "          - Well No\n",
    "          - Formation\n",
    "          - Top (ft)\n",
    "\n",
    "        Filters to selected well (normalized) using LAS base filename.\n",
    "        Stores state.tops_df with columns: Top, Depth.\n",
    "        \"\"\"\n",
    "        def normalize_name(s: str) -> str:\n",
    "            return re.sub(r\"\\s+\", \" \", str(s).strip().upper())\n",
    "\n",
    "        if not os.path.exists(tops_xlsx_path):\n",
    "            self.console.append(f\"⚠️ Tops file not found: {os.path.abspath(tops_xlsx_path)}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            df = pd.read_excel(\n",
    "                tops_xlsx_path,\n",
    "                usecols=[\"Welll Name\", \"Well No\", \"Formation\", \"Top (ft)\"]\n",
    "            ).copy()\n",
    "        except Exception as e:\n",
    "            self.console.append(f\"⚠️ Failed reading tops Excel: {e}\")\n",
    "            return\n",
    "\n",
    "        df.columns = [\"well_name\", \"well_no\", \"formation\", \"top_ft\"]\n",
    "        df[\"well_name_norm\"] = df[\"well_name\"].apply(normalize_name)\n",
    "        df[\"formation\"] = df[\"formation\"].astype(str).str.strip()\n",
    "        df[\"top_ft\"] = pd.to_numeric(df[\"top_ft\"], errors=\"coerce\")\n",
    "\n",
    "        df = df.dropna(subset=[\"top_ft\"]).sort_values([\"well_name_norm\", \"top_ft\"]).reset_index(drop=True)\n",
    "\n",
    "        well_norm = normalize_name(well_key)\n",
    "        df_well = df[df[\"well_name_norm\"] == well_norm].copy()\n",
    "        df_well = df_well.sort_values(\"top_ft\").reset_index(drop=True)\n",
    "\n",
    "        if df_well.empty:\n",
    "            self.console.append(f\"⚠️ No tops found for well='{well_key}' (norm='{well_norm}')\")\n",
    "            self.controller.state.tops_df = pd.DataFrame(columns=[\"Top\", \"Depth\"])\n",
    "            self.controller.refresh_plots()\n",
    "            return\n",
    "\n",
    "        tops_df = pd.DataFrame({\n",
    "            \"Top\": df_well[\"formation\"].astype(str),\n",
    "            \"Depth\": df_well[\"top_ft\"].astype(float),\n",
    "        })\n",
    "\n",
    "        self.controller.state.tops_df = tops_df\n",
    "        self.controller.refresh_plots()\n",
    "        self.console.append(f\"✅ Tops loaded for '{well_key}' | n={len(tops_df)}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Demo loader\n",
    "    # -------------------------\n",
    "    def demo_load(self, ds: Dataset):\n",
    "        self.controller.set_dataset(ds)\n",
    "\n",
    "        df = ds.data\n",
    "        dmin, dmax = float(df.index.min()), float(df.index.max())\n",
    "        self.depth_panel.set_depth_limits(dmin, dmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2d26b-3588-4d2e-aa64-aa6482a4313a",
   "metadata": {},
   "source": [
    "## workflow_controller.py\n",
    "\n",
    "### ./apps/merge_gui/controllers/workflow_controller.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b12dd4-aed5-4c0e-bf3d-87fc01d992c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apps/merge_gui/controllers/workflow_controller.py\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "from petrocore.models.dataset import Dataset\n",
    "\n",
    "class WorkflowController:\n",
    "    def __init__(self, state, plots_panel, console):\n",
    "        self.state = state\n",
    "        self.plots_panel = plots_panel\n",
    "        self.console = console\n",
    "\n",
    "    # -------------------------\n",
    "    # Dataset\n",
    "    # -------------------------\n",
    "    def set_dataset(self, dataset: Dataset):\n",
    "        self.state.dataset = dataset\n",
    "    \n",
    "        dmin, dmax = float(dataset.depth.min()), float(dataset.depth.max())\n",
    "        self.state.depth_limits = (dmin, dmax)\n",
    "    \n",
    "        self.state.analysis_df = dataset.data\n",
    "        self.state.analysis_df_view = dataset.data\n",
    "    \n",
    "        self.console.append(\n",
    "            f\"Dataset loaded: {dataset.name} | curves={len(dataset.curves())} | depth={dmin:.1f}-{dmax:.1f}\"\n",
    "        )\n",
    "    \n",
    "        self.refresh_plots()\n",
    "    \n",
    "\n",
    "    def set_depth_range(self, top: float, base: float):\n",
    "        \"\"\"\n",
    "        Called by DepthRangePanel or TopsPanel.\n",
    "        Sets analysis window from full dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            t = float(top)\n",
    "            b = float(base)\n",
    "        except Exception:\n",
    "            return\n",
    "    \n",
    "        if b < t:\n",
    "            t, b = b, t\n",
    "    \n",
    "        self.state.depth_top = t\n",
    "        self.state.depth_base = b\n",
    "    \n",
    "        ds = getattr(self.state, \"dataset\", None)\n",
    "        if ds is None:\n",
    "            return\n",
    "    \n",
    "        df = ds.data\n",
    "    \n",
    "        try:\n",
    "            win = df.loc[t:b].copy()\n",
    "        except Exception:\n",
    "            win = df\n",
    "    \n",
    "        self.state.analysis_df = win\n",
    "        self.state.analysis_df_view = win\n",
    "    \n",
    "        self.console.append(f\"Analysis window set: {t:.2f} – {b:.2f} ft | rows={len(win)}\")\n",
    "    \n",
    "        self.refresh_plots()\n",
    "\n",
    "\n",
    "    def refresh_plots(self):\n",
    "        ds = getattr(self.state, \"dataset\", None)\n",
    "        if ds is None:\n",
    "            return\n",
    "    \n",
    "        # full vs zoi\n",
    "        plot_ds = ds.zoi if (getattr(ds, \"zoi_depth_range\", None) is not None) else ds\n",
    "    \n",
    "        # Always keep the full df available\n",
    "        self.state.analysis_df = plot_ds.data\n",
    "    \n",
    "        # If a depth range is set, build a view; otherwise clear it\n",
    "        t = getattr(self.state, \"depth_top\", None)\n",
    "        b = getattr(self.state, \"depth_base\", None)\n",
    "    \n",
    "        if t is not None and b is not None:\n",
    "            try:\n",
    "                t = float(t); b = float(b)\n",
    "                if b < t:\n",
    "                    t, b = b, t\n",
    "                self.state.analysis_df_view = self.state.analysis_df.loc[t:b].copy()\n",
    "            except Exception:\n",
    "                self.state.analysis_df_view = self.state.analysis_df\n",
    "        else:\n",
    "            self.state.analysis_df_view = None\n",
    "    \n",
    "        self.plots_panel.update_all(self.state)\n",
    "    \n",
    "\n",
    "    # -------------------------\n",
    "    # ZOI\n",
    "    # -------------------------\n",
    "    def set_zoi(self, top: float, base: float):\n",
    "        ds: Optional[Dataset] = getattr(self.state, \"dataset\", None)\n",
    "        if ds is None:\n",
    "            self.console.append(\"⚠️ No dataset loaded.\")\n",
    "            return\n",
    "\n",
    "        z = ds.set_zoi(top, base)\n",
    "        self.console.append(f\"ZOI set: {ds.zoi_depth_range[0]:.1f}-{ds.zoi_depth_range[1]:.1f} ft | rows={len(z.data)}\")\n",
    "        self.refresh_plots()\n",
    "\n",
    "    def clear_zoi(self):\n",
    "        ds: Optional[Dataset] = getattr(self.state, \"dataset\", None)\n",
    "        if ds is None:\n",
    "            return\n",
    "        ds.clear_zoi()\n",
    "        self.console.append(\"ZOI cleared.\")\n",
    "        self.refresh_plots()\n",
    "\n",
    "\n",
    "    def update_plots(self):\n",
    "        \"\"\"Back-compat for UI panels that call update_plots().\"\"\"\n",
    "        self.refresh_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c822ba4e-5d58-4d0e-8616-7a781f78b95e",
   "metadata": {},
   "source": [
    "## main_window.py\n",
    "\n",
    "### ./petrocore/main_window.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca133d6-a54a-46dd-aa2e-42ca7503dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_gui/ui/main_window.py\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from PySide6.QtCore import Qt\n",
    "from PySide6.QtWidgets import (\n",
    "    QMainWindow, QDockWidget, QTabWidget, QTextEdit,\n",
    "    QWidget, QVBoxLayout, QLabel\n",
    ")\n",
    "\n",
    "from petrocore.viz.log_canvas_pg import LogCanvasPG\n",
    "from petrocore.models.dataset import Dataset\n",
    "\n",
    "from petrocore.workflow.state import WorkflowState\n",
    "from petrocore.workflow.pipeline import WorkflowRegistry\n",
    "\n",
    "from apps.merge_gui.controllers.workflow_controller import WorkflowController\n",
    "from apps.merge_gui.ui_panels.plots_panel import PlotsPanel\n",
    "\n",
    "\n",
    "class MainWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Merge/QC + Petro Workflow\")\n",
    "        self.resize(1600, 900)\n",
    "\n",
    "        # -------------------------\n",
    "        # A) CENTRAL UI: tabs\n",
    "        # -------------------------\n",
    "        self.tabs = QTabWidget()\n",
    "        self.setCentralWidget(self.tabs)\n",
    "\n",
    "        # Tab 1: Log canvas\n",
    "        self.log_canvas = LogCanvasPG()\n",
    "        self.tabs.addTab(self.log_canvas, \"Log Canvas\")\n",
    "\n",
    "        # Tab 2: Workflow plots (your new workflow UI lives here)\n",
    "        self.state = WorkflowState()\n",
    "        self.plots_panel = PlotsPanel()\n",
    "        self.tabs.addTab(self.plots_panel, \"Workflow Plots\")\n",
    "\n",
    "        # Placeholder tabs (keep for now)\n",
    "        self.tabs.addTab(self._placeholder(\"QC\"), \"QC\")\n",
    "        self.tabs.addTab(self._placeholder(\"Alignment Score\"), \"Alignment Score\")\n",
    "\n",
    "        # -------------------------\n",
    "        # B) CONTROLLER + LOG\n",
    "        # -------------------------\n",
    "        self.console = QTextEdit()\n",
    "        self.console.setReadOnly(True)\n",
    "\n",
    "        self.controller = WorkflowController(\n",
    "            state=self.state,\n",
    "            plots_panel=self.plots_panel,\n",
    "            log_console=self.console\n",
    "        )\n",
    "\n",
    "        # Configure workflow steps / defaults\n",
    "        self._configure_workflow()\n",
    "\n",
    "        # -------------------------\n",
    "        # C) DOCKS (existing merge/qc layout)\n",
    "        # -------------------------\n",
    "        self.addDockWidget(Qt.LeftDockWidgetArea,  self._dock(\"Project\",         self._placeholder(\"Project Tree\")))\n",
    "        self.addDockWidget(Qt.LeftDockWidgetArea,  self._dock(\"Curve Inventory\", self._placeholder(\"Curve list + families\")))\n",
    "        self.addDockWidget(Qt.RightDockWidgetArea, self._dock(\"Alignment\",       self._placeholder(\"Bulk shift + anchors\")))\n",
    "        self.addDockWidget(Qt.RightDockWidgetArea, self._dock(\"Merge\",           self._placeholder(\"Merge params + preview\")))\n",
    "        self.addDockWidget(Qt.RightDockWidgetArea, self._dock(\"Export\",          self._placeholder(\"Export options\")))\n",
    "\n",
    "        dock_log = QDockWidget(\"Workflow Log\", self)\n",
    "        dock_log.setWidget(self.console)\n",
    "        self.addDockWidget(Qt.BottomDockWidgetArea, dock_log)\n",
    "\n",
    "    # -------------------------\n",
    "    # UI helpers\n",
    "    # -------------------------\n",
    "    def _dock(self, title: str, widget: QWidget) -> QDockWidget:\n",
    "        d = QDockWidget(title, self)\n",
    "        d.setWidget(widget)\n",
    "        d.setFeatures(QDockWidget.DockWidgetMovable | QDockWidget.DockWidgetFloatable)\n",
    "        return d\n",
    "\n",
    "    def _placeholder(self, text: str) -> QWidget:\n",
    "        w = QWidget()\n",
    "        lay = QVBoxLayout(w)\n",
    "        lay.addWidget(QLabel(text))\n",
    "        lay.addStretch(1)\n",
    "        return w\n",
    "\n",
    "    # -------------------------\n",
    "    # Workflow config\n",
    "    # -------------------------\n",
    "    def _configure_workflow(self):\n",
    "        self.state.registry = WorkflowRegistry()\n",
    "\n",
    "        from petrocore.workflow.steps_petrophysics import (\n",
    "            step_vsh_hl,\n",
    "            step_cbw,\n",
    "            step_waxman_smits,\n",
    "            step_lith_opt,\n",
    "        )\n",
    "\n",
    "        self.state.registry.add(\"vsh_hl\", \"Hodges–Lehmann Vsh\", step_vsh_hl, enabled_by_default=True)\n",
    "        self.state.registry.add(\"cbw\",    \"Clay Bound Water + PHIE\", step_cbw, enabled_by_default=True)\n",
    "        self.state.registry.add(\"ws\",     \"Waxman–Smits Sw + BVW\", step_waxman_smits, enabled_by_default=True)\n",
    "        self.state.registry.add(\"lith\",   \"Lithology Optimization\", step_lith_opt, enabled_by_default=False)\n",
    "\n",
    "        self.state.enabled_steps = {s.key: s.enabled_by_default for s in self.state.registry.steps}\n",
    "\n",
    "        self.state.params.update({\n",
    "            \"gr_curve\": \"GR\",\n",
    "            \"rhob_curve\": \"RHOZ\",\n",
    "            \"tnph_curve\": \"TNPH\",\n",
    "            \"rt_curve\": \"RT\",\n",
    "            \"Rw\": 0.08,\n",
    "            \"m\": 2.0,\n",
    "            \"n\": 2.0,\n",
    "            \"B\": 4.5,\n",
    "        })\n",
    "\n",
    "    # -------------------------\n",
    "    # Quick test hook\n",
    "    # -------------------------\n",
    "    def demo_load(self, ds: Dataset):\n",
    "        # 1) Log canvas\n",
    "        self.log_canvas.set_dataset(ds)\n",
    "        self.log_canvas.set_tracks(self.log_canvas.standard_4track_template(ds))\n",
    "\n",
    "        # 2) Workflow side\n",
    "        self.controller.set_dataset(ds)   # IMPORTANT: controller expects Dataset (not df)\n",
    "        self.console.append(f\"Demo loaded dataset: {ds.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede897d-b6a2-4de9-9a52-451fd050c645",
   "metadata": {},
   "source": [
    "## dataset.py\n",
    "\n",
    "### ./petrocore/models/dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f2725-73e2-4324-9783-70f41bb84f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages/petrocore/petrocore/models/dataset.py\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, asdict, field\n",
    "#from typing import Dict, List, Optional, Any, Iterable\n",
    "from typing import Dict, List, Optional, Any, Iterable, Tuple\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CurveMeta:\n",
    "    name: str\n",
    "    units: str = \"\"\n",
    "    family: str = \"\"                 # e.g. \"GR\", \"RHOB\", \"TNPH\", \"RT\", \"NMR\"\n",
    "    source: str = \"\"                 # e.g. \"run0\", \"merged\", filename, etc.\n",
    "    description: str = \"\"\n",
    "    role: str = \"\"                   # e.g. \"key\", \"aux\"\n",
    "    display: Dict[str, Any] = field(default_factory=dict)  # future: scale, etc.\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Core data contract shared by both GUIs.\n",
    "    - data: DataFrame indexed by depth (float), columns are curve mnemonics\n",
    "    - meta: per-curve metadata\n",
    "    - families_map: family -> candidate curves in preference order\n",
    "    - history: list of steps / audit records\n",
    "    \"\"\"\n",
    "    data: pd.DataFrame\n",
    "    meta: Dict[str, CurveMeta] = field(default_factory=dict)\n",
    "    families_map: Dict[str, List[str]] = field(default_factory=dict)\n",
    "    units_map: Dict[str, str] = field(default_factory=dict)\n",
    "    history: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    name: str = \"dataset\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------\n",
    "    # ZOI / analysis state\n",
    "    # -------------------------\n",
    "    zoi_depth_range: Optional[Tuple[float, float]] = None\n",
    "\n",
    "    @property\n",
    "    def zoi(self) -> Optional[\"Dataset\"]:\n",
    "        \"\"\"Return a sliced Dataset for the current ZOI, or None if not set.\"\"\"\n",
    "        if self.zoi_depth_range is None:\n",
    "            return None\n",
    "        top, base = self.zoi_depth_range\n",
    "        return self.slice_depth(top, base)\n",
    "\n",
    "    def set_zoi(self, top: float, base: float) -> \"Dataset\":\n",
    "        \"\"\"Set the ZOI depth range and return the sliced ZOI Dataset.\"\"\"\n",
    "        top, base = float(top), float(base)\n",
    "        if top > base:\n",
    "            top, base = base, top\n",
    "        self.zoi_depth_range = (top, base)\n",
    "        # return the sliced dataset\n",
    "        return self.slice_depth(top, base)\n",
    "\n",
    "    def clear_zoi(self) -> None:\n",
    "        \"\"\"Clear ZOI selection.\"\"\"\n",
    "        self.zoi_depth_range = None\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Ensure depth index is float and sorted ascending\n",
    "        self.data = self.data.copy()\n",
    "        self.data.index = pd.to_numeric(self.data.index, errors=\"coerce\").astype(float)\n",
    "        self.data = self.data[~self.data.index.isna()].sort_index()\n",
    "\n",
    "        # If units_map exists, apply to meta if missing\n",
    "        for c in self.data.columns:\n",
    "            if c not in self.meta:\n",
    "                self.meta[c] = CurveMeta(name=c)\n",
    "            if c in self.units_map and not self.meta[c].units:\n",
    "                self.meta[c].units = self.units_map[c]\n",
    "\n",
    "    @property\n",
    "    def depth(self) -> pd.Index:\n",
    "        return self.data.index\n",
    "\n",
    "    def curves(self) -> List[str]:\n",
    "        return list(self.data.columns)\n",
    "\n",
    "    def has_curve(self, curve: str) -> bool:\n",
    "        return curve in self.data.columns\n",
    "\n",
    "    def add_curve(self, name: str, series: pd.Series, meta: Optional[CurveMeta] = None, overwrite: bool = True):\n",
    "        if (not overwrite) and (name in self.data.columns):\n",
    "            raise ValueError(f\"Curve already exists: {name}\")\n",
    "        s = pd.to_numeric(series, errors=\"coerce\")\n",
    "        s = s.reindex(self.data.index)  # align to dataset depth\n",
    "        self.data[name] = s\n",
    "        if meta is None:\n",
    "            meta = CurveMeta(name=name)\n",
    "        self.meta[name] = meta\n",
    "        if meta.units:\n",
    "            self.units_map[name] = meta.units\n",
    "\n",
    "    def get_family_candidates(self, family: str) -> List[str]:\n",
    "        return list(self.families_map.get(family, []))\n",
    "\n",
    "    def first_present(self, candidates: Iterable[str]) -> Optional[str]:\n",
    "        s = set(self.data.columns)\n",
    "        for c in candidates:\n",
    "            if c in s:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    def best_curve_for_family(self, family: str) -> Optional[str]:\n",
    "        return self.first_present(self.get_family_candidates(family))\n",
    "\n",
    "    def slice_depth(self, top: float, base: float) -> Dataset:\n",
    "        top, base = (top, base) if top <= base else (base, top)\n",
    "        df = self.data.loc[(self.data.index >= top) & (self.data.index <= base)].copy()\n",
    "        out = Dataset(\n",
    "            data=df,\n",
    "            meta={k: self.meta[k] for k in df.columns if k in self.meta},\n",
    "            families_map=self.families_map.copy(),\n",
    "            units_map=self.units_map.copy(),\n",
    "            history=self.history.copy(),\n",
    "            name=self.name,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "    # -------------------------\n",
    "    # Parquet I/O\n",
    "    # -------------------------\n",
    "    def to_parquet(self, path: str):\n",
    "        \"\"\"\n",
    "        Writes:\n",
    "          - {path} as parquet for curve data\n",
    "          - {path}.meta.json sidecar for metadata (simple, robust)\n",
    "        \"\"\"\n",
    "        self.data.to_parquet(path, index=True)\n",
    "\n",
    "        meta_dict = {\n",
    "            \"name\": self.name,\n",
    "            \"curves\": {k: asdict(v) for k, v in self.meta.items()},\n",
    "            \"families_map\": self.families_map,\n",
    "            \"units_map\": self.units_map,\n",
    "            \"history\": self.history,\n",
    "        }\n",
    "        with open(path + \".meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta_dict, f, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_parquet(path: str) -> Dataset:\n",
    "        df = pd.read_parquet(path)\n",
    "        # Depth index expected\n",
    "        if df.index.name is None:\n",
    "            df.index.name = \"DEPT\"\n",
    "\n",
    "        meta_path = path + \".meta.json\"\n",
    "        try:\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                meta_dict = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            meta_dict = {}\n",
    "\n",
    "        curve_meta = {}\n",
    "        for k, v in (meta_dict.get(\"curves\", {}) or {}).items():\n",
    "            curve_meta[k] = CurveMeta(**v)\n",
    "\n",
    "        return Dataset(\n",
    "            data=df,\n",
    "            meta=curve_meta,\n",
    "            families_map=meta_dict.get(\"families_map\", {}) or {},\n",
    "            units_map=meta_dict.get(\"units_map\", {}) or {},\n",
    "            history=meta_dict.get(\"history\", []) or [],\n",
    "            name=meta_dict.get(\"name\", \"dataset\"),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d85b5-78d7-472f-99f1-14efab7df7f1",
   "metadata": {},
   "source": [
    "## log_canvas_pg.py\n",
    "\n",
    "### ./petrocore/viz/log_canvas_pg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b946a15-92d3-4562-8b37-e6d711e26d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages/petrocore/petrocore/viz/log_canvas_pg.py\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PySide6.QtWidgets import QWidget, QVBoxLayout\n",
    "import pyqtgraph as pg\n",
    "\n",
    "from petrocore.models.dataset import Dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrackSpec:\n",
    "    name: str\n",
    "    curves: List[str] = field(default_factory=list)  # curve mnemonics to plot\n",
    "    x_range: Optional[tuple[float, float]] = None    # optional fixed x-range\n",
    "    log_x: bool = False                               # resistivity track etc.\n",
    "    show_grid: bool = True\n",
    "\n",
    "\n",
    "class LogCanvasPG(QWidget):\n",
    "    \"\"\"\n",
    "    A Geolog-style log canvas starter:\n",
    "      - multiple tracks (columns)\n",
    "      - shared depth axis (y)\n",
    "      - per-track curve lists\n",
    "      - supports log-x track for resistivity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent=None):\n",
    "        super().__init__(parent)\n",
    "\n",
    "        self._dataset: Optional[Dataset] = None\n",
    "        self._tracks: List[TrackSpec] = []\n",
    "\n",
    "        pg.setConfigOptions(antialias=True)\n",
    "        self.glw = pg.GraphicsLayoutWidget()\n",
    "        self.glw.setBackground(\"w\")\n",
    "\n",
    "        layout = QVBoxLayout(self)\n",
    "        layout.setContentsMargins(0, 0, 0, 0)\n",
    "        layout.addWidget(self.glw)\n",
    "\n",
    "        self._plot_items: List[pg.PlotItem] = []\n",
    "        self._curves_items: Dict[tuple[int, str], pg.PlotDataItem] = {}\n",
    "        \n",
    "        # Optional: override pen per curve name (e.g., base vs moving)\n",
    "        # self.curve_pens: Dict[str, pg.QtGui.QPen] = {}\n",
    "        self.curve_pens: Dict[str, pg.QtGui.QPen] = {}\n",
    "\n",
    "    def set_dataset(self, ds: Dataset):\n",
    "        self._dataset = ds\n",
    "        self.refresh()\n",
    "\n",
    "    def set_tracks(self, tracks: List[TrackSpec]):\n",
    "        self._tracks = tracks\n",
    "        self.refresh()\n",
    "\n",
    "    def refresh(self):\n",
    "        self.glw.clear()\n",
    "        self._plot_items = []\n",
    "        self._curves_items = {}\n",
    "\n",
    "        if self._dataset is None or self._dataset.data.empty or not self._tracks:\n",
    "            # placeholder\n",
    "            p = self.glw.addPlot()\n",
    "            p.hideAxis(\"left\")\n",
    "            p.hideAxis(\"bottom\")\n",
    "            p.addItem(pg.TextItem(\"Load data + define tracks\", color=\"k\"))\n",
    "            return\n",
    "\n",
    "        ds = self._dataset\n",
    "        depth = ds.depth.values.astype(float)\n",
    "\n",
    "        # Create track plots side-by-side\n",
    "        first_plot: Optional[pg.PlotItem] = None\n",
    "\n",
    "        for i, tr in enumerate(self._tracks):\n",
    "            p = self.glw.addPlot(row=0, col=i)\n",
    "            self._plot_items.append(p)\n",
    "\n",
    "            # Depth axis\n",
    "            p.invertY(True)                      # depth increases downward\n",
    "            p.showAxis(\"left\", i == 0)           # show y-axis only on first track\n",
    "            p.showAxis(\"bottom\", True)\n",
    "\n",
    "            if tr.show_grid:\n",
    "                p.showGrid(x=True, y=True, alpha=0.2)\n",
    "\n",
    "            p.setLabel(\"bottom\", tr.name, color=\"k\")\n",
    "            p.setLabel(\"left\", \"DEPT\", units=\"ft\", color=\"k\")\n",
    "\n",
    "            # Link Y (depth) between tracks\n",
    "            if first_plot is None:\n",
    "                first_plot = p\n",
    "            else:\n",
    "                p.setYLink(first_plot)\n",
    "\n",
    "            # Optional log-x (e.g., resistivity)\n",
    "            if tr.log_x:\n",
    "                p.setLogMode(x=True, y=False)\n",
    "\n",
    "            # Plot each curve in this track\n",
    "            for curve in tr.curves:\n",
    "                if curve not in ds.data.columns:\n",
    "                    continue\n",
    "                x = pd.to_numeric(ds.data[curve], errors=\"coerce\").values.astype(float)\n",
    "                m = np.isfinite(x) & np.isfinite(depth)\n",
    "                if m.sum() < 2:\n",
    "                    continue\n",
    "\n",
    "                # Create line (default pen; you can later map family->style)\n",
    " \n",
    "                pen = self.curve_pens.get(curve, pg.mkPen(width=1.2))\n",
    "\n",
    "                \n",
    "                item = p.plot(x[m], depth[m], pen=pen, name=curve)\n",
    "                \n",
    "                self._curves_items[(i, curve)] = item\n",
    "\n",
    "            # Set x range if provided\n",
    "            if tr.x_range is not None:\n",
    "                p.setXRange(tr.x_range[0], tr.x_range[1], padding=0.02)\n",
    "\n",
    "        # Nice: add a shared legend on first track\n",
    "        if self._plot_items:\n",
    "            leg = self._plot_items[0].addLegend(offset=(10, 10))\n",
    "            # pyqtgraph legend picks up curve names from plot(name=...)\n",
    "\n",
    "    # Convenience helpers for your “templates”\n",
    "    @staticmethod\n",
    "    def standard_4track_template(ds: Dataset) -> List[TrackSpec]:\n",
    "        # Use your family preference lists if present\n",
    "        gr  = ds.best_curve_for_family(\"GR\")  or ds.first_present([\"HSGR\", \"GR\", \"SGR\"])\n",
    "        rhb = ds.best_curve_for_family(\"RHOB\") or ds.first_present([\"RHOZ\", \"RHOB\"])\n",
    "        tnp = ds.best_curve_for_family(\"TNPH\") or ds.first_present([\"TNPH\", \"NPHI\", \"NPOR\"])\n",
    "        rt  = ds.best_curve_for_family(\"RT\")   or ds.first_present([\"AT90\", \"AF90\", \"ILD\", \"RT\"])\n",
    "\n",
    "        tracks = [\n",
    "            TrackSpec(name=\"GR\", curves=[c for c in [gr] if c], x_range=(0, 150)),\n",
    "            TrackSpec(name=\"Porosity\", curves=[c for c in [rhb, tnp] if c]),\n",
    "            TrackSpec(name=\"Resistivity\", curves=[c for c in [rt] if c], log_x=True),\n",
    "            TrackSpec(name=\"Computed\", curves=[]),\n",
    "        ]\n",
    "        return tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abfb20-2418-4926-8796-c857ab3732f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96e837-cc5b-4d06-83a7-1077ebb60699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a0f6c-972c-4037-a1fd-f3d8452f86c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (petro)",
   "language": "python",
   "name": "petro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
